# 파인 튜닝 가능 여부/ 방법 찾아보기
## Whisper 모델 파인튜닝 (STT 정확도 향상)
- Hugging Face Transformers 사용
OpenAI의 Whisper는 Hugging Face 생태계에서 매우 활발하게 사용됩니다. transformers 라이브러리를 사용하면 복잡한 파이토치(PyTorch) 코드를 직접 작성할 필요 없이 쉽게 파인튜닝할 수 있습니다.

- 경량화 기법 활용 (LoRA/QLoRA)
large 모델을 사용하셨기 때문에 전체 모델을 학습시키려면 엄청난 GPU 자원이 필요합니다.
**LoRA (Low-Rank Adaptation)**나 QLoRA와 같은 매개변수 효율적 파인튜닝(PEFT) 기법을 사용하면, 적은 GPU 메모리(예: Colab Pro GPU)로도 large 모델의 성능을 유지하면서 효율적으로 파인튜닝할 수 있습니다.

## 화자 분할 모델 파인튜닝 (Pyannote Diarization 정확도 향상)


## WhisperX를 통한 통합 접근


11.07 오후 진행
# Senko + STT(Whisper) -> data : 애슐리_소음.m4a
음성 전처리, 피크 정규화, RMS 정규화, 잡음 제거 시도
-> 정규화는 해도 효과가 딱히 없었음
16:30 팀별 진행상황 발표, 발표자 : 이홍기
# AssemblyAI 테스트
애슐리_잡음.m4a, 회의록 자료/1106_오후회의.m4a 2개 