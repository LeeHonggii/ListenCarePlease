# 사용 모델 지정
- GPT 5 Mini
    - 요약 시간 : 38.30초

- Upstage Solar Mini
    - 요약 시간 : 31.75초

- DeepSeek V3와 Gemma 2 9B는 비용적/시간적 문제로 제외
- GPT-5-mini와 Upstage-Solar-mini 두 모델을 사용하며 고민

# 진행 방식 구상
1. 통계적 키워드 추출
    - KeyBERT
    - TF-IDF
    - 문서 내 중요도 판단하여 추출

2. 형태소 분석기 + 필터링
    - Kiwi, Okt 등으로 형태소 분석 후 자주 쓰이는 단어 제외
    - 오타 / 무의미한 단어까지 잡히는 문제가 생길 수 있다

3. LLM
    - LLM에게 텍스트를 주고, 단어 찾아 뽑아내기
    - 정확도가 가장 높을 것으로 기대
    - LLM 의존도/가격 부분에서도 생각 필요

4. Hybrid
    - 1 또는 2번 진행 후 LLM에게 추리게 하는 방식
    - 단순 LLM 사용보다 비용을 아끼고 정확도를 높일 수 있다.

# 방식 별 결과
1. 통계적 키워드 추출
    - 공통 사용(Kiwi)
        - 불용어 걸러내기
        - 품사 확인
        - 명사 고르기
    - Kiwi + KeyBERT 사용
        - KeyBERt
            - Kiwi에서 골라진 명사들 Embedding
            - cosine 유사도로 중요도 판단
            - 주제 판단
                - 각 단어들을 스칼라 벡터로 변환
                - 각 좌표들 합산 후 평균(평균 풀링)
                - 위 평균으로 나온 좌표가 주제
            - 주제에 가까울수록 높은 점수
        - MMR 알고리즘
            - 1등 단어와 의미가 너무 비슷한 단어들은 점수 감소
            - 이 과정으로 서로 다른 측면의 단어들이 골고루 뽑힘
        - 사용 parameter
            - candidates : kiwi에서 진행 된 명사 명단
            - keyphrase_ngram_range(최소 개수, 최대 개수) : 단어 묶임
                - ex) (1, 1) : 사과, 배, (1, 2) : 맛있는 사과
            - stop_words : 불용어 리스트
            - use_mmr : 다양성 모드(MMR 진행 여부)
            - diversity : 다양성 조절 다이얼 - 주제와 어느 정도 유사도가 있는 단어를 뽑아야하나?
            - top_n : 추출할 단어 개수
        - 전문용어나 어려운 용어보다는 자주 쓰이거나 주제와 가까운 단어 뽑는 용도
    
    - Kiwi + TF-IDF 사용
        - TF-IDF
            - TF : 단어 등장 횟수 판단
                - 많이 등장할 수록 높은 점수
            - IDF : 다른 문서에서의 등장 횟수 판단
                - 다른 문서에 자주 등장한 단어 중요도 감소
                - 현재 진행중인 프로젝트에서는 적절하지 않다고 판단
                - 한 문서씩 투입하는 상황에서 IDF 작업이 정상적으로 이루어지지는 않을 것
                - 결국 TF 결과만 적용
                - 초반엔 결과가 없고, 여러 문서가 DB에 쌓여야 결과 나올것으로 추측

2. 형태소 분석기 + 필터링
    - 불용어 사전, 기초 한국어사전 필요
    - 위 2가지를 사용하지 않으면 단순 형태소 분석 작업
    - 만약 다국어로 사용 시 각 언어의 사전도 2가지씩 추가 필요
        - 형태소 분석기 모델도 언어별 필요
            - 영어 : NLTK
            - 일본어 : Mecab
            - etc
    - 제외

3. LLM 사용
    - 기초 파이프라인
        - Split : 긴 회의록을 1000~2000자 단위의 Chunk로 분할
        - Batch Processing : 각 Chunk를 LLM에게 보내서 단어 추출을 반복 요청
        - Merge : Chunk마다 나온 결과들을 하나로 병합
        - Deduplication : 중복 제거 과정

    - 테스트(1113_오후_회의.txt 사용)
        - GPT-5-mini
            - 평균 3분 30초 소요
            - 1차 시도
                - 92개 단어 추출
                - 사용 prompt
===================================================
[Role]
당신은 '수석 텍스트 분석가'입니다. 회의록의 문맥을 파악하여 키워드를 추출합니다.

[Task]
아래 [Text]에서 '비전공자'가 이해하기 힘든 전문용어, 기술용어, 신조어(은어)를 추출하십시오.

[Negative Constraints]
- '회의', '진행', '공유', '일정', '네', '아니요' 같은 일상적인 단어는 절대 제외하세요.
- 사람 이름은 제외하세요.
- 사람의 역할도 제외하세요.

[Output Format]
- 설명 없이 오직 JSON 리스트 포맷으로만 출력하세요. 예: ["단어1", "단어2"]

[Text]
{chunk}
===================================================
            - 2차 시도
                - 79개 단어 추출
                - 사용 prompt
===================================================
[Role]
당신은 '수석 텍스트 분석가'입니다. 회의록의 문맥을 파악하여 키워드를 추출합니다.

[Task]
아래 [Text]에서 '비전공자'가 이해하기 힘든 전문용어, 기술용어, 신조어(은어)를 추출하십시오.
문맥에서, 사람이 파악하기 힘들어하던 단어도 추출하십시오.
추출된 단어들 중 유사도가 높은 단어들은 하나로 합치고, 그 중, 표준어에 가까운 단어로 추출하세요.

[Negative Constraints]
- '회의', '진행', '공유', '일정', '네', '아니요' 같은 일상적인 단어는 절대 제외하세요.
- 사람 이름은 제외하세요.
- 사람의 역할도 제외하세요.

[Output Format]
- 설명 없이 오직 JSON 리스트 포맷으로만 출력하세요. 예: ["단어1", "단어2"]

[Text]
{chunk}
===================================================
            - 3차 시도
                - 80개 단어 추출
                - 사용 prompt
===================================================
[Role]
당신은 '수석 텍스트 분석가'입니다. 회의록의 문맥을 파악하여 키워드를 추출합니다.

[Task]
아래 [Text]에서 '비전공자'가 이해하기 힘든 전문용어, 기술용어, 신조어(은어)를 추출하십시오.
문맥에서, 사람이 파악하기 힘들어하던 단어도 추출하십시오.
추출된 단어들 중 유사도가 높은 단어들은 표준어에 가까운 하나의 단어만 추출하고 나머지는 제외하세요.

[Negative Constraints]
- '회의', '진행', '공유', '일정', '네', '아니요' 같은 일상적인 단어는 절대 제외하세요.
- 사람 이름은 제외하세요.
- 사람의 역할도 제외하세요.

[Output Format]
- 설명 없이 오직 JSON 리스트 포맷으로만 출력하세요. 예: ["단어1", "단어2"]

[Text]
{chunk}
===================================================
            - 4차 시도
                - 50개 단어 추출
                - 사용 prompt
===================================================
[Role]
당신은 '수석 텍스트 분석가'입니다. 회의록의 문맥을 파악하여 키워드를 추출합니다.

[Task]
아래 [Text]에서 '비전공자'가 이해하기 힘든 전문용어, 기술용어, 신조어(은어)를 추출하십시오.
문맥에서, 사람이 파악하기 힘들어하던 단어도 추출하십시오.
추출된 단어들 중 유사도가 높은 단어들은 표준어에 가까운 하나의 단어만 추출하고 나머지는 제외하세요.
단어를 추출하는 기준을 100으로 두었을 때, 60 이상에 부합하는 단어들만 추출하십시요.

[Negative Constraints]
- '회의', '진행', '공유', '일정', '네', '아니요' 같은 일상적인 단어는 절대 제외하세요.
- 사람 이름은 제외하세요.
- 사람의 역할도 제외하세요.
- 사람들이 자주 사용하여 뜻을 어렵지 않게 알 수 있는 단어는 제외하세요.

[Output Format]
- 설명 없이 오직 JSON 리스트 포맷으로만 출력하세요. 예: ["단어1", "단어2"]

[Text]
{chunk}
===================================================
            - 4-2차 시도
                - 52개 단어 추출
            - 4-3차 시도
                - 61개 단어 추출
            - Seed = 42 고정 시도
                - 54개 단어 추출
                - 4차 시도와 같은 prompt 사용
            - 시행마다 다른 결과 출력 → LLM 내부의 Seed값 고정 필요
    
        - Solar-mini
            - 약 7초 소요
            - 87개 단어 추출
            - 사용 prompt
===================================================
[Role]
당신은 '수석 텍스트 분석가'입니다. 회의록의 문맥을 파악하여 키워드를 추출합니다.

[Task]
아래 [Text]에서 '비전공자'가 이해하기 힘든 전문용어, 기술용어, 신조어(은어)를 추출하십시오.
문맥에서, 사람이 파악하기 힘들어하던 단어도 추출하십시오.
추출된 단어들 중 유사도가 높은 단어들은 표준어에 가까운 하나의 단어만 추출하고 나머지는 제외하세요.
단어를 추출하는 기준을 100으로 두었을 때, 80 이상에 부합하는 단어들만 추출하십시요.
오타로 판단되는 단어는 가장 가까운 단어로 변경하십시오.

[Negative Constraints]
- '회의', '진행', '공유', '일정', '네', '아니요' 같은 일상적인 단어는 절대 제외하세요.
- 사람 이름은 제외하세요.
- 사람의 역할도 제외하세요.
- 사람들이 자주 사용하여 뜻을 어렵지 않게 알 수 있는 단어는 제외하세요.

[Output Format]
- 설명 없이 오직 JSON 리스트 포맷으로만 출력하세요. 예: ["단어1", "단어2"]
- 추출된 단어들 중 중요도가 가장 높게 판단되는 25개의 단어만 출력하세요.

[Text]
{chunk}
===================================================
        - Gemma 2 9B
            - 약 1분 20초 소요
            - 88개 단어 추출
            - 사용 prompt
===================================================
[Task]
아래 [Text]에서 '비전공자'가 이해하기 힘든 전문용어, 기술용어, 신조어(은어)를 추출하십시오.
문맥에서, 사람이 파악하기 힘들어하던 단어도 추출하십시오.
추출된 단어들 중 유사도가 높은 단어들은 표준어에 가까운 하나의 단어만 추출하고 나머지는 제외하세요.
단어를 추출하는 기준을 100으로 두었을 때, 80 이상에 부합하는 단어들만 추출하십시요.
오타로 판단되는 단어는 가장 가까운 단어로 변경하십시오.

[Negative Constraints]
- '회의', '진행', '공유', '일정', '네', '아니요' 같은 일상적인 단어는 절대 제외하세요.
- 사람 이름은 제외하세요.
- 사람의 역할도 제외하세요.
- 사람들이 자주 사용하여 뜻을 어렵지 않게 알 수 있는 단어는 제외하세요.

[Output Format]
- 설명 없이 오직 JSON 리스트 포맷으로만 출력하세요. 예: ["단어1", "단어2"]
- 추출된 단어들 중 중요도가 가장 높게 판단되는 25개의 단어만 출력하세요.

[Text]
{chunk}
===================================================

# 추가
- LLM 단어 선별 → 사전 load → LLM으로 뜻 요약 및 생성
- 위 과정이면 사전 load가 없으면 한번에 가능
- 사전 load 과정이 중요한가 판단 필요
    - 사전 유무에 대한 결과를 비용/정확도 면에서 테스트 필요
- 회의록 전사 파일이 100% 정확하지 않아 이상한 단어들이 추출
    - 단어 개수 제한 필요성?
        - 몇개?
    - 회의록이 아닌 제대로 되어있는 텍스트 파일 사용?
        - 어떤 데이터들이 존재?