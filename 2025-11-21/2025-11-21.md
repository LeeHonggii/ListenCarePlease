# LLM 모델 탐색 및 테스트
##API 기반

- GPT-5-Mini
    - Open AI의 GPT-5-Mini API 모델
    - 요약 시간 : 38.30 초
    - 빠른 속도와 높은 정확도
    - prompt 이해/실행도가 높음
    - 신조어
        - 자주 쓰이는 신조어의 경우 알맞은 뜻 출력
        - 자주 쓰이지 않거나, 신생 신조어의 경우 잘못된 뜻 출력
        - 결국 신조어까지 신경 쓰지 않아도 되는 걸까?
    - 빠른 시간과 정확도를 자랑하지만 비용이 발생 → 팀원들과 협의 후 판단 필요($0.25)
===============================================
- GPT 호출 함수 내부에서 따로 prompt를 다시 준 이유
### 1. 입력 포맷의 차이 (Raw String vs JSON)

- **Local LLM (Hugging Face):**
    - 모델에게 **"완성된 하나의 긴 문자열(Prompt)"**을 던져줘야 합니다.
    - 그래서 `tokenizer.apply_chat_template`이라는 함수를 써서, 시스템/유저 대화를 특수 토큰(`<|im_start|>`, `[INST]` 등)이 포함된 **문자열로 변환**하는 과정이 필수적입니다.
- **OpenAI GPT:**
    - 모델에게 **"대화 리스트(List of Dictionaries)"**를 그대로 던져주면 됩니다.
    - API 내부에서 알아서 처리를 해주기 때문에, 우리가 굳이 문자열로 합치거나 템플릿을 신경 쓸 필요가 없습니다.

### 2. 출력 처리의 차이 (Parsing Logic)

이게 가장 큰 이유입니다.

- **Local LLM:**
    - 대부분 **[입력한 프롬프트 + 생성된 답변]**을 통째로 뱉어냅니다.
    - 그래서 코드 내부에 **"입력한 프롬프트 길이만큼 잘라내라"** (`generated_text[len(prompt):]`)는 파싱 로직이 반드시 프롬프트 변수와 함께 있어야 합니다.
- **OpenAI GPT:**
    - 깔끔하게 **[생성된 답변]**만 딱 줍니다.
    - 프롬프트를 잘라내는 로직이 필요 없습니다.

### 3. 모델의 "눈치" 차이 (Prompt Engineering)

- **Local LLM (7B~12B 소형 모델):**
    - 눈치가 조금 부족합니다. 그래서 프롬프트 끝에 `답변:` 혹은 `Answer:` 같은 **"트리거(Trigger)"** 단어를 억지로 붙여줘야 딴소리를 안 하고 바로 대답을 시작하는 경우가 많습니다.
    - `add_generation_prompt=True` 옵션이 그 역할을 합니다.
- **GPT-4o/5 (대형 모델):**
    - 눈치가 빠릅니다. 굳이 `답변:`이라고 안 써줘도, "요약해줘"라고 하면 알아서 요약만 깔끔하게 합니다.
    - 오히려 Local용 프롬프트를 그대로 쓰면 결과물에 "답변:"이라는 글자까지 같이 출력해버리는 부작용이 생길 수 있습니다.
===============================================
- HyyperCLOVA X (naver)
    - 다른 모델 대비 압도적으로 비싼 가격으로 폐기

- Mistral Large 2 (Mistral AI)
    - 다른 모델 대비 압도적으로 비싼 가격으로 폐기

## Local Model
- Gemma 2
    - google/gemma-2-9b-it
    - 요약 시간 : 80.83 초
    - 매우 양호한 결과를 보임
    - gpt-5-mini보다 약간 떨어지지만, 이정도면 사용할 정도는 된다고 생각.
    - 1~2문장으로 프롬프트를 입력하였지만, 1문장으로만 출력. 어떤 상황인지 확인이 필요할 것 같다.
    - 신조어 부분에서는 약한 모습을 보임.
        - 신조어도 주석 처리가 필요한 것인가? 의문
    - 시간적으로도 빠른편

- Gemma 3
    - google/gemma-3-4b-it
    - 지속적인 에러로 인한 폐기
        - 이미 많은 모델들이 테스트 된 상태에서, 오류가 나는 모델을 오래 잡고있을 이유가 없다.
===============================================
AcceleratorError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
===============================================

- Upstage Solar 10.7B (v1.0)
    - Upstage/SOLAR-10.7B-Instruct-v1.0
    - 모델 불러오기 ~ 요약 : 1시간 40분(12단어)
    - 요약 시간 : 5027.23초
    - 너무 오래 걸림
    - 심지어 성능도 좋지 않음
    - 다른 모델들과 같은 프롬프트를 사용했지만, 영어, 한자 등, 한국어보다 다른 언어의 비중이 높음
    - 신조어 등의 부분에도 매우 약한 모습을 보임

- Mistral-Nemo (8B)
    - nvidia/Mistral-NeMo-Minitron-8B-Instruct
    - 요약 시간 : 168.11 초
    - 첫 요약 문장은 정상적으로 출력
    - 그 이후에 prompt 내용, 쓸데없는 내용 등 덧붙는 상황
    - 신조어 부분에도 약한 모습을 보임
    - 시간적으로도 오래 걸림
    - max token을 200으로 제한했을 때, 문장이 이상하게 끊기는 상황이 나옴
    - 이후 max token 제한을 풀었을 때, 위와 같은 현상 발생

- Microsoft Phi-3.5 Mini
    - microsoft/Phi-3.5-mini-instruct
    - 요약 시간 : 109.26초
    - 다수의 깨진 단어
    - 신조어 약함
    - 문장 연결 및 끝마침 불안정

# 사용 Parameter
- Temperature
    - 0.0 ~ 1.0
    - 낮을수록 안정적, 흔한 단어
    - 높을수록 과감함, 할루시네이션 확률 증가

- top_p
    - 0.0 ~ 1.0
    - 각 단어의 확률을 정하여 top_p*100%의 값까지 수용

# 사용 Prompt
[지시사항]
1. 아래 [참고 자료]를 읽고 '{word}'의 뜻을 정의하세요.
2. [참고 자료]가 부족하면 당신의 지식을 활용하세요.
3. 답변은 반드시 '정의:'로 시작하고 1~2문장으로 끝내세요.
4. 답변할 때의 언어는 주어진 단어를 제외하고는 무조건 선택된 언어로 답변합니다.
5. 현재 선택된 언어는 '한글'입니다.
6. 주어진 단어의 뜻이 여러가지일 때는, 여러가지 뜻을 각각 한문장씩으로 정의합니다.
7. 답변의 종결이 여러 형태를 띄는 언어의 경우, 모든 답변의 형태를 통일합니다.

[예시]
단어: 리펙터링
참고 자료: [Wikipedia] 결과의 변경 없이 코드의 구조를 재조정함.
답변: 리펙터링이란 외부 동작은 유지한 채 내부 코드 구조를 개선하여 가독성과 유지보수성을 높이는 작업을 의미합니다.