Whisper 정리
- 기본 Whisper 전처리~후처리 코드 각각 다른 커널에 존재
- 각 task를 함수화 후 호출로 사용하는 방식으로 변경
- I/O 부분 최소화

특징 추출 task 진행
=============================================
- 팀원의 코드 파이프라인 정리 후 이해
『1. 환경 세팅
→ Numpy/Pandas/NeMo/Whisper 설치 & 버전 맞추기

2. 폴더 구조 세팅
→ /content/audio, /content/work, /content/work/outputs, /content/work/cache

3. 입력 오디오 확보
→ 드라이브에서 자동 탐색 or 업로드 버튼으로 받아서 AUDIO_IN에 고정

4. 오디오 전처리
→ ffmpeg로 AUDIO_IN → meeting_16k.wav (16kHz mono)

5. 화자 분리 (NeMo)
→ manifest + yaml 설정으로 diarization 수행 → RTTM 생성 → segments 리스트로 파싱

6. STT (Whisper)
→ meeting_16k.wav에 대해 한글 STT + word_timestamps → stt_words.json

7. 단어-화자 매칭 & 발화 생성
→ 단어마다 diarization segment에 겹치는 시간으로 speaker 할당 →
같은 화자 & 0.8초 이내 간격은 한 문장으로 묶음 → final_transcript.json

8. 기본 확인 + SRT(화자만)
→ 화자별 발화 시간/턴 수 출력 → [SPEAKER] 텍스트 형식 meeting.srt

9. 역할 분석 (One-shot)
→ 발화 텍스트 기반으로 요청/질문/백채널/부정/안건/진행멘트 패턴을 세고
회의 길이·발화 시점·공백 길이 등을 이용해
진행자/요청자/백채널러/반대자/잠깐 다녀간 사람/재입장자 등을 판정
- 질문자/요청多/안건 언급/진행 멘트/늦게 합류/중도 이탈/발화 적음 태그 부여

10. 최종 결과물 파일들
- final_transcript.json : 화자+텍스트만 있는 발화 리스트
- final_transcript_with_roles.json : 각 발화에 role, role_tags까지 포함
- roles_mapping.json : 화자별 역할/태그/통계
- roles_diagnostics.csv : 화자별 역할+수치 통계 (엑셀 분석용)
- meeting.srt : 화자만 표시된 자막
- meeting_roles.srt : 역할/화자 표기가 붙은 자막
- stt_words.json + RTTM : 중간 산출물(단어 단위 STT, diarization 결과)』
=============================================
- 모듈화 한 whisper 코드와 팀원의 태깅 clustering 코드 수정 및 병합 진행
- 1~7번까지 폐기, 8~10까지와 병합 시도
- 위 파이프라인을 diarization + whisper 결과를 투입하여 진행하는 방식으로 변경 시도
- 위 코드 그대로 따라가기 보다는 큰 pipeline을 따라 개인 진행 생각