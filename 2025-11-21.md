# TO DO

- 정량적 지표 test
    1. 실제 결과를 구할 수 있는지 (계산 가능?)
    2. 시각화 가능?
- [x]  정보량
- [x]  엔트로피
- [ ]  PPL
- [ ]  PMI

https://drive.google.com/drive/folders/1DYjba19FxBIOCYUaP49ZdiJL_nX6veOE?usp=drive_link

# data

[result_final.json](attachment:f00d4d34-f86c-4b90-9cc1-ec24be527403:result_final.json)

[result_final_with_embeddings.json](attachment:946bab8f-5c4b-484a-9980-dc371e5b8597:result_final_with_embeddings.json)

# #2. 언어적 정보 기반 지표 (Information-based Metrics)

**정보량 / 엔트로피 / perplexity / PMI / TTR**

---

## **4) 정보량 (Information Content, -log p(x))**

문장의 정보량을 어떻게 계산할까?

1. 통계적 확률 기반 (LLM) : “이 문장이 문맥상 얼마나 예측 불가능한가?
    
    $$
    I(S) = -\sum^{n}_{i=1}logP(w_i | w_{1...i-1},context)
    $$
    
    P : 앞의 문맥을 봤을 때 해당 단어가 나올 확룰
    
    I : 문장의 총 정보
    
    도구 : hf → GPT2LMHeadModel, CaualLM
    
2. 의미적 거리 기반 (Embedding) : “이 문장이 앞의 내용가 얼마나 다른 이야기를 하는가?”
    1. 벡터화 : 현재 문장과 이전 윈도우의 문장들을 벡터로 변환
    2. 컨텍스트 벡터 생성 : 이전 문장들의 평균 벡터 계산
    3. 거리 계산 :
        
        $$
        Info(S_t) = 1 - CosinSimliarity(V_s, V_{context})
        $$
        
3. 키워드 희소성 기반 (TF-IDF) 

### 결론

```markdown
⚡ Z-Score 이상치 상세 분석
==========================================================================================

🔺 높은 이상치 (Z ≥ 2) - 극단적으로 높은 정보량:
------------------------------------------------------------------------------------------
순위    시간(분)      정보량          Z-Score      화자           문장                                      
------------------------------------------------------------------------------------------
1     0.00       1.0000       4.1585       SPEAKER_5    다 알고 있는 사람들한테 보여줄게요                     
2     34.02      0.8459       2.8083       SPEAKER_0    예.                                      
3     27.00      0.8455       2.8055       SPEAKER_1    리눅스에서요?                                 
4     13.19      0.8160       2.5466       SPEAKER_0    안녕하세요 어서오세요                             
5     26.26      0.8125       2.5165       SPEAKER_1    유명하시던데요. 유튜브에 김영희 AI팀에 바로 얼굴 나오시던데요.    
6     33.21      0.8055       2.4549       SPEAKER_5    아니면 듣거나.                                
7     12.54      0.7802       2.2335       SPEAKER_0    지금 전반적으로 흐름은                            
8     6.27       0.7702       2.1453       SPEAKER_0    지금 약간 피곤한가보네                            
9     15.93      0.7694       2.1381       SPEAKER_5    엄청 빠르겠다                                 
10    13.87      0.7670       2.1175       SPEAKER_3    전체 화면으로 켜고                              

🔻 낮은 이상치 (Z ≤ -2) - 극단적으로 낮은 정보량:
------------------------------------------------------------------------------------------
순위    시간(분)      정보량          Z-Score      화자           문장                                      
------------------------------------------------------------------------------------------
1     24.01      0.2320       -2.5687      SPEAKER_0    그럼 투두에 진행 중에 팀 단위에 멘토링 브리핑 준비 거기다가 제가...
2     11.94      0.2583       -2.3386      SPEAKER_0    이제 그 STT 된 걸로 텍스트 군집화를 해서 군집화를 한거는 텍스...
3     32.30      0.2600       -2.3233      SPEAKER_2    그러면 이 후처리를 또 사용하게 된다고 하면은, 전처리 부분에서 어...
4     15.30      0.2655       -2.2750      SPEAKER_5    인베딩한 결과값이 필요한 거니까                       
5     25.68      0.2717       -2.2213      SPEAKER_3    내일 오전에 이때 최종 마무리 지어서.                   
6     34.00      0.2759       -2.1841      SPEAKER_0    그게 아니라면 정량평가는 정말 Reliable한 게 어떤 게 있는지...
7     25.34      0.2902       -2.0593      SPEAKER_5    나머지 더 생긴 건 내일 추가적으로 질문하면 되니까.           
==========================================================================================
```

문장 길이가 영향을 많이줌

- 알고리즘 추가
    
    **① [가장 추천] 짧은 문장 버퍼링 (Short Sentence Buffering)**
    
    짧은 문장은 독립적인 의미를 갖기보다 앞뒤 문장에 종속된 경우가 많습니다. 일정 길이 미만의 문장은 **단독으로 계산하지 않고 앞 문장에 붙여서** 계산합니다.
    • **로직:**
        1. 현재 문장 $S_t$의 길이가 기준(예: 15자) 미만인가?
        2. **YES:** $S_t$를 $S_{t-1}$ 뒤에 합친다. (벡터 계산 안 함)
        3. **NO:** 정상적으로 벡터화하고 정보량을 계산한다.
    • **효과:** "네", "알겠습니다", "저기요" 같은 노이즈가 제거되고, 문맥이 풍부해집니다.
    
    **② 로그 길이 정규화 (Log-Length Normalization)**
    
    정보량 점수에 문장 길이를 반영하여 보정합니다. 단순히 길이로 나누면 긴 문장이 너무 손해를 보므로, **로그($\log$)**를 씌워 완만하게 적용합니다.
    • **기존:** $Info = 1 - \text{Similarity}$
    • 보정:
    $$Info_{new} = Info_{old} \times \log(\text{len}(S_t))$$
    
    (또는 길이에 비례하는 가중치 함수 $W(L)$ 적용)
    • **효과:** 아주 짧은 문장은 로그 값이 작아져 정보량 점수가 깎입니다(노이즈 억제). 긴 문장은 점수를 온전히 받습니다.
    

---

## **5) 정보 엔트로피 (Information Entropy)**

- **알고리즘 개요 (Workflow)**
    
    
    1. **전처리:** 회의록에서 **핵심 명사**만 추출하고 불필요한 단어(불용어)는 버린다.
    2. **윈도우 설정:** 분석할 **구간의 크기(Window Size)**와 **이동 간격(Step)**을 정한다.
    3. **순회(Sliding):** 윈도우를 한 칸씩 밀면서 **지역 확률(Local Probability)**을 구한다.
    4. **계산:** 각 윈도우의 **엔트로피(Entropy)**를 산출한다.
    5. **시각화:** 시간 흐름에 따른 엔트로피 변화를 그래프로 그린다.
    
    **🛠️ STEP 1: 파라미터 설정 (Configuration)**
    
    분석의 민감도를 결정하는 변수를 정의합니다.
    • **`WINDOW_SIZE` (예: 50개):** 한 번에 분석할 명사의 개수.
        ◦ *너무 작으면(10개):* 그래프가 미친 듯이 튐 (노이즈 심함).
        ◦ *너무 크면(100개):* 변화가 둔감해짐 (화제 전환을 늦게 잡음).
    • **`STEP_SIZE` (예: 1개):** 윈도우를 몇 칸씩 밀 것인가. (보통 1 추천)
    
    **🧹 STEP 2: 데이터 전처리 (Preprocessing)**
    
    회의록 텍스트를 분석 가능한 **'명사 리스트'**로 변환합니다.
    1. **입력:** `[(시간1, "문장1"), (시간2, "문장2"), ...]`
    2. **형태소 분석:** `Mecab`, `Kiwi` 등을 사용해 명사(`NNG`, `NNP`)만 추출합니다.
    3. **불용어 필터링 (중요!):**
        ◦ 의존명사 제거: `것`, `수`, `분`, `데`, `바` ...
        ◦ 범용명사 제거: `회의`, `생각`, `말씀`, `저기`, `관련` ...
    4. **출력:** 시간 정보가 매핑된 명사 시퀀스 생성.
        ◦ `Data = [(t1, "서버"), (t1, "비용"), (t2, "AWS"), (t2, "증가"), ...]`
    
    **🪟 STEP 3: 슬라이딩 윈도우 반복 (Iteration)**
    
    `Data` 리스트 위를 윈도우가 지나가며 계산합니다.
    • **Loop:** $i = 0$ 부터 `len(Data) - WINDOW_SIZE` 까지 반복.
    • **Current Window:** `Data[i : i + WINDOW_SIZE]`
        ◦ 예: `['서버', '비용', 'AWS', '증가', ...]` (총 50개 단어)
    • **Current Time:** 윈도우의 중간 지점($i + \text{WINDOW\_SIZE}/2$)의 시간을 현재 시간으로 기록.
    
    **🧮 STEP 4: 로컬 엔트로피 계산 (Calculation)**
    
    **가장 중요한 단계**입니다. 윈도우 **내부의** 단어 빈도만으로 확률을 구합니다.
    1. **빈도 카운트 (Local Count):** 윈도우 안의 단어 개수를 셉니다.
        ◦ `{'서버': 10, '비용': 5, 'AWS': 5, ...}` (총합 50)
    2. **확률 계산 ($P_x$):** (단어 빈도 / 윈도우 크기)
        ◦ $P(\text{서버}) = 10/50 = 0.2$
        ◦ $P(\text{비용}) = 5/50 = 0.1$
    3. **엔트로피 공식 적용:** $\sum -P_x \log_2(P_x)$
        ◦ $-0.2 \log(0.2) - 0.1 \log(0.1) ...$ 의 총합.
    4. **결과 저장:** `(시간, 엔트로피_값)`을 리스트에 추가.
    
    **📈 STEP 5: 시각화 및 해석 (Result)**
    
    저장된 `(시간, 엔트로피)` 리스트를 라인 차트로 그립니다.
    • **Y축 (Entropy):**
        ◦ **높음 (High):** 다양한 단어가 섞여 나옴 $\rightarrow$ **발산(Brainstorming) / 산만함 / 화제 전환**
        ◦ **낮음 (Low):** 특정 단어가 반복됨 $\rightarrow$ **수렴(Focusing) / 합의 / 강조**
    • **해석 가이드:**
        ◦ 그래프가 **우하향(↘)** 하는가? $\rightarrow$ 회의가 결론을 향해 잘 가고 있다.
        ◦ 그래프가 **스파이크(∧)**를 그리는가? $\rightarrow$ 새로운 안건이 제시되었다.
    

### 결론

![entropy_with_trend.png](attachment:abe799f0-5e39-4c5b-9e5e-06e2818a4f38:entropy_with_trend.png)

```markdown
# 최종 요약 — 총 10개의 명확한 주제 전환 포인트

1. 발표/잡담 → 기술 보고 (≈120s)

2. 기술 보고 → 난이도/전략 논의 (≈233s)

3. 작업 논의 → 멘토 소개 (≈303s)

4. 멘토 → 다음 발표자(GLiNER) (≈376–382s)

5. NER → STT 모델 품질 (≈585s)

6. STT/NLP 기술 → 클러스터링/역할 분석 아이디어 (≈689s)

7. 기술 → 잡담(이모티콘) (≈775s)

8. 잡담 → 화자태깅/인베딩 기술 복귀 (≈858s)

9. 기술 구현 → 팀 주차 목표 논의 (≈1136–1180s)

10. 투두 → 멘토링 질문 정리 (≈1375s)
```

---

## **6) Perplexity 변화 추이**

(PPL은 모델 기준 예측 난이도)

---

## **7) TTR (Type-Token Ratio: 어휘 다양성)**

$$
TTR = \frac{고유\ 단어\ 수}{전체\ 단어\ 수}
$$

- 알고리즘
    
    🛠️ STEP 1: 데이터 준비 및 전처리
    1.1 JSON 파일 로드
    1.2 형태소 분석 (Mecab/Kiwi)
    1.3 명사(NNG, NNP) 추출
    1.4 Stopwords 제거
    1.5 시간 정보와 함께 저장 → [{time, word, pos}, ...]
    
    🪟 STEP 2: 슬라이딩 윈도우 설정
    2.1 윈도우 크기 설정 (명사 30~50개 또는 시간 60~120초)
    2.2 이동 간격 설정 (1~5개 또는 5~10초)
    2.3 최소 윈도우 크기 체크 (< 10개면 skip)
    
    🧮 STEP 3: 구간별 TTR 계산
    3.1 각 윈도우에서 Types/Tokens 계산
    3.2 (선택) RTTR 또는 LogTTR 계산
    3.3 윈도우 중심 시간 기록
    
    📈 STEP 4: 시각화
    4.1 원본 TTR 라인 플롯
    4.2 이동 평균(MA) 적용 및 신뢰구간 표시
    4.3 전체 평균 기준선 추가
    4.4 급변 구간 하이라이트
    4.5 해석 가이드 추가
    
    📊 STEP 5: 추가 분석 (선택)
    5.1 TTR 변화율 계산 (기울기)
    5.2 구간별 대표 키워드 추출
    5.3 TTR 급등/급락 구간의 실제 발화 내용 출력
    

![ttr_timeline.png](attachment:dd771d24-1397-42d7-bd60-b18c617899b5:ttr_timeline.png)

평가 : 추세를 확인할 수는 있다. 이게 TTR이 낮아질 수 있을까?

단어 추출해놓은거 보면 쉽지 않아 보인다.

---

## **8) PMI (Pointwise Mutual Information)**

### ✨ PMI × Perplexity 조합

| **구분** | **PMI 높음 (내용 끈끈함)** | **PMI 낮음 (내용 산만함)** |
| --- | --- | --- |
| **PPL 낮음**
(흐름 매끄러움) | **🏆 최상의 회의 (Deep Flow)**
주제도 깊고, 티키타카도 잘됨.
*(이상적인 전략 회의)* | **🗣️ 잡담/친목 (Small Talk)**
대화는 즐겁고 매끄럽지만,
남는 내용은 없음. |
| **PPL 높음**
(흐름 끊김/낯섦) | **⚔️ 격렬한 논쟁 (Debate)**
주제는 확실하지만,
서로 반박하느라 흐름이 거침. | **💥 최악의 회의 (Chaos)**
주제도 중구난방이고,
서로 무슨 말 하는지 모름. |

---

# 진행 중 어려운 점 (장애물이나 막힌곳 설명)

1. 너무 직관적이지 못하다….