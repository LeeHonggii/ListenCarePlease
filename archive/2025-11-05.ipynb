{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parakeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lee-hong-gi/anaconda3/envs/STT/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1105 11:47:32.674000 88627 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-05 11:48:58 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 8192 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-05 11:49:00 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    use_lhotse: true\n",
      "    skip_missing_manifest_entries: true\n",
      "    input_cfg: null\n",
      "    tarred_audio_filepaths: null\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    shuffle: true\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    max_duration: 10.0\n",
      "    min_duration: 1.0\n",
      "    text_field: answer\n",
      "    batch_duration: null\n",
      "    max_tps: null\n",
      "    use_bucketing: true\n",
      "    bucket_duration_bins: null\n",
      "    bucket_batch_size: null\n",
      "    num_buckets: 30\n",
      "    bucket_buffer_size: 20000\n",
      "    shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-11-05 11:49:00 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    use_lhotse: true\n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    max_duration: 40.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 2\n",
      "    pin_memory: true\n",
      "    text_field: answer\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-05 11:49:00 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-11-05 11:49:03 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
      "[NeMo I 2025-11-05 11:49:03 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-05 11:49:03 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-05 11:49:03 nemo_logging:393] Using RNNT Loss : tdt\n",
      "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-11-05 11:49:03 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: CUDA is not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-11-05 11:49:05 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from /Users/lee-hong-gi/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v3/snapshots/be0d803fd1970eca8627f5467c208118f0f6c171/parakeet-tdt-0.6b-v3.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing: 100%|██████████| 1/1 [03:32<00:00, 212.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, you don't take on someone or a wheel. Time to get a little bit of a little bit of a donate on the hand of cool. I go at the criteria to want to talk about A hope to be so cool. Oh, like on one day in the coy serious advice one. Maybe we have a book of money someway. Take a month well. Okay, thank you. Tom signal each on a colour. The only tango only change you say. Uhango. Okay. Could you want the poison detection AI days on the gun? Wait, wait, uh command to get the shot away. Eh, yeah. Okay, target some of the panium. Ah, they're gone. Okay. Okay. Okay. So the truth. Oh, but if you're parada, but something else means the parada how to put a has a eh? Some bind room. How to say this for the two and head on to go? Uh take a young tone, put it pink ago. So you do have single sunkilla. I'm good. I can't do this. Tell my saddle colonic one. You would say Hannah Second Day. Oh, you can take a chat upon the car, take us a banyan bans to tell the Krigo Tato on taste on early got Hadin and the Chinamatia. Got Chusogi. A betum to be too cut in a full boat to come at a good option. But could you push for chatter meeting? Oh but single can get your hair the case quite an hoxable day or says in the room man head to some jungle. Oh, could you go by the meeting guy? Oh, can they S T T full down and text it to the until you couldn't get double tango? Yes, Two and Kazoo I go find to another can I told my cousin can turn it. How do you dano? But the bane shapuck is to take got there. Okay. Take a parada taking manual bandus in the woman. Oh dear guy. Oh, this is the same. I don't I didn't have someone in the door to one rush.\n"
     ]
    }
   ],
   "source": [
    "output = asr_model.transcribe(['1105_오전회의.m4a'])\n",
    "print(output[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faster_whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-05 12:02:34.768] [ctranslate2] [thread 7197460] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 2.00s]  시작되면 말하기 시작\n",
      "[2.00s -> 3.00s]  시작\n",
      "[3.00s -> 4.00s]  네, 눌렀습니다\n",
      "[4.00s -> 5.00s]  네\n",
      "[5.00s -> 8.00s]  다시 제 것부터 간략히 말씀드리면\n",
      "[8.00s -> 12.00s]  저는 세 개의 주제에 대해서 찾아보고 있었는데요\n",
      "[12.00s -> 14.00s]  어제는 주로 저는\n",
      "[15.00s -> 17.00s]  작가 문체로 자동 교정해주는 것을\n",
      "[17.00s -> 19.00s]  염두에 두고 시작해봤는데\n",
      "[19.00s -> 21.00s]  결과적으로 다 마음에 들지만\n",
      "[21.00s -> 23.00s]  데이터도 쉽게\n",
      "[23.00s -> 25.00s]  쉽게 구한다는 말은\n",
      "[25.00s -> 27.00s]  쉽게 꿀 빠하려는 의미보다는\n",
      "[27.00s -> 30.00s]  쉽게 더 강력한 데이터를 구해서\n",
      "[30.00s -> 33.00s]  우리 모델한테 더 좋은 먹이를 주고 싶다는\n",
      "[33.00s -> 35.00s]  연료를 주고 싶다는 마음이 컸어요\n",
      "[35.00s -> 36.00s]  그래가지고\n",
      "[37.00s -> 40.00s]  그러다 보니까 또 실시간 세계 경제 뉴스나\n",
      "[40.00s -> 41.00s]  바로 하자 자동 태깅 쪽으로\n",
      "[41.00s -> 43.00s]  갈 수밖에 좀 없는 판단을 했고요\n",
      "[43.00s -> 44.00s]  자발적으로\n",
      "[44.00s -> 46.00s]  그래서 이제 또\n",
      "[46.00s -> 47.00s]  그러면 이제 그 두 개 중에 좀 더\n",
      "[47.00s -> 49.00s]  제가 힘을 싣고 싶었던 거는\n",
      "[49.00s -> 52.00s]  좀 더 이렇게 어필링한 어떤 포트폴리오를 위해서\n",
      "[52.00s -> 54.00s]  바로 하자 자동 태깅을 좀 더 해보자\n",
      "[54.00s -> 56.00s]  그리고 팀원들의 어떤 의견 수렴도\n",
      "[56.00s -> 57.00s]  전반적으로 좀 더\n",
      "[57.00s -> 58.00s]  기류였던 것 같아서\n",
      "[58.00s -> 61.00s]  같이 그냥 동참해서 가는 게 좋을 것 같아서\n",
      "[61.00s -> 63.00s]  신속한 의사 결정을 위해서\n",
      "[63.00s -> 64.00s]  그래서 이제 자료를 조사해 봤더니\n",
      "[64.00s -> 66.00s]  국립국어원에서 상당히 강력하고\n",
      "[66.00s -> 69.00s]  풍부한 데이터를 제공해 주는 것 같습니다\n",
      "[69.00s -> 71.00s]  근데 이게 지금 음성 형태인지 아니면 그냥\n",
      "[71.00s -> 74.00s]  포퍼스라니까 그냥 어떤 그 텍스트 형태인지\n",
      "[74.00s -> 78.00s]  좀 더 뜯어보고 공유 드릴 예정이고요\n",
      "[78.00s -> 79.00s]  그리고 이제 두 번째로\n",
      "[79.00s -> 80.00s]  제가 지금 염두에 두고 있는 게\n",
      "[80.00s -> 83.00s]  AI 허브의 이제 어떤 대화 데이터는\n",
      "[83.00s -> 85.00s]  정말 무궁무진하고 많더라고요\n",
      "[85.00s -> 86.00s]  그리고\n",
      "[86.00s -> 87.00s]  상황도 다양해요\n",
      "[87.00s -> 88.00s]  예를 들면 긴급 출동 상황\n",
      "[88.00s -> 90.00s]  뭐 공항 주변 소음 상황\n",
      "[90.00s -> 92.00s]  뭐 공사장 상황\n",
      "[92.00s -> 96.00s]  그리고 그런 어떤 노이즈가 포함되었냐 안되었냐를 떠나서도\n",
      "[96.00s -> 98.00s]  뭐 엄청 그 많더라고요\n",
      "[98.00s -> 100.00s]  뭐 아동의 대화 뭐 이런 거\n",
      "[100.00s -> 101.00s]  그래서 우리가\n",
      "[101.00s -> 103.00s]  우리에게 필요한 데이터로 보여지는 게 많아서\n",
      "[103.00s -> 106.00s]  좀 이제 많이 스크랩 해놨으니까\n",
      "[106.00s -> 109.00s]  이따 또 한번 리스트업해서 공유하도록 하고\n",
      "[109.00s -> 111.00s]  어쨌든 그래서 결과적으로 저는\n",
      "[111.00s -> 114.00s]  바라자 자동 태깅 쪽으로 데이터를 좀 많이\n",
      "[114.00s -> 115.00s]  일단 구비해 놨다 정도로\n",
      "[115.00s -> 117.00s]  볼 수 있고\n",
      "[117.00s -> 119.00s]  여기서 어 이런 데이터는 충분하네요 라고 해서\n",
      "[119.00s -> 121.00s]  되면 이제 바로 파이프라인 설계로 넘어가도\n",
      "[121.00s -> 125.00s]  저는 괜찮을 것 같습니다\n",
      "[125.00s -> 128.00s]  다른 분들\n",
      "[128.00s -> 130.00s]  이제부터 해야 되나요\n",
      "[130.00s -> 135.00s]  저는 어제 했던 데이터는 정리해 놨고\n",
      "[135.00s -> 139.00s]  근데 이제 데이터가 NLP보다는 비전 쪽에 좀 가깝다\n",
      "[139.00s -> 144.00s]  왜냐면 PDF 자체를 이미지를 분류하다 보니까\n",
      "[144.00s -> 146.00s]  비전 쪽 양식인 것 같아서\n",
      "[146.00s -> 150.00s]  그 부분은 조금 고민을 안 했습니다\n",
      "[150.00s -> 151.00s]  안 할 것 같아서 저도\n",
      "[151.00s -> 155.00s]  다른 그래서 두 번째로 저도 작가 문체를 조금 찾아봤습니다\n",
      "[155.00s -> 161.00s]  작가 문체에 대한 데이터는 어느 정도 AI 허브에도 있었고\n",
      "[161.00s -> 165.00s]  허깅 페이스나 기탑에도 좀 있었던 것 같아요\n",
      "[165.00s -> 168.00s]  그래서 좀 데이터를 가져왔고\n",
      "[168.00s -> 172.00s]  근데 이제 어떻게 좀 진행을 해야 되는가\n",
      "[172.00s -> 173.00s]  너무 어려운 것 같아요 저는 문체에 대한 데이터를 좀 가져왔고\n",
      "[174.00s -> 180.00s]  문체를 변경한다는 학습 자체가 너무 어렵고\n",
      "[180.00s -> 184.00s]  그걸 어떻게 진행해야 될까는 아무리 생각해도\n",
      "[184.00s -> 186.00s]  저는 이때 실패했거든요\n",
      "[186.00s -> 187.00s]  일단 실패했고\n",
      "[187.00s -> 194.00s]  지금도 생각하기에는 학습을 통해서 문체를 바꿀 수 있는가에 대해서는 좀 의문이라\n",
      "[194.00s -> 196.00s]  일단은 찾아놓긴 했습니다\n",
      "[196.00s -> 199.00s]  어떻게 해야 될지랑 데이터를 찾아놨고\n",
      "[199.00s -> 203.00s]  이제 설득할 자신은 없다 정도입니다\n",
      "[204.00s -> 206.00s]  일단은 hype의 소희 기자 같이 찾아볼까요?\n",
      "[206.00s -> 224.00s]  Hbuild\n",
      "[224.00s -> 227.00s]  입장이 없으면 공격 꺼지는 거였 Kennedy\n",
      "[227.00s -> 230.00s]  티브이\n",
      "[230.00s -> 231.00s]  존\n",
      "[231.00s -> 232.00s]  철 had\n",
      "[232.00s -> 233.00s]  퀸\n",
      "[233.00s -> 240.00s]  저는 어제 애초에 워크플로우 자체를 생각 안하고 있었어요 무조건 데이터 그냥 알아보자 이렇게만 알고 있었어가지구\n",
      "[240.00s -> 241.00s]  좋아요\n",
      "[245.00s -> 246.00s]  그럼 바로 저로 넘어갈까요?\n",
      "[246.00s -> 247.00s]  네\n",
      "[247.00s -> 254.00s]  저도 제가 맡은게 세계경제뉴스 정리 AI하고 볼스피싱 판별 AI인데\n",
      "[254.00s -> 260.00s]  솔직히 말해서 볼스피싱 판별은 어제 좀 해보다가 이제 완전 드롭 상태고요\n",
      "[260.00s -> 279.00s]  세계경제뉴스 정리 쪽을 이제 해결해봤는데 여기는 이제 제가 좀 생각하기에 처음에 데이터 수집할 때 BBC, 저널리스트, 로밍 뭐 되게 많더라구요\n",
      "[279.00s -> 289.00s]  진짜 미국쪽에서 유명한 그런 경제 사이트들 그래서 그것들을 이제 크롤링하는 스크립트를 짜가지고\n",
      "[289.00s -> 290.00s]  DBA에서\n",
      "[290.00s -> 300.00s]  거기에 그냥 일정 시간에 보통 찾아보니까 한국 시간으로는 오전 7시에서 9시가 주요 기사가 정리되는 시간이라고 해가지고\n",
      "[300.00s -> 307.00s]  한 9시나 그때쯤에 스크립트 들어가게 해놓으면은 이제 DBA로 자동으로 쌓이니까\n",
      "[307.00s -> 319.00s]  그걸 생각했는데 여기서 단점이 그거는 이제 매일매일 추가되는 거지 그 이전까지 있던 거는 좀 어떻게 해야될까가 좀 문제의 상황이고요\n",
      "[319.00s -> 320.00s]  그래서 일단 그렇게 이제\n",
      "[320.00s -> 327.00s]  단점 데이터를 이제 우리는 결국엔 한국으로 변화 할 예정이니까\n",
      "[327.00s -> 339.00s]  한국으로 변환하고 이제 거기에서 이제 요약하는 그런 느낌으로 HI 활동을 계속 해가지고 그렇게 생각을 해봤습니다\n",
      "[339.00s -> 349.00s]  약간 그러면 그냥 여쭤보자면 데이터가 이제 조금 충분히 확보에 대한 어떤 자신감이나 가시권에 들어 왔다 생각하세요?\n",
      "[349.00s -> 353.78s]  하세요? 이걸 진행한 다음에 우리가 향후에?\n",
      "[353.78s -> 360.38s]  점점 쌓이는 거여서 이전에 다른 그리고 쿠를링이\n",
      "[360.38s -> 364.32s]  저는 그렇게 안 좋아하는 게 평소가 좀 많아서\n",
      "[364.32s -> 365.18s]  벅이 잘 나죠?\n",
      "[365.18s -> 368.40s]  네 벅도 잘 가고 제가 잘하는 것도 아니고\n",
      "[368.40s -> 370.70s]  그래서 일단 그거에 대한 불안감도 있고\n",
      "[370.70s -> 373.06s]  이게 오늘부터 실행한다 해도\n",
      "[373.06s -> 374.84s]  오늘부터 데이터가 쌓이는 거예요\n",
      "[374.84s -> 379.78s]  오늘치 뉴스에 이전 거 오늘치 쌓이고 내일치 쌓여도\n",
      "[379.78s -> 382.98s]  애초에 데이터 양 자체가 많은 게 아니어서\n",
      "[382.98s -> 385.68s]  많아지려면 그 전 거를 가져야 되는데\n",
      "[385.68s -> 391.08s]  그걸 어떻게 가져야 될지 아직 잘 안 그려져 가지고\n",
      "[391.08s -> 392.44s]  그리고 놓친 거 같은데\n",
      "[392.44s -> 400.68s]  보이스피싱 디텍션 AI에 대해서는 약간 왜 그만두게 되셨다고 했었죠?\n",
      "[400.68s -> 404.84s]  일단 데이터 자체가\n",
      "[404.84s -> 407.16s]  좀 계속 찾아봤는데\n",
      "[407.16s -> 410.08s]  어떤 식으로 해야 될지\n",
      "[410.08s -> 413.70s]  처음 생각했던 거와 다르게 그림이 잘 안 그려져요\n",
      "[413.70s -> 414.90s]  막상 찾아보니까?\n",
      "[414.90s -> 416.44s]  네\n",
      "[416.44s -> 419.30s]  오케이 좋습니다 잘 알겠습니다\n",
      "[419.30s -> 427.40s]  저는 일단 밑에 거 작가 벤체부터 말씀드리면\n",
      "[427.40s -> 430.50s]  이거는 반 이상 포기한 상태네요\n",
      "[430.50s -> 433.74s]  이걸 어제 데이터도 찾아보고 파이프라인은 해봤는데\n",
      "[433.74s -> 434.54s]  이거는\n",
      "[434.84s -> 439.08s]  이게 일단 첫 번째로 언어 쪽에서 이제\n",
      "[439.08s -> 442.34s]  외국 책 같은 경우 데이터도 많은데\n",
      "[442.34s -> 445.14s]  가져왔을 때 이걸 멀티링기어로 한다고 해도\n",
      "[445.14s -> 449.20s]  그러면 그 외국 문장들의 문체는 어떻게 되는 거냐\n",
      "[449.20s -> 451.28s]  같은 게 일단 제 고민이었고요\n",
      "[451.28s -> 454.66s]  두 번째로 이게 매력이 있냐라고 했을 때 저는\n",
      "[454.66s -> 456.40s]  아 뭐 완성해도\n",
      "[456.40s -> 458.34s]  결과물에 매력을 잘 모르겠더라고요\n",
      "[458.34s -> 460.00s]  그럴 수 있어요 맞아요\n",
      "[460.00s -> 461.08s]  그 부분도 그렇고\n",
      "[461.08s -> 464.40s]  또 피처 사용에 있어서 다른 쪽에 고민이 많이 되더라고요\n",
      "[464.84s -> 468.44s]  그래서 이 부분은 좀 조사하다가 버려졌고\n",
      "[468.44s -> 471.44s]  이제 이게 보이스피싱 같은 경우는\n",
      "[471.44s -> 477.08s]  해당 데이터에서 어제 찾은 건 금광원하고 KORCCVI 외에는 못 찾았어요\n",
      "[477.08s -> 480.48s]  근데 이걸 사용하게 되면 이제\n",
      "[480.48s -> 485.84s]  얘는 개인적으로 주제 자체하고 결과물이 상상을 했을 때는 마음에 드는데\n",
      "[485.84s -> 487.70s]  어떻게 진행하냐라고 했을 때\n",
      "[487.70s -> 491.80s]  결국에는 KORCCVI 같은 경우는 무조건 텍스트 데이터니까\n",
      "[491.80s -> 493.96s]  그 텍스트 내 키워드라던가 그 어투 많은 것들에 대해서는 잘 알았고\n",
      "[493.96s -> 494.68s]  그 어투 많은 것들에 대해서는 잘 알았고\n",
      "[494.68s -> 496.44s]  그 어투 많은 것들에 대해서는 잘 알았고\n",
      "[496.44s -> 499.66s]  금광원 데이터는 MPC나 LP4기 때문에\n",
      "[499.66s -> 502.74s]  WAV로 바꿔서 사용을 한다고 했을 때\n",
      "[502.74s -> 504.54s]  그 안에서 또 운송만 나올 수 있는 피처가 있잖아요\n",
      "[504.54s -> 509.02s]  그럼 그거를 또 일반데이터로 해서\n",
      "[509.02s -> 511.64s]  2번을 합숙해서 이렇게 한다는 게\n",
      "[511.64s -> 517.20s]  지금 제 지식에선 잘 모르겠더라고요\n",
      "[517.20s -> 520.90s]  사실 이 생각이 든 것도 어제 회의 때\n",
      "[520.90s -> 522.54s]  민서님 말씀하시는 거에 저도 혹해가지고\n",
      "[522.54s -> 523.56s]  그렇죠 그렇죠\n",
      "[523.56s -> 524.56s]  바라자 그걸\n",
      "[524.68s -> 528.90s]  살짝 좀 떨어지게 조사한 감이 있긴 한데\n",
      "[528.90s -> 533.58s]  살짝 어필을 하고 싶긴 한데\n",
      "[533.58s -> 536.14s]  저기 좋은 줄 알았는데 어필을 해야지\n",
      "[536.14s -> 537.60s]  맞아 저도 그런 거 맞아요\n",
      "[537.60s -> 546.00s]  그러면 지금 다들 이제 꺼내놓으신 거죠?\n",
      "[546.00s -> 546.66s]  다섯 명 다\n",
      "[546.66s -> 551.00s]  그럼 이제 뭐 투표를 할까요?\n",
      "[551.00s -> 556.34s]  바라자로부터 손을 들어볼까요?\n",
      "[556.34s -> 557.54s]  손 들어요?\n",
      "[557.54s -> 560.04s]  그럼 어떻게 하세요?\n",
      "[560.04s -> 563.12s]  제가 바라자로까지만 투표할 거 같은데\n",
      "[563.12s -> 564.28s]  손 들겠습니다\n",
      "[564.28s -> 566.82s]  바라자로 찬성하시는 분?\n",
      "[566.82s -> 568.56s]  인당 하나예요\n",
      "[568.56s -> 570.48s]  중복 투표 가능할까요?\n",
      "[570.48s -> 572.88s]  중복 하시죠\n",
      "[572.88s -> 574.72s]  바라자로 가능할까요?\n",
      "[574.72s -> 577.38s]  이거 중복을 왜 물어본 거야\n",
      "[577.38s -> 579.26s]  지금 네 명입니다\n",
      "[579.26s -> 580.98s]  좀 굳어진 거 같은데\n",
      "[580.98s -> 582.58s]  일단 형식적으로 할게요\n",
      "[582.58s -> 583.44s]  선진국 마인드로\n",
      "[583.44s -> 587.72s]  자 다음 주제 로우 데이터\n",
      "[587.72s -> 588.94s]  저는 안 합니다\n",
      "[588.94s -> 589.88s]  저는 이제 손을 들...\n",
      "[589.88s -> 591.42s]  아 오케이 그러면 해당 사항 없고\n",
      "[591.42s -> 592.94s]  투표해서 탈락됐고\n",
      "[592.94s -> 598.38s]  자 세계경제 실시간 정리 브리핑 AI\n",
      "[598.38s -> 599.38s]  이거?\n",
      "[599.38s -> 601.50s]  저도 의류로 제가 했으니까 투표\n",
      "[601.50s -> 604.08s]  저도 손 들게요\n",
      "[604.08s -> 605.66s]  이름 쓸 필요가 없어 보이고\n",
      "[605.66s -> 608.32s]  자 보이스피싱\n",
      "[608.32s -> 609.32s]  한 표는 돼요\n",
      "[609.32s -> 610.66s]  한 표 그래도 의류로 오케이\n",
      "[610.98s -> 613.48s]  자 뭐 다들 아시겠지만\n",
      "[613.48s -> 615.98s]  뭐야 이거\n",
      "[615.98s -> 617.18s]  뭐죠? 바라\n",
      "[617.18s -> 619.64s]  바라 태깅으로 네 표 됐습니다\n",
      "[619.64s -> 620.36s]  괜찮아요?\n",
      "[620.36s -> 621.68s]  아니\n",
      "[621.68s -> 624.34s]  조 이름을 내 말을 들어줘 어때요\n",
      "[624.34s -> 626.02s]  오 좋아\n",
      "[626.02s -> 627.56s]  근데 이게 주제가 나와야 된다니까\n",
      "[627.56s -> 629.08s]  주제가 나와야 된다니까 이게\n",
      "[629.08s -> 630.04s]  깔끔한데요?\n",
      "[630.04s -> 630.72s]  좋아 좋아 좋아\n",
      "[630.72s -> 632.08s]  제가 수정할게요 지금\n",
      "[632.08s -> 632.96s]  수정할게요\n",
      "[632.96s -> 636.58s]  뭔가 너의 목소리가 들려 이런 것보다는 덜 뻔하고 좋다\n",
      "[636.58s -> 637.98s]  내 말을 들어줘\n",
      "[637.98s -> 640.82s]  그냥 딱 조 이거 딱 말해?\n",
      "[640.82s -> 643.24s]  맞아 지금 맞아\n",
      "[643.24s -> 643.74s]  지금 딱 csak 좋을 거 같아\n",
      "[643.74s -> 645.32s]  좋아요\n",
      "[645.32s -> 648.60s]  자 그럼 이제 뭐 확정 났으니까 데이터 더\n",
      "[648.60s -> 651.10s]  진짜 중요하잖아요 데이터 구하는 게\n",
      "[651.10s -> 653.04s]  그럼 데이터 구하고\n",
      "[653.04s -> 655.68s]  그리고 어떻게 보면 우리 이제 아까 강사님이\n",
      "[655.68s -> 657.94s]  최소한의 어떤 오늘 할 일을\n",
      "[657.94s -> 660.70s]  약간 이제 템하는 사람 걸러 내기 위해\n",
      "[660.70s -> 662.10s]  어떤 최소한 요구 치가 하나였잖아요\n",
      "[662.10s -> 663.14s]  우리가 데이터 세트\n",
      "[663.14s -> 664.56s]  근데 제가 봤을 때는 하나 이상\n",
      "[664.56s -> 667.18s]  정말 이거 정말 좋다. 하면 다 가져 와야 될 거 같아요\n",
      "[667.18s -> 668.50s]  뭔지 알죠? 데이터 세트를\n",
      "[668.50s -> 668.76s]  응\n",
      "[668.76s -> 669.34s]  지금 제가 Uno 프로 Active Santos 가 그 задач을, 그는 기구를 � turbul는데요\n",
      "[669.34s -> 669.78s]  일단 우리 가 parte 3이니까\n",
      "[669.78s -> 670.62s]  그리고 itself is a business event li tão 저희가 이 세트에서\n",
      "[670.62s -> 674.06s]  찾아보니까 데이터셋 많으면 많으시고 좋대요 무조건\n",
      "[674.06s -> 676.80s]  그리고 데이터셋에 어떤 음성데이터뿐만 아니라\n",
      "[676.80s -> 682.00s]  진짜 어노테이션이 별에 별까지 다 돼 있는 게 중요한 거 같아요\n",
      "[682.00s -> 685.12s]  그러니까 주석이 몇붕떠 몇초까지는 소음이 있었고\n",
      "[685.12s -> 687.34s]  뭐 누가 말했고 무슨 내용이고 이런 거\n",
      "[687.34s -> 689.96s]  그런 게 없어도 일단은 최소화는 되니까\n",
      "[689.96s -> 693.66s]  최대한 많이 가져와서 마지막에 간추리면 되잖아요\n",
      "[693.66s -> 695.46s]  네 그러면 될 거 같고요\n",
      "[695.46s -> 698.12s]  그리고 이제 어 말씀하세요\n",
      "[698.12s -> 700.34s]  일단은 데이터 찾을 때\n",
      "[700.34s -> 702.80s]  어쨌든 저희가 데이터셋을 찾아보고\n",
      "[702.80s -> 706.04s]  까보고 확인을 하고 해야지만 좋은 데이터를 찾는 거니까\n",
      "[706.04s -> 706.78s]  맞아요\n",
      "[706.78s -> 709.26s]  오전 중에는 너무 무리하지 않게\n",
      "[709.26s -> 712.14s]  한 두개 정도 데이터 찾는 걸 목표로 하고\n",
      "[712.14s -> 715.34s]  오후에 더 찾고 싶은 건 찾으면서\n",
      "[715.34s -> 717.42s]  간단하게 파이프라인이 그쵸\n",
      "[717.42s -> 719.38s]  그러니까 최대한 찾아오라는 게\n",
      "[719.38s -> 722.18s]  나 하나 했으니까 끝 이걸 하지 말라는 거지\n",
      "[722.18s -> 724.10s]  너 왜 다섯 개도 안 해왔어 이건 아니에요\n",
      "[724.10s -> 725.88s]  뭔지 알죠?\n",
      "[725.88s -> 726.64s]  자 그렇고요\n",
      "[726.64s -> 727.74s]  어쨌든 그리고 파이프라인까지\n",
      "[727.74s -> 728.10s]  오늘\n",
      "[728.10s -> 730.02s]  가급적이면 청소실 나오면 좋을 것 같아요.\n",
      "[730.80s -> 732.70s]  일단 제가 생각한 게 있었어요.\n",
      "[733.34s -> 735.38s]  저희가 선택을 해야 될 게 있어요.\n",
      "[735.66s -> 740.26s]  STT를 과연 학습으로 구현할 것인가에 대해서 선택을 해봐야 돼요.\n",
      "[740.26s -> 747.88s]  왜냐하면 요새 STT는 학습이 없어도 가진 모델로만 해도 성능이 굉장히 높은 영역이라\n",
      "[747.88s -> 752.24s]  저희가 만약에 학습을 했을 때 이점이 있는가예요.\n",
      "[753.32s -> 755.62s]  포트폴리오에 쓸 수 있는가도 의문이죠.\n",
      "[755.62s -> 758.16s]  왜냐하면 NLP의 영역은 아니라고 생각해요.\n",
      "[758.26s -> 759.28s]  저는 STT 자체가.\n",
      "[759.70s -> 760.82s]  저 그래서 여쭤봤는데 어제.\n",
      "[761.40s -> 768.40s]  근데 이제 STT 후에 나오는 텍스트에 대한 처리를 저희가 진행을 하는 게 맞다고 생각을 하는데\n",
      "[769.54s -> 773.04s]  STT 모델은 가져와도 파인튜닝 같은 거 안 해도 괜찮아요?\n",
      "[773.30s -> 774.78s]  정말 그 정도로 괜찮아요? 요즘?\n",
      "[775.18s -> 778.12s]  요즘은 아주 가무지게 된다고 알고 있어요.\n",
      "[778.20s -> 781.96s]  그럼 저는 이거는 뭐 찬성이에요. 안 해도 된다에 찬성이에요.\n",
      "[783.08s -> 785.40s]  네. 저희가 몰랐는데 STT는\n",
      "[785.62s -> 791.22s]  STT를 안 한다는 말씀이 어쨌든 바라자 탱킹을 하려면 제가 생각했던 거는\n",
      "[791.22s -> 796.82s]  이제 음성으로 들어간 것들에서 바라자를 이제 찾아서 나눈다라고 생각을 했거든요.\n",
      "[796.82s -> 798.90s]  그래서 만약에 STT를 안 하게 되면은\n",
      "[798.90s -> 800.90s]  STT를 데려오는 거죠.\n",
      "[800.90s -> 806.90s]  맞아요. STT 자체가 바라자를 나누는 게 아니라 그냥 말을 바꿔주는 거예요.\n",
      "[806.90s -> 808.90s]  말을 텍스트로 바꿔주는 거네요.\n",
      "[808.90s -> 814.26s]  이미 잘 나온 게 있으니까 우리가 개발할 필요는 없이 그냥 본인이 돼 있는 걸 쓰자는 말이라고 생각하고 바꾸는 거예요.\n",
      "[814.26s -> 815.46s]  만약에 그런 거 있겠습니까?\n",
      "[815.46s -> 818.66s]  바라자 나누는 부분을 저희가 하는 거죠.\n",
      "[818.66s -> 823.30s]  바라자를 나누고 내용에 대해서 정리를 하고\n",
      "[823.30s -> 828.58s]  회의로 만들어주는 애는 고용하고 그걸 이제 또 우리가 처리하는 건 우리가 개입하자.\n",
      "[828.58s -> 830.58s]  그 영역에 거해가지고요.\n",
      "[830.58s -> 837.86s]  아니면 이걸 해보고 싶으면 일단은 논 내로 그냥 우리 방금 말한 대로 진행하되\n",
      "[837.86s -> 843.30s]  나중에 시간 나오면 이제 개별적으로 더 힘주고 싶은 데가 있을 수도 있으니까\n",
      "[843.30s -> 845.30s]  그때 그냥 들어가도 되는 거고\n",
      "[845.46s -> 848.74s]  그래서 메인디쉬하고 디저트는 각자 다른 거로 되잖아요.\n",
      "[848.74s -> 850.74s]  그런 식으로 해도 될 것 같아요.\n",
      "[850.74s -> 852.74s]  오케이.\n",
      "[852.74s -> 854.74s]  그러면 지금 뭐 다들 괜찮아요?\n",
      "[854.74s -> 856.74s]  풀해요? 오케이.\n",
      "[856.74s -> 858.74s]  그러면 STT는 일단 그런 식으로 가고요.\n",
      "[858.74s -> 863.46s]  제일 급한 거는 NLP에 하는 걸로 된 것 같고요.\n",
      "[863.46s -> 867.46s]  그러니까 그러면 이제 오늘 투두는 이제 아까 데이터 구하고\n",
      "[867.46s -> 871.46s]  그리고 서로 도움 될 만큼 매력적인 데이터 찾은 다음에\n",
      "[871.46s -> 875.46s]  그리고 어쨌든 오늘 파이프라인은 막 뭐 확정적으로 하진 않지만\n",
      "[875.46s -> 879.46s]  각자 하나씩은 좀 이렇게 구상해보고 시뮬레이션 해보고\n",
      "[879.46s -> 883.46s]  한번 내가 혼자서 이렇게 해보는 정도까지 해오면 될까요?\n",
      "[883.46s -> 885.46s]  오늘 저녁까지? 그리고 다시 회의하면 되나요?\n",
      "[885.46s -> 889.46s]  좀 파이프라인에 대해서 한번 생각을 해보시는 게 좋을 것 같아요.\n",
      "[889.46s -> 895.46s]  자기가 이 바라자 태깅만으로 무슨 프로젝트를 만들 수 있는가에 대해서\n",
      "[895.46s -> 897.46s]  조금 고민을 해보는 게 맞는 것 같아요.\n",
      "[897.46s -> 903.46s]  그러면은 일단 회의실 오후에는 미리 지금 써두죠.\n",
      "[903.46s -> 905.46s]  오후 예정으로. 어디 할까요?\n",
      "[905.46s -> 907.46s]  저는 어디든 상관없습니다.\n",
      "[907.46s -> 909.46s]  가운데로?\n",
      "[909.46s -> 911.46s]  아 나 저 제 손에 줘.\n",
      "[911.46s -> 913.46s]  아 뭐 어디든 좋은데 저는 사실 창문 있는 데가 좋아요.\n",
      "[913.46s -> 915.46s]  그러면은 1번으로 하시면 될 것 같아요.\n",
      "[915.46s -> 919.46s]  1번으로? 지금 써놓을게요.\n",
      "[919.46s -> 921.46s]  나 마커 있는 것 같은데?\n",
      "[921.46s -> 925.46s]  마커 좀 맞춰.\n",
      "[925.46s -> 927.46s]  저기 저쪽에.\n",
      "[927.46s -> 929.46s]  저쪽에?\n",
      "[929.46s -> 933.46s]  네.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\"large-v3\")\n",
    "\n",
    "segments, info = model.transcribe(\"1105_오전회의.m4a\")\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Transcription with Faster-Whisper\n",
      "================================================================================\n",
      "Loading Whisper model...\n",
      "Transcribing audio...\n",
      "Detected language: ko (probability: 1.00)\n",
      "\n",
      "Transcription segments:\n",
      "[0.00s -> 2.00s]  시작되면 말하겠습니다.\n",
      "[2.00s -> 4.00s]  시작을 눌렀습니다.\n",
      "[4.00s -> 8.00s]  다시 제 것부터 간략히 말씀드리면\n",
      "[8.00s -> 12.00s]  저는 3개의 주제에 대해서 찾아보고 있었는데요\n",
      "[12.00s -> 14.00s]  어제는 주로 저는\n",
      "[14.00s -> 19.29s]  작가 문체로 자동 교정해주는 것을 영대 두고 시작해봤는데\n",
      "[19.29s -> 21.29s]  결과적으로 다 마음에 들지만\n",
      "[21.29s -> 25.29s]  데이터도 쉽게 구한다는 말은\n",
      "[25.29s -> 27.29s]  쉽게 꿀빠하려는 의미보다는\n",
      "[27.29s -> 30.29s]  쉽게 더 강력한 데이터를 구해서\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m transcription = []\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTranscription segments:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msegments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstart\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 처음 10개만 출력\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/STT/lib/python3.11/site-packages/faster_whisper/transcribe.py:1851\u001b[39m, in \u001b[36mrestore_speech_timestamps\u001b[39m\u001b[34m(segments, speech_chunks, sampling_rate)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrestore_speech_timestamps\u001b[39m(\n\u001b[32m   1845\u001b[39m     segments: Iterable[Segment],\n\u001b[32m   1846\u001b[39m     speech_chunks: List[\u001b[38;5;28mdict\u001b[39m],\n\u001b[32m   1847\u001b[39m     sampling_rate: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   1848\u001b[39m ) -> Iterable[Segment]:\n\u001b[32m   1849\u001b[39m     ts_map = SpeechTimestampsMap(speech_chunks, sampling_rate)\n\u001b[32m-> \u001b[39m\u001b[32m1851\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/STT/lib/python3.11/site-packages/faster_whisper/transcribe.py:1213\u001b[39m, in \u001b[36mWhisperModel.generate_segments\u001b[39m\u001b[34m(self, features, tokenizer, options, log_progress, encoder_output)\u001b[39m\n\u001b[32m   1198\u001b[39m     tokenizer.language_code = language\n\u001b[32m   1200\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.get_prompt(\n\u001b[32m   1201\u001b[39m     tokenizer,\n\u001b[32m   1202\u001b[39m     previous_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1205\u001b[39m     hotwords=options.hotwords,\n\u001b[32m   1206\u001b[39m )\n\u001b[32m   1208\u001b[39m (\n\u001b[32m   1209\u001b[39m     result,\n\u001b[32m   1210\u001b[39m     avg_logprob,\n\u001b[32m   1211\u001b[39m     temperature,\n\u001b[32m   1212\u001b[39m     compression_ratio,\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m options.no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1216\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n\u001b[32m   1217\u001b[39m     should_skip = result.no_speech_prob > options.no_speech_threshold\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/STT/lib/python3.11/site-packages/faster_whisper/transcribe.py:1446\u001b[39m, in \u001b[36mWhisperModel.generate_with_fallback\u001b[39m\u001b[34m(self, encoder_output, prompt, tokenizer, options)\u001b[39m\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1441\u001b[39m     kwargs = {\n\u001b[32m   1442\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbeam_size\u001b[39m\u001b[33m\"\u001b[39m: options.beam_size,\n\u001b[32m   1443\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpatience\u001b[39m\u001b[33m\"\u001b[39m: options.patience,\n\u001b[32m   1444\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_no_speech_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuppress_blank\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuppress_blank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_initial_timestamp_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_initial_timestamp_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1461\u001b[39m tokens = result.sequences_ids[\u001b[32m0\u001b[39m]\n\u001b[32m   1463\u001b[39m \u001b[38;5;66;03m# Recover the average log prob from the returned score.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# full_transcription_with_diarization.py\n",
    "\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "from pyannote.audio import Pipeline\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ==================== 설정 ====================\n",
    "AUDIO_FILE = \"1105_오전회의.m4a\"\n",
    "HF_TOKEN = \"\"  # HuggingFace 토큰\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 출력 폴더 생성\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==================== 1. Whisper 전사 ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Transcription with Faster-Whisper\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Loading Whisper model...\")\n",
    "whisper_model = WhisperModel(\"medium\", device=device, compute_type=\"float16\" if device == \"cuda\" else \"int8\")\n",
    "\n",
    "print(\"Transcribing audio...\")\n",
    "segments, info = whisper_model.transcribe(\n",
    "    AUDIO_FILE,\n",
    "    language=\"ko\",\n",
    "    beam_size=5,\n",
    "    vad_filter=True,\n",
    "    vad_parameters=dict(min_silence_duration_ms=500)\n",
    ")\n",
    "\n",
    "print(f\"Detected language: {info.language} (probability: {info.language_probability:.2f})\")\n",
    "\n",
    "# 전사 결과 저장\n",
    "transcription = []\n",
    "print(\"\\nTranscription segments:\")\n",
    "for i, segment in enumerate(segments):\n",
    "    transcription.append({\n",
    "        'start': segment.start,\n",
    "        'end': segment.end,\n",
    "        'text': segment.text\n",
    "    })\n",
    "    if i < 10:  # 처음 10개만 출력\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "# JSON 저장\n",
    "with open(f'{OUTPUT_DIR}/transcription.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(transcription, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Transcription complete! {len(transcription)} segments\")\n",
    "print(f\"  Saved to: {OUTPUT_DIR}/transcription.json\")\n",
    "\n",
    "# 메모리 정리\n",
    "del whisper_model\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ==================== 2. Speaker Diarization ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Speaker Diarization with pyannote.audio\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Loading diarization pipeline...\")\n",
    "try:\n",
    "    # 최신 버전: token 사용\n",
    "    diarization_pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        token=HF_TOKEN  # ✅ use_auth_token이 아니라 token\n",
    "    )\n",
    "except TypeError:\n",
    "    # 구버전: use_auth_token 사용\n",
    "    diarization_pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=HF_TOKEN\n",
    "    )\n",
    "\n",
    "if device == \"cuda\":\n",
    "    diarization_pipeline = diarization_pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "print(\"Performing speaker diarization (this may take a while)...\")\n",
    "try:\n",
    "    diarization = diarization_pipeline(AUDIO_FILE)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Diarization failed: {e}\")\n",
    "    print(\"\\nFalling back to transcription only (no speaker labels)\")\n",
    "    \n",
    "    # 화자 정보 없이 저장\n",
    "    with open(f'{OUTPUT_DIR}/final_result.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(transcription, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(f'{OUTPUT_DIR}/final_result.txt', 'w', encoding='utf-8') as f:\n",
    "        for seg in transcription:\n",
    "            f.write(f\"[{seg['start']:.2f}s -> {seg['end']:.2f}s] {seg['text']}\\n\")\n",
    "    \n",
    "    print(f\"✓ Saved transcription without speaker labels\")\n",
    "    exit(0)\n",
    "\n",
    "# 화자 정보 추출\n",
    "speaker_segments = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    speaker_segments.append({\n",
    "        'start': turn.start,\n",
    "        'end': turn.end,\n",
    "        'speaker': speaker\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Diarization complete! Found {len(speaker_segments)} speaker segments\")\n",
    "\n",
    "# 화자 통계\n",
    "speakers = set([seg['speaker'] for seg in speaker_segments])\n",
    "print(f\"  Number of speakers detected: {len(speakers)}\")\n",
    "for speaker in sorted(speakers):\n",
    "    count = sum(1 for seg in speaker_segments if seg['speaker'] == speaker)\n",
    "    duration = sum(seg['end'] - seg['start'] for seg in speaker_segments if seg['speaker'] == speaker)\n",
    "    print(f\"    {speaker}: {count} segments ({duration:.1f}s)\")\n",
    "\n",
    "# 화자 세그먼트 저장\n",
    "with open(f'{OUTPUT_DIR}/diarization.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(speaker_segments, f, indent=2)\n",
    "print(f\"  Saved to: {OUTPUT_DIR}/diarization.json\")\n",
    "\n",
    "# ==================== 3. 전사와 화자 정보 결합 ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Matching transcription with speakers\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def find_speaker(start, end, speaker_segments):\n",
    "    \"\"\"주어진 시간 구간에 가장 많이 겹치는 화자 찾기\"\"\"\n",
    "    max_overlap = 0\n",
    "    best_speaker = \"UNKNOWN\"\n",
    "    \n",
    "    for seg in speaker_segments:\n",
    "        overlap_start = max(start, seg['start'])\n",
    "        overlap_end = min(end, seg['end'])\n",
    "        overlap = max(0, overlap_end - overlap_start)\n",
    "        \n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_speaker = seg['speaker']\n",
    "    \n",
    "    return best_speaker\n",
    "\n",
    "# 전사 결과에 화자 정보 추가\n",
    "final_results = []\n",
    "for seg in transcription:\n",
    "    speaker = find_speaker(seg['start'], seg['end'], speaker_segments)\n",
    "    final_results.append({\n",
    "        'start': seg['start'],\n",
    "        'end': seg['end'],\n",
    "        'speaker': speaker,\n",
    "        'text': seg['text']\n",
    "    })\n",
    "\n",
    "print(f\"✓ Matching complete! {len(final_results)} segments with speaker labels\")\n",
    "\n",
    "# ==================== 4. 결과 저장 ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Saving results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# JSON 저장\n",
    "with open(f'{OUTPUT_DIR}/final_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/final_result.json\")\n",
    "\n",
    "# TXT 저장\n",
    "with open(f'{OUTPUT_DIR}/final_result.txt', 'w', encoding='utf-8') as f:\n",
    "    for seg in final_results:\n",
    "        f.write(f\"[{seg['start']:.2f}s -> {seg['end']:.2f}s] {seg['speaker']:12s}: {seg['text']}\\n\")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/final_result.txt\")\n",
    "\n",
    "# SRT 자막 저장\n",
    "with open(f'{OUTPUT_DIR}/final_result.srt', 'w', encoding='utf-8') as f:\n",
    "    for i, seg in enumerate(final_results, 1):\n",
    "        start_h = int(seg['start'] // 3600)\n",
    "        start_m = int((seg['start'] % 3600) // 60)\n",
    "        start_s = seg['start'] % 60\n",
    "        end_h = int(seg['end'] // 3600)\n",
    "        end_m = int((seg['end'] % 3600) // 60)\n",
    "        end_s = seg['end'] % 60\n",
    "        \n",
    "        f.write(f\"{i}\\n\")\n",
    "        f.write(f\"{start_h:02d}:{start_m:02d}:{start_s:06.3f}\".replace('.', ','))\n",
    "        f.write(\" --> \")\n",
    "        f.write(f\"{end_h:02d}:{end_m:02d}:{end_s:06.3f}\".replace('.', ','))\n",
    "        f.write(f\"\\n{seg['speaker']}: {seg['text']}\\n\\n\")\n",
    "print(f\"✓ Saved: {OUTPUT_DIR}/final_result.srt\")\n",
    "\n",
    "# ==================== 5. 결과 미리보기 ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Results Preview (first 20 segments):\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for seg in final_results[:20]:\n",
    "    print(f\"[{seg['start']:6.2f}s -> {seg['end']:6.2f}s] {seg['speaker']:12s}: {seg['text']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ All done!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal segments: {len(final_results)}\")\n",
    "print(f\"Unique speakers: {len(set([s['speaker'] for s in final_results]))}\")\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}/\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - transcription.json (Whisper only)\")\n",
    "print(f\"  - diarization.json (pyannote only)\")\n",
    "print(f\"  - final_result.json (combined, with speakers)\")\n",
    "print(f\"  - final_result.txt (human readable)\")\n",
    "print(f\"  - final_result.srt (subtitle format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
