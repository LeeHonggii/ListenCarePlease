11-04 조사 자료 
주제 - NLP를 위한 Raw-Data를 깔끔한 형태의 데이터 구조에 알맞게 정리, 정돈, 저장해 주는 AI
OCR을 해야하는가 


```

## 🏗️ **전체 아키텍처**

DocLLM은 바운딩 박스 정보만을 사용하여 공간 레이아웃 구조를 통합하는 레이아웃 인식 생성 언어 모델로, 비싼 이미지 인코더를 피하고 텍스트와 공간 모달리티 간의 교차 정렬을 고전적 트랜스포머의 어텐션 메커니즘을 분해하여 포착합니다 .
```
[문서 이미지]
    ↓
┌─────────────────────────────────┐
│  OCR 엔진 (Tesseract/EasyOCR)   │
│  - 텍스트 추출                    │
│  - 바운딩 박스 추출               │
└─────────────────────────────────┘
    ↓
┌─────────────────────────────────┐
│  Layout Analysis                │
│  - 제목, 본문, 표 등 구분         │
│  - 문서 구조 파악                │
└─────────────────────────────────┘
    ↓
┌─────────────────────────────────┐
│  LayoutLM/DocLLM (딥러닝)       │
│  입력:                           │
│   - 텍스트 토큰                  │
│   - 2D 위치 (바운딩 박스)         │
│   - 이미지 특징                  │
│  출력:                           │
│   - 구조화된 JSON                │
└─────────────────────────────────┘
    ↓
[구조화된 데이터베이스]


🎓 추천 모델

LayoutLMv3 (Microsoft) - 가장 널리 사용
DocLLM (ACL 2024) - 최신 생성 모델
Donut - OCR-free 접근
YOLO + Custom - 빠른 처리

완벽한 질문입니다! DocLLM이 어떻게 텍스트+위치+이미지를 종합적으로 이해하는지 자세히 설명드리겠습니다.완벽합니다! DocLLM이 어떻게 텍스트+위치+이미지를 종합 이해하는지 **핵심 메커니즘**을 설명드리겠습니다.

## 🧠 **DocLLM의 핵심: Disentangled Spatial Attention**

### **1. 입력 구조**

입력 문서는 텍스트 토큰과 바운딩 박스로 구성되며, DocLLM은 비싼 이미지 인코더를 피하고 바운딩 박스 정보만을 사용하여 공간 레이아웃 구조를 통합합니다.

```python
# 입력 예시
input = {
    "text_tokens": ["Invoice", "Number", ":", "INV-123"],
    "bounding_boxes": [
        [10, 20, 100, 40],   # Invoice의 위치 (x,y,width,height)
        [110, 20, 150, 40],  # Number의 위치
        [260, 20, 270, 40],  # : 의 위치
        [280, 20, 380, 40]   # INV-123의 위치
    ]
}
```

### **2. Disentangled (분리된) 표현**

공간 정보를 별도의 모달리티로 취급하여 별도의 벡터를 사용하여 이 두 모달리티를 표현하고, 트랜스포머 아키텍처의 self-attention 메커니즘을 확장하여 분리된 방식으로 상호 의존성을 계산합니다.

**일반 LLM (잘못된 방법):**
```python
# ❌ 단순 결합 (정보 손실)
embedding = text_embedding + position_embedding
```

**DocLLM (올바른 방법):**
```python
# ✅ 분리된 표현 유지
H = text_hidden_vectors    # 텍스트 의미
S = spatial_hidden_vectors # 공간 위치

# 각각 독립적으로 유지!
```

### **3. 4가지 Attention 계산**

모델은 T2T(텍스트-텍스트), T2S(텍스트-공간), S2S(공간-공간) 계산에서 얻은 어텐션 스코어를 "분리된 방식"으로 결합하며, 이는 모델이 별도의 어텐션 스트림을 유지한 다음 통합할 수 있음을 의미합니다.

DocLLM은 어텐션 메커니즘을 4개의 스코어로 분해합니다: 텍스트-텍스트, 텍스트-공간, 공간-텍스트, 공간-공간으로, 투영 행렬과 하이퍼파라미터를 사용하여 각 스코어의 중요도 균형을 맞춥니다.

```python
# Attention 계산 (수식 단순화)

# 1. Text-to-Text (T2T) - 일반적인 언어 이해
A_t2t = softmax(Q_text @ K_text.T / sqrt(d))

# 2. Text-to-Spatial (T2S) - 텍스트가 위치 정보 참조
A_t2s = softmax(Q_text @ K_spatial.T / sqrt(d))

# 3. Spatial-to-Text (S2T) - 위치가 텍스트 의미 참조
A_s2t = softmax(Q_spatial @ K_text.T / sqrt(d))

# 4. Spatial-to-Spatial (S2S) - 레이아웃 구조 이해
A_s2s = softmax(Q_spatial @ K_spatial.T / sqrt(d))

# 최종 결합
Attention = A_t2t + λ1*A_t2s + λ2*A_s2t + λ3*A_s2s
```

여기서 Ws,q와 Ws,k는 공간 모달리티에 해당하는 새로 도입된 투영 행렬이며, λs는 각 스코어의 상대적 중요도를 제어하는 하이퍼파라미터입니다.

### **4. 실제 동작 예시**

```
문서 예시:
┌─────────────────────────────┐
│ Invoice Number: INV-123     │  ← 같은 줄, 인접
│                             │
│ Date: 2024-01-01           │  ← 다른 줄, 왼쪽 정렬
│                             │
│ Total: $1,500              │  ← 더 아래, 왼쪽 정렬
└─────────────────────────────┘

DocLLM의 이해 과정:

1. T2T Attention:
   "Invoice" ↔ "Number" (의미적 연관)
   "Total" ↔ "$1,500" (의미적 연관)

2. S2S Attention:
   "Invoice Number:"의 위치 → 오른쪽에 값이 있을 것
   "Date:"의 위치 → 오른쪽에 날짜가 있을 것
   세로로 정렬된 항목들 → 양식 구조 파악

3. T2S + S2T Attention:
   "Invoice Number" 텍스트 + 오른쪽 위치 정보
   → "INV-123"이 답이라고 판단!
```

## 🎯 **왜 이 방식이 중요한가?**

Disentangled Spatial Attention은 공간 레이아웃 정보를 어텐션 프로세스에 명시적으로 통합함으로써 전통적인 어텐션 메커니즘을 넘어서며, 이를 통해 모델이 텍스트 및 공간 차원에서 더 효과적으로 추론할 수 있습니다.

### **비교:**

**일반 LLM:**
```
"Invoice Number: INV-123" 
→ 텍스트만 봄
→ "Invoice"와 "INV-123"의 관계를 
   단어 순서로만 이해
```

**DocLLM:**
```
"Invoice Number: INV-123"
→ 텍스트 + 위치 동시에 봄
→ "Invoice Number"가 왼쪽 상단
→ "INV-123"이 그 오른쪽에 인접
→ 이것이 키-값 쌍임을 이해!
```

## 💡 **Pre-training: Block Infilling**

DocLLM의 사전 학습은 "중간 채우기" 작업과 유사한 infilling 목표를 포함하며, 모델이 주변 컨텍스트를 기반으로 누락된 콘텐츠를 예측할 수 있게 하여 정보가 불완전하거나 무질서한 실제 문서 구조를 처리하는 데 중요합니다.

```python
# 학습 방식
원본: [Invoice][Number][:][INV-123][Date][:]...
      ↓
마스킹: [Invoice][MASK][MASK][MASK][Date][:]...
      ↓
예측: "Number", ":", "INV-123"을 
      텍스트 의미 + 공간 위치로 예측!
```

## 📊 **Architecture 다이어그램**

왼쪽: 텍스트 토큰 xi와 바운딩 박스 bi가 있는 입력 문서. 일부 텍스트 세그먼트가 무작위로 마스킹되고 모델이 이러한 텍스트 세그먼트의 토큰을 자동회귀적으로 예측합니다. 오른쪽: 어텐션 메커니즘이 텍스트와 공간 모달리티 간의 교차 어텐션을 설명하도록 확장됩니다.

```
┌─────────────────────────────────────┐
│  Input Layer                        │
│  - Text Tokens: [t1, t2, ...]     │
│  - Bounding Boxes: [(x,y,w,h)...] │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│  Embedding Layer                    │
│  - Text Embeddings: H              │
│  - Spatial Embeddings: S (별도!)   │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│  Disentangled Attention (N layers) │
│  ┌─────────────────────────────┐  │
│  │ Multi-Head Attention:       │  │
│  │ - A_t2t (텍스트↔텍스트)     │  │
│  │ - A_t2s (텍스트→공간)       │  │
│  │ - A_s2t (공간→텍스트)       │  │
│  │ - A_s2s (공간↔공간) ⭐      │  │
│  └─────────────────────────────┘  │
│  ┌─────────────────────────────┐  │
│  │ Feed Forward Network        │  │
│  └─────────────────────────────┘  │
└─────────────────────────────────────┘
           ↓
┌─────────────────────────────────────┐
│  Output Layer                       │
│  - Structured JSON                  │
│  - Q&A Response                     │
│  - Extracted Information            │
└─────────────────────────────────────┘
```


1. **분리된 표현**: 텍스트와 위치를 **별도 벡터**로 유지
2. **4-way Attention**: T2T, T2S, S2T, S2S를 **동시에 계산**
3. **교차 모달 학습**: 텍스트 의미 + 공간 구조를 **함께 학습**
4. **경량화**: 이미지 인코더 없이 **바운딩 박스만** 사용
