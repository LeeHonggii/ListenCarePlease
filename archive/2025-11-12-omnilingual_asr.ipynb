{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40182e231b1545818139f9c3b2b7924f": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_da5da059c6d44833a982b689c152dbbb",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "parameter load: \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2199/2199\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">parameter load: <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\"> 2199/2199</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "da5da059c6d44833a982b689c152dbbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. 현재 설치된 PyTorch, TorchAudio 모두 제거\n",
        "!pip uninstall torch torchaudio -y\n",
        "print(\"--- 기존 PyTorch/TorchAudio 제거 완료 ---\")\n",
        "\n",
        "# 2. PyTorch와 TorchAudio를 'CUDA 12.1' 버전으로 강제 재설치\n",
        "!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "print(\"--- PyTorch/TorchAudio (CUDA 12.1) 설치 완료 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvkEc1OJhwuJ",
        "outputId": "85699974-e9ae-47c4-c2ac-0cb4fc0327a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "--- 기존 PyTorch/TorchAudio 제거 완료 ---\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 triton-3.1.0\n",
            "--- PyTorch/TorchAudio (CUDA 12.1) 설치 완료 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. [핵심] fairseq2의 'CUDA 12.1' 버전을 설치\n",
        "!pip install \"fairseq2[cuda-12.1]\"\n",
        "print(\"--- fairseq2 (CUDA 12.1 variant) 설치 완료 ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_MZLgR3ni1sB",
        "outputId": "460587e2-e1ab-4c56-ea80-e36f1673b2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq2[cuda-12.1]\n",
            "  Downloading fairseq2-0.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "\u001b[33mWARNING: fairseq2 0.7.0 does not provide the extra 'cuda-12-1'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting clusterscope~=0.0.31 (from fairseq2[cuda-12.1])\n",
            "  Downloading clusterscope-0.0.31-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: editdistance~=0.8 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (0.8.1)\n",
            "Collecting fairseq2n==0.7.0 (from fairseq2[cuda-12.1])\n",
            "  Downloading fairseq2n-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: huggingface_hub~=0.32 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (0.36.0)\n",
            "Collecting importlib_metadata~=7.0 (from fairseq2[cuda-12.1])\n",
            "  Downloading importlib_metadata-7.2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: importlib_resources~=6.4 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (6.5.2)\n",
            "Collecting mypy-extensions~=1.0 (from fairseq2[cuda-12.1])\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting numpy~=1.23 (from fairseq2[cuda-12.1])\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging~=24.1 (from fairseq2[cuda-12.1])\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: psutil~=5.9 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (5.9.5)\n",
            "Collecting ruamel.yaml~=0.18 (from fairseq2[cuda-12.1])\n",
            "  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: rich~=13.7 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (13.9.4)\n",
            "Collecting sacrebleu~=2.4 (from fairseq2[cuda-12.1])\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors~=0.6 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (0.6.2)\n",
            "Requirement already satisfied: tiktoken~=0.7 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (0.12.0)\n",
            "Collecting torcheval~=0.0.6 (from fairseq2[cuda-12.1])\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: typing_extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from fairseq2[cuda-12.1]) (4.15.0)\n",
            "Collecting blobfile~=3.0.0 (from fairseq2[cuda-12.1])\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tbb>=2021.8 in /usr/local/lib/python3.12/dist-packages (from fairseq2n==0.7.0->fairseq2[cuda-12.1]) (2022.3.0)\n",
            "Collecting torch==2.9.0 (from fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (75.2.0)\n",
            "Collecting sympy>=1.13.3 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.0 (from torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1])\n",
            "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.12/dist-packages (from blobfile~=3.0.0->fairseq2[cuda-12.1]) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from blobfile~=3.0.0->fairseq2[cuda-12.1]) (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.12/dist-packages (from blobfile~=3.0.0->fairseq2[cuda-12.1]) (5.4.0)\n",
            "Collecting click!=8.3.0,>=8.0.0 (from clusterscope~=0.0.31->fairseq2[cuda-12.1])\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting click-option-group (from clusterscope~=0.0.31->fairseq2[cuda-12.1])\n",
            "  Downloading click_option_group-0.5.9-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2[cuda-12.1]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2[cuda-12.1]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2[cuda-12.1]) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2[cuda-12.1]) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata~=7.0->fairseq2[cuda-12.1]) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich~=13.7->fairseq2[cuda-12.1]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich~=13.7->fairseq2[cuda-12.1]) (2.19.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml~=0.18->fairseq2[cuda-12.1])\n",
            "  Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting portalocker (from sacrebleu~=2.4->fairseq2[cuda-12.1])\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu~=2.4->fairseq2[cuda-12.1]) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu~=2.4->fairseq2[cuda-12.1]) (0.9.0)\n",
            "Collecting colorama (from sacrebleu~=2.4->fairseq2[cuda-12.1])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich~=13.7->fairseq2[cuda-12.1]) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub~=0.32->fairseq2[cuda-12.1]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub~=0.32->fairseq2[cuda-12.1]) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub~=0.32->fairseq2[cuda-12.1]) (2025.10.5)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.12/dist-packages (from tbb>=2021.8->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (1.4.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->fairseq2n==0.7.0->fairseq2[cuda-12.1]) (3.0.3)\n",
            "Downloading fairseq2n-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clusterscope-0.0.31-py3-none-any.whl (22 kB)\n",
            "Downloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fairseq2-0.7.0-py3-none-any.whl (554 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.0/554.0 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (753 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_option_group-0.5.9-py3-none-any.whl (11 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, torcheval, sympy, ruamel.yaml.clib, portalocker, packaging, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mypy-extensions, importlib_metadata, colorama, click, blobfile, sacrebleu, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, click-option-group, nvidia-cusolver-cu12, clusterscope, torch, fairseq2n, fairseq2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvshmem-cu12\n",
            "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
            "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
            "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "  Attempting uninstall: blobfile\n",
            "    Found existing installation: blobfile 3.1.0\n",
            "    Uninstalling blobfile-3.1.0:\n",
            "      Successfully uninstalled blobfile-3.1.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.9.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.9.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blobfile-3.0.0 click-8.2.1 click-option-group-0.5.9 clusterscope-0.0.31 colorama-0.4.6 fairseq2-0.7.0 fairseq2n-0.7.0 importlib_metadata-7.2.1 mypy-extensions-1.1.0 numpy-1.26.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 packaging-24.2 portalocker-3.2.0 ruamel.yaml-0.18.16 ruamel.yaml.clib-0.2.14 sacrebleu-2.5.1 sympy-1.14.0 torch-2.9.0 torcheval-0.0.7 triton-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "packaging"
                ]
              },
              "id": "186be60414f5444ca1efb15036cadd4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- fairseq2 (CUDA 12.1 variant) 설치 완료 ---\n",
            "Collecting git+https://github.com/facebookresearch/omnilingual-asr.git\n",
            "  Cloning https://github.com/facebookresearch/omnilingual-asr.git to /tmp/pip-req-build-xx0vm4c5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/omnilingual-asr.git /tmp/pip-req-build-xx0vm4c5\n",
            "  Resolved https://github.com/facebookresearch/omnilingual-asr.git to commit a7fb36017a46eee8953f76bd628c174d51aefeef\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "--- omnilingual-asr 설치 완료 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. omnilingual-asr 설치\n",
        "!pip install git+https://github.com/facebookresearch/omnilingual-asr.git\n",
        "print(\"--- omnilingual-asr 설치 완료 ---\")\n",
        "\n",
        "# 4. 오디오 라이브러리 설치\n",
        "!pip install librosa soundfile\n",
        "print(\"--- 오디오 라이브러리 설치 완료 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAQB5mUnkwmI",
        "outputId": "dacf56a7-a0a3-4745-8739-646fa25fc4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/omnilingual-asr.git\n",
            "  Cloning https://github.com/facebookresearch/omnilingual-asr.git to /tmp/pip-req-build-nw9wyvgs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/omnilingual-asr.git /tmp/pip-req-build-nw9wyvgs\n",
            "  Resolved https://github.com/facebookresearch/omnilingual-asr.git to commit a7fb36017a46eee8953f76bd628c174d51aefeef\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fairseq2<=0.6,>=0.5.2 (from fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0)\n",
            "  Downloading fairseq2-0.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pyarrow>=20.0.0 (from omnilingual-asr==0.1.0)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from omnilingual-asr==0.1.0) (2.9.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from omnilingual-asr==0.1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from omnilingual-asr==0.1.0) (0.60.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from omnilingual-asr==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from omnilingual-asr==0.1.0) (1.26.4)\n",
            "Collecting kenlm (from omnilingual-asr==0.1.0)\n",
            "  Downloading kenlm-0.3.0.tar.gz (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.5/427.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting polars>=1.29.0 (from omnilingual-asr==0.1.0)\n",
            "  Downloading polars-1.35.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: editdistance~=0.8 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.8.1)\n",
            "Collecting fairseq2n==0.6 (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0)\n",
            "  Downloading fairseq2n-0.6-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: huggingface_hub~=0.32 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.36.0)\n",
            "Requirement already satisfied: importlib_metadata~=7.0 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (7.2.1)\n",
            "Requirement already satisfied: importlib_resources~=6.4 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (6.5.2)\n",
            "Requirement already satisfied: mypy-extensions~=1.0 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: packaging~=24.1 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (24.2)\n",
            "Requirement already satisfied: psutil~=5.9 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: ruamel.yaml~=0.18 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.18.16)\n",
            "Requirement already satisfied: rich~=13.7 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: sacrebleu~=2.4 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2.5.1)\n",
            "Requirement already satisfied: safetensors~=0.6 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.6.2)\n",
            "Requirement already satisfied: tiktoken~=0.7 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.12.0)\n",
            "Requirement already satisfied: torcheval~=0.0.6 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.0.7)\n",
            "Requirement already satisfied: tqdm~=4.62 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: blobfile~=3.0.0 in /usr/local/lib/python3.12/dist-packages (from fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: tbb>=2021.8 in /usr/local/lib/python3.12/dist-packages (from fairseq2n==0.6->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2022.3.0)\n",
            "Collecting torch (from omnilingual-asr==0.1.0)\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->omnilingual-asr==0.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->omnilingual-asr==0.1.0) (1.13.1.3)\n",
            "Collecting triton==3.4.0 (from torch->omnilingual-asr==0.1.0)\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting retrying~=1.3.4 (from fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0)\n",
            "  Downloading retrying-1.3.7-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: xxhash~=3.5 in /usr/local/lib/python3.12/dist-packages (from fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.6.0)\n",
            "Collecting polars-runtime-32==1.35.2 (from polars>=1.29.0->omnilingual-asr==0.1.0)\n",
            "  Downloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->omnilingual-asr==0.1.0) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->omnilingual-asr==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->omnilingual-asr==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->omnilingual-asr==0.1.0) (2025.2)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio (from omnilingual-asr==0.1.0)\n",
            "  Downloading torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.12/dist-packages (from blobfile~=3.0.0->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.23.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from blobfile~=3.0.0->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.12/dist-packages (from blobfile~=3.0.0->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (5.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub~=0.32->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata~=7.0->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->omnilingual-asr==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich~=13.7->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich~=13.7->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2.19.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.12/dist-packages (from ruamel.yaml~=0.18->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.2.14)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu~=2.4->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu~=2.4->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu~=2.4->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu~=2.4->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.4.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->omnilingual-asr==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->omnilingual-asr==0.1.0) (3.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich~=13.7->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub~=0.32->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub~=0.32->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub~=0.32->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (2025.10.5)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.12/dist-packages (from tbb>=2021.8->fairseq2n==0.6->fairseq2<=0.6,>=0.5.2->fairseq2[arrow]<=0.6,>=0.5.2->omnilingual-asr==0.1.0) (1.4.1)\n",
            "Downloading fairseq2-0.6-py3-none-any.whl (548 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.8/548.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fairseq2n-0.6-cp312-cp312-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polars-1.35.2-py3-none-any.whl (783 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m783.6/783.6 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.7-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: omnilingual-asr, kenlm\n",
            "  Building wheel for omnilingual-asr (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for omnilingual-asr: filename=omnilingual_asr-0.1.0-py3-none-any.whl size=82001 sha256=b7e19a90cd479588c6849312cfc8274c437784cca53a91497355a2c360bfbc92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_ymdnv94/wheels/06/39/d6/af04da80bc3d37769d5e80a19c3cf5558607435dd08e30fb44\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.3.0-cp312-cp312-linux_x86_64.whl size=3188056 sha256=682c2a494aa896c1ffa0c49e8a75bca219adacea6697e94561b729e2f40f95e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/e6/ad/18d2d3f1290a6be6a14a24e90f2b78bb3300aab3852ceb06a6\n",
            "Successfully built omnilingual-asr kenlm\n",
            "Installing collected packages: retrying, kenlm, triton, pyarrow, polars-runtime-32, nvidia-nccl-cu12, polars, torch, torchaudio, fairseq2n, fairseq2, omnilingual-asr\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: polars\n",
            "    Found existing installation: polars 1.25.2\n",
            "    Uninstalling polars-1.25.2:\n",
            "      Successfully uninstalled polars-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0\n",
            "    Uninstalling torch-2.9.0:\n",
            "      Successfully uninstalled torch-2.9.0\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu121\n",
            "    Uninstalling torchaudio-2.5.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "  Attempting uninstall: fairseq2n\n",
            "    Found existing installation: fairseq2n 0.7.0\n",
            "    Uninstalling fairseq2n-0.7.0:\n",
            "      Successfully uninstalled fairseq2n-0.7.0\n",
            "  Attempting uninstall: fairseq2\n",
            "    Found existing installation: fairseq2 0.7.0\n",
            "    Uninstalling fairseq2-0.7.0:\n",
            "      Successfully uninstalled fairseq2-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-polars-cu12 25.6.0 requires polars<1.29,>=1.25, but you have polars 1.35.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fairseq2-0.6 fairseq2n-0.6 kenlm-0.3.0 nvidia-nccl-cu12-2.27.3 omnilingual-asr-0.1.0 polars-1.35.2 polars-runtime-32-1.35.2 pyarrow-22.0.0 retrying-1.3.7 torch-2.8.0 torchaudio-2.8.0 triton-3.4.0\n",
            "--- omnilingual-asr 설치 완료 ---\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n",
            "--- 오디오 라이브러리 설치 완료 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n",
        "import torch\n",
        "\n",
        "# 장치 설정\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"사용 장치: {device}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# ❗ 모델 카드 (7B 모델)\n",
        "# (메모리 부족 시 'omniASR_LLM_1B'로 변경 시도)\n",
        "# ----------------------------------------------------\n",
        "model_card = \"omniASR_LLM_7B\"\n",
        "# model_card = \"omniASR_LLM_1B\" # <--- 7B 모델 메모리 부족 시 대안\n",
        "\n",
        "print(f\"'{model_card}' 모델 로드 중... (시간이 매우 오래 걸릴 수 있습니다)\")\n",
        "\n",
        "# 님께서 찾아오신 GitHub의 정식 파이프라인 클래스를 사용합니다.\n",
        "asr_pipeline = ASRInferencePipeline(\n",
        "    model_card=model_card,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"파이프라인 로드 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "40182e231b1545818139f9c3b2b7924f",
            "da5da059c6d44833a982b689c152dbbb"
          ]
        },
        "id": "0SFNPwgZf_L0",
        "outputId": "944411fa-8f78-4434-af63-b8801c53cadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용 장치: cuda:0\n",
            "'omniASR_LLM_7B' 모델 로드 중... (시간이 매우 오래 걸릴 수 있습니다)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.1G/29.1G [01:59<00:00, 261MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40182e231b1545818139f9c3b2b7924f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85.6k/85.6k [00:00<00:00, 37.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파이프라인 로드 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf # .wav 파일 저장을 위해 import\n",
        "import os\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. 원본 M4A 파일 경로\n",
        "# (파일이 /content/ 에 다시 업로드되었는지 확인해주세요)\n",
        "# ----------------------------------------------------\n",
        "M4A_FILE = \"/content/1106_오후회의.m4a\"\n",
        "\n",
        "# 2. 새로 저장할 WAV 파일 경로\n",
        "WAV_FILE = \"/content/1106_오후회의.wav\"\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(f\"오디오 변환 시작: {M4A_FILE} -> {WAV_FILE}\")\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(M4A_FILE):\n",
        "        print(f\"오류: {M4A_FILE}을 찾을 수 없습니다. 파일을 다시 업로드해주세요.\")\n",
        "    else:\n",
        "        # librosa로 M4A 로드 (16kHz, 모노로 강제 변환)\n",
        "        speech_array, sampling_rate = librosa.load(M4A_FILE, sr=16000, mono=True)\n",
        "\n",
        "        # soundfile을 사용해 16kHz WAV 파일로 저장\n",
        "        sf.write(WAV_FILE, speech_array, 16000)\n",
        "\n",
        "        print(f\"변환 및 저장 완료: {WAV_FILE} ({len(speech_array)/sampling_rate:.2f} 초)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오디오 변환 중 오류 발생: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG8InqKlmnFT",
        "outputId": "bf0bb3d0-f7d3-4329-bbc2-1fe0edc9934a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "오디오 변환 시작: /content/1106_오후회의.m4a -> /content/1106_오후회의.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2303810652.py:22: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  speech_array, sampling_rate = librosa.load(M4A_FILE, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "변환 및 저장 완료: /content/1106_오후회의.wav (2086.03 초)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "import math\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 설정\n",
        "# ----------------------------------------------------\n",
        "# 1. 원본 WAV 파일\n",
        "WAV_FILE = \"/content/1106_오후회의.wav\"\n",
        "# 2. 모델이 허용하는 최대 시간 (안전하게 39초로 설정)\n",
        "CHUNK_SEC = 39\n",
        "# 3. 샘플링 레이트\n",
        "SAMPLING_RATE = 16000\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(\"오디오 파일 로드 및 분할 시작...\")\n",
        "\n",
        "# 1. 긴 WAV 파일 로드\n",
        "try:\n",
        "    speech_array, sr = librosa.load(WAV_FILE, sr=SAMPLING_RATE)\n",
        "    print(f\"'{WAV_FILE}' 로드 완료. (총 {len(speech_array)/sr:.2f} 초)\")\n",
        "\n",
        "    # 2. 청크 단위로 자르기\n",
        "    chunk_len_samples = CHUNK_SEC * SAMPLING_RATE\n",
        "    total_samples = len(speech_array)\n",
        "    num_chunks = math.ceil(total_samples / chunk_len_samples)\n",
        "\n",
        "    chunk_files = [] # 분할된 파일 경로 리스트\n",
        "    chunk_langs = [] # 파일 개수만큼 언어 코드 리스트 생성\n",
        "\n",
        "    print(f\"{CHUNK_SEC}초 단위로 총 {num_chunks}개의 파일 생성 중...\")\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_sample = i * chunk_len_samples\n",
        "        end_sample = start_sample + chunk_len_samples\n",
        "\n",
        "        chunk_audio = speech_array[start_sample:end_sample]\n",
        "\n",
        "        # 파일 이름 (예: /content/chunk_0001.wav)\n",
        "        chunk_filename = f\"/content/chunk_{i:04d}.wav\"\n",
        "\n",
        "        # 청크 파일 저장\n",
        "        sf.write(chunk_filename, chunk_audio, SAMPLING_RATE)\n",
        "\n",
        "        chunk_files.append(chunk_filename)\n",
        "        chunk_langs.append(\"kor_Hang\") # 각 파일에 \"kor_Hang\" 지정\n",
        "\n",
        "    print(f\"총 {len(chunk_files)}개의 청크 파일 생성 완료.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오디오 분할 중 오류 발생: {e}\")\n",
        "    chunk_files = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNNJy02Hm-Wb",
        "outputId": "7142a0ba-3eb8-4016-eb63-2ad8688434ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "오디오 파일 로드 및 분할 시작...\n",
            "'/content/1106_오후회의.wav' 로드 완료. (총 2086.03 초)\n",
            "39초 단위로 총 54개의 파일 생성 중...\n",
            "총 54개의 청크 파일 생성 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if chunk_files:\n",
        "    print(f\"총 {len(chunk_files)}개의 청크에 대해 STT 변환 시작...\")\n",
        "\n",
        "    # VRAM 크기에 따라 batch_size 조절 (예: 4 또는 8)\n",
        "    # 7B 모델이므로 메모리 부족(OOM) 시 1로 줄여야 할 수 있습니다.\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    try:\n",
        "        # [핵심] 파일 리스트와 언어 리스트를 전달\n",
        "        transcriptions_list = asr_pipeline.transcribe(\n",
        "            chunk_files,\n",
        "            lang=chunk_langs,\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "        print(\"STT 변환 완료.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"STT 실행 중 오류 발생: {e}\")\n",
        "        transcriptions_list = None\n",
        "else:\n",
        "    print(\"STT를 실행할 청크 파일이 없습니다 (3.5단계 오류 확인).\")\n",
        "    transcriptions_list = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgF0Udbfc855",
        "outputId": "f74d9b1a-5272-4782-9c1b-124c80e93d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 54개의 청크에 대해 STT 변환 시작...\n",
            "STT 변환 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if transcriptions_list:\n",
        "    print(\"\\n[ 최종 변환 텍스트 (취합본) ]\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 모든 청크의 텍스트를 공백으로 연결\n",
        "    full_text = \" \".join(transcriptions_list)\n",
        "    print(full_text)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"표시할 STT 결과가 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNLCCyPqdkOY",
        "outputId": "aa400df3-e7a9-4278-dc3f-1bebbf778c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ 최종 변환 텍스트 (취합본) ]\n",
            "==================================================\n",
            "그러면 너무 회의만하지 말고 좀 토 ⁇  어봐서 해볼까요 여유롭게 처음에 이 아이트 브레이 ⁇  그러면 지금 누는 거예요 자 제 오는 만하세요 방법 강제도 이십분 상태가 다 와 왔습니다 강제구요 감배서도 약 편이 좀 갔다가 신분 좀 없다고 빠꾸 먹어가지고 여기 칼국수티 쪽 편이 좀 갔다가 거기서도 빠꾸 맞아가지고 왜인지 안 올라오전하죠 저 반대쪽 편이 좀 가서 거기서 겨보사왔어요 고사 없는 같게네 그럴 이 없는데 네 예가 예가 그랬어요 그 정말까 어 이상 할 말이 없어요 아니면 중간중간에 그럼 계속 말을 해봐요 우리가 하는 것처럼 언제든지 치고 들어와봐요 그럼 진행해 주시죠 아 나오 주승에서 지금 이거 뭐지 구글 구글 클라우드 에스 시티 라는 걸 처음 저는 그냥 이게 어려운 거죠 말하죠 어려운 다섯 에스 엠블 에이 아이 스틴데 그걸 저는 이거 지금 구글 클라우드랑 구글 소리지 맞나요 이거 처음소 봐요 그 수 지금 약간 매우 같은데 그리고 또 안 돼 신이죠 약간 좀 어디 단계에서 막혔어요 그래가지고 약간 이게 막혔다는 게 제 말은 에스티 에서 막히는 게 아니라 구글 클라우드 그 허가 접근 권하에서 지금 에먼드에서 막혀가지고 지금 좀 어 그 정체 상태구요 근데 해보고 싶긴 해요 그래서 오늘 집에 가서라도 해 볼게요 네 그런데 어제 지금 너무 저는 그리고 마음은 편하게 다행기도 우리 팀원분들이 이 어 ⁇ 블리 에이아 해보셨고 또 지금 리모 엔비디 아이 리모 게 해보셨고 스피치 블레이너 그리는 어 ⁇ 어요 해보겠는데 빠르게 진짜 빠르거든요 이제 십오분자리 파일이면은 한 그 시간 스템프가 영  ⁇  삼 초 사 영  ⁇  오  ⁇  원에 짜락 찍혀요 근데 결과가 안 좋아요 어떤 느낌이 어떻게요 어 어떤 느낌이 어 열 개요 어 아 중간 운축구나 그래서 이제 그러면 지금 리무랑 어 그니까 스피치 브레인 말씀하셨다 네 좀 더 말씀해 주실 수 있어요 음 성능이 안 좋아요 그니까 이게 처음에는 화다가 두 명으로 나와가지고 그거를 이제 좀 더 수정해서 성능을 올렸는데 화자를 네 명까지 뽑아냈거든요 네 문제는 그다음에 이제 미스퍼랑에서 맞춰보니까 그 문장을 이삭해 잡더라고요 각각화자를 문장에 대한 걸 그래서 이거 성능이 그게 좋다고 생각을 안 해서 지금 이 엔 디 쪽으로 넘어갈려고 주문이 좀 입니다 어 아 그걸로 오후 좋습니다 이 엔 디 가 지금 그 제가 올린 이스트에 있는데 이 엔 디 라고 이 엔 디 패밀리라고 있어요 나 새로 모델 해 주실 수 있는데 그리고 저는 구글 클라우드 지금 그 ჷ 아날 생각하는 값이 진 게 스피치 브레인이 무슨 뭐 몇 초 마이 딱 되는 됐잖아요 이것이 몇 일 십오 분 자리라서 대용량으로 지급해 가지고 또 막 돈 내고 써야 되는가 지금은 무료해요 한 삼백 달러 자리 충전했을 때 예내가 처음 쓰는다고 아 에이피 이 아 아 그 그 그 별로부터 다르잖아요 이게 그 편리함과 어떤 이 속도가 십오분 사실 녹지 파일 십오분 아니고도 많지 않은 사십 분 자리도 있고 그런데 십오분이 너무 이제 큰 파일이라고 벌써 이렇게 딱 어떤 허드를 있는 것 보니까 좀 구글 클라우드 에스티트 좀 벌써 약간 전 전 이 일 상실하고 있어요 그렇고 어 이제 니모는 좀 어때였어요 굉장히 잘 잡는 건지는 모르겠는데 잡고 있고 하면서 느낀 건 그냥 단순히 모델에 넣고 화자를 분리한다고만 했을 때 속능은 당연히 안 나올 것 같다 라는 점 드려요 아 좀 어떤 증언에서 그렇게 느끼셨어요 일단은 오디오에 대한 전철이를 할 때마다 결과 값이 너무 많이 달라이고 아 전철이 할 때마다 그냥 넣는 거랑이 보다는 다들 한 번씩 검색해 봤을 때 한 블로그 하나 나오지 않았나요 그 뭐가 어떻게 전철이해서 상담원을 뭐 시키했다 그 블로그를 저는 같아 수 있는데 어느 정도의 전철이는 좀 해줘야 되지 않나 어 저는 지금 음성을 올렸고 노이즈 좀 제거해서 진행했을 때 네 명까지 잡거든요 한 명의 일도가 안 잡히는데 그 괜찮지 않을 거예요 아 그러니까 알고리즘을 가격게라도 어 어 고객님 그 그때부터 이제 지금은 우려 직접 하는 거지만 그때부터 이제 바로 어떤 파이트로 보면 전철이부터 거친다는 말씀인 거죠 네네 어 고객님 그리고 사자 불리 일 분 영도 걸려줘 그리고 가장 올리 일 분 영도 걸려줘 그리고 그리고 어 ⁇ 블리 에이 아이는 어떠셨어요 저는 이거 동작 자체는 어제 에이티 아이키 그런 거 바로 하는 것 사전자국으로 치면은 하는 수준호 자체는 이십팔 초 걸렸어요 이 일단 하자는 다섯 명 다 잡았고 내용도 그렇게 나쁘진 않은데 이게 저도 아까 말씀하신 것처럼 비용이 나 봤는데 이게 사이트 가서 보니까 오십 달러 충전되는 것 같더라고요 아키본 그래스티 그래서 이거를 계속 가도 될는지를 물어겠어요 그냥 이게 에스티티나 그런 것도 어  ⁇ 블리 자체에서 다 되는 거예서 따로 보내 쓰는 게 아니라 그리고 그 오십 달러도 그 충분히 많이 쓸 수 있어요 아니면 그 다 들고 같아요 지금 두 번 둘렸을 때 영  ⁇  일 육 썼어요 괜찮으면요 아 네 이게 치고 보자 이자 두 번 돌리고 전화 좀 돌리고 전화 저희가 돈을 내고 썼다라는 게 프로 ⁇ 트에서 조금  ⁇  걸립돌이 되죠 네 아 너 내가 좀 더 능동적으로 고객님 게 아니라 아 아 저 다 썼구나 어 어 그게 또 그렇게 되는가 그래서 이게 좀 걸려줘 그게 그게 그래서 이게 좀 걸려줘 그게 아 그게 또 그렇게 되는 그래서 이게 좀 걸려줘 어 지금 벌써 장단 좀 나오고 있고요 잘  ⁇ 코 어때였어요 어 먼저 어  ⁇ 코를 봤는데 제가 이  ⁇ 에서 그 모델을 봤을 때 그 파이어노트를 활용한 음용한 모델이라고 판단이 됐고요 왜냐면 그 모델 설명 안에 파이어노트 내용이 있어가지고 어 파이어노트를 좀 개인이 좀 만진 게 아닌가 그럼 좀 발 좀 속도를 치중한 데 모델인 인 것 같고요 음 그리고 예능 이제 그냥 기세서 모델 받아가지고 인스 인스토'를 하면은 바로 쓸 수 있는 모델이고요 그리고 결과를 말씀드리자면 십오 분 했을 때 일단 엘포 기준으로는 동작이 잠시만요 네 제 알 칠  ⁇  칠 팔 초 정도 고객님 다음에 티포가 한 이십오 총가 십오 총가 이 종 이십오 초 정도 되어 십오 초 정도 되어 십오 초 어 십오 초 대가 이십오 초 그 다음 하는 네 비 어 그 다음 화장하는 네 명을 분류하는 걸 확인할 수 있었습니다 그런데 이제 여기서는 이제 포인트가 제가 한데까지 말씀드리자면 그 로 세그먼트라고 해서 결과값을 받아볼 수 있는데 여기에서 이제 그 리스트랑  ⁇ 셔널이 구조로 돼 있어요 그때 뭐 스피커 스타트 엔드 이렇게 돼 있어가지고 스피커와 바라자와 시작 네 끝난 시간 이런 정보를 받아낼 수 있고요 이제 티스 티에스 기능을 따로 제공하지 않아서 위스퍼를 이용해서 그니까 택스트 대화 내용을 얻어 내려면 위스퍼를 이용해서 이 스타트와 엔드 사용해서 어 텍스트를 받는 과정을 거쳐야 되지 않을까 라고 생각을 합니다 티스 사는 에스티티 만약에 맞아요 그리고 한국어에서 아직은 시작할 수 있다 중국어 만달인 너 기준으로 좀 더 출발했다 연이 있던데 지금 괜찮은 거 같아요 어 실제 어 일단은 그 테스트 시간 기준으로 된 거고 그거 그걸 봐야 하지 않기가 정확한 건 이제 잘 연한 거 그 기장을 잘 살 때 잘 되는 좀 확인을 해야 될 거 같고요 세그먼트 수를 추려받는데 그냥 각각 말씀드리겠습니다 공일번 바라자가 백육번 이번 바라자 오십번 삼번 바라자 칠십육번 사번 바라자 이렇게 나누 있는 걸 확인할 수 있었고요 그 언어 특화는 사실은 이제 좀 그 예는 그냥 사운드만 듣고 불리를 하는 거잖아요 그런 스티티가 아니다 보니까 사실 그 이런 스피커 다이어라이제이션 모델 자체가 좀 언어에 좀 물론 이제 학습된 모델에서 어 학습된 원어에서는 더 좋은 성능을 내겠지만 개인적은 생각으로는 좀 언어 다양성에서 제 자유롭지 않은 아니다 자유롭지 않은 아니까 자유롭다 지금한 이런 이 이 나 이 떨어져 같긴다고 해서 상상이 그 가까이 떨어지고 이르지 않는 거 저도 똑같이 생각하고 있어요 어쩌다 이 되어 이 우리나요 그 그래도 동양이랑 그래도 서양이랑 좀 다 해서 다 그러면 그런 거 이런 동양이 그 어쩌이 그럼 그런데 이 이 너무 내부에서 각 특징을 뽑아 내서 사람을 구분한 거일 거기 때문에 그럼 어쩌면 이건 정말 뭐 재형님 뭐라 한 개 아니라 어쩌면 내게로 분류된 게 어떻게 되고 그냥 원래 지금 사항은 내게로 분류된 게 어 ⁇ 서 없나 봐요 그거 제휴가 하는 게 하는 게 하는 게 그거 지금 마지고 그 그 하는 게 한번 그러면 지금 예는 다 문제 없는 거잖아요 그러면 지금 예는 다 문제 없는 거잖아요 그 전에 그리고 그니까 이거 지금 이 ኤ 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 나 이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이 이월 이십일 일 오 일 오 일 오 일 오 시 사십일 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 일 오 이 되어 수준 수 있는 수 있는 그리고 아니다 이  ⁇  거예요 이런 이  ⁇  이  ⁇  공지 공지  ⁇ 감한 듣기 좋은 소음에 없는 사실 지금 어떤 옷 수치는 소리도 안 들리 위해 굉장히 지금 독립적으로 잘 예동에 서 있고 그니까 약간  ⁇ 렌지만 어떤 음원도 한번 해보 해봐서 지금 들어보니까 니모 스피치브랜 어 ⁇ 블리 에아이랑  ⁇ 고 다 괜찮거든요 네게 예매가 갑자기 좀 확 나니도 있는 거 되면 아니면 심지어 그런 게 아니라도 겹칠 때 대화가 어 그런 거 예를 이게 예를 그냥 뭐 예시에요 구 부싸운 상황 같이 서로 안 듣고 계속 자기 말만 하는 그런 상황에서도 뭐 당연히 퍼포먼스 떨어지겠지만 덜 떨어지는 애 같은 거 출연히 그런 것 같은 거 수도 없고 같고 사실 다 떠나서 지금 홍대님이 니모를 또 하는 것이라는 걸 두 보고 내가 깨달은 게 홍대님의 의도가 아닐 수 있어요 제 분석에 의하면 니모를 넣으면 우리 포트폴리오에  ⁇  팬시해 전 그런 것이 아니다 이런 데 이런 뭐 이런 데 뭐 인천이 아니다 그거 도로 이 이 이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇   ⁇  네 네 어 가던 미크로 넘어지고 예를 막아처럼 그니까 그런 어떤 생활성국이 충분히 가능한 밥 먹는 사항으로 해 본 좋을 것 같아요 네 그냥 그렇게 두 개 지금 이거하고 내일 잠시만 그거 두 개 해가지고 저는 내일 아침 스타복스 한 시간 짜리 녹음에 있게요 생활성 어 그냥 이제 생활 고객님 공절 사람은 아니다 아니까요 아 아니까요 그냥 이제 생활 고객님 고객님 공골 어 그리고 이제 백인분들도 많이 해야 뭐 그리고 이제 백인분들도 많이 해야 뭐 그리고 이야 나 오늘 경복궁 하고 어디 가 영어로 그 스타복스는 좁아서 어 제 바로 뒤에 시기 아  ⁇  있다는데 그러면 그 해서 물고 해서 하는 데 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 그러면 그 해서 이 한 번 해 보는 거 줘 그냥 파이프라임만 좀 짜왕이라도 그냥 그거는 첫 집 팔 티 하는 그런데요 그렇게 어려운 가 그거 어려운 가 그거 어려운 가 그거 어떻게 해야 해야 하나요 네 전다고 주말 그러니까 그거 어떻게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해서 하는 게 해 이런 거 있었어요 그런 거 없으면 지금 그런 거 없으면 그런 거 있었어요 아 그러니까 그리고 하면서 이제 매칭을 하는 거잖아요 그리고 저기가 굉장히 제가 바트는 지금 특이한 거 아니요 아니요 저도 그래요 이 이 오 드 그 그 그 이 이 오 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이 이  ⁇ 네요 또 우리 논의하고 있을까요 그러면 지금 제가 봤을 때 내일 정신까지 이렇게 제 것은 더 어제 이거 코- 크레디트니까 그냥 더 안 권드린게 맞는 거 아고요 네네네 그럼 제가 이제 붕 뜨니까 제가 그 원래 우리 내일까지 기획들을 써야 되잖아요 아 내일까지구나 그거를 제가 부틀게 그래서요 네 저도 해도 될 어 어 고객 그래요 저를 도와주시면 되어 아 네 그러니까 제가 지금 이제 서포트 에 컬로 들어갈게요 어 제가 하던 거는 이제 필요가 없어졌으니까 저도 지금 이거 구글 클라우드 안 할 것 같아요 지금 너무 딱 이월 이십일 일 오후 이 시간 차이도 너무 많이 하고 돈도 많이 들어요 어 그럼 제용님 쪽이나 아니면 저희가 상주님이 그 기획서를 작성하는 기획서 그래요 아 네 이 미있어 가지 네 개네 아 뭐 들어와서요 네 기획서도 하고 그 다음에 지금 어제 우리 다음 주 할 내 그 멘토링이니까 이거도 그 살짝 조금 손바도 괜찮을 것 같아요 조금 작성 어 네일 오전 중에 나누는 걸로 하죠 어 그러면 말씀해 주하죠 어 우리 모델은 이랬 네 모델은 이랬 네 모델은 이랬고 이러한 작동 원리 때문에 이렇게 됐다 음 그러니까 원리적인 얘기 다루자는 말씀이시죠 그니까 이미지도 필요하고 어 오케 오케 코드도 마찬가지고 네일 네일 오전의 회의가 우리가 각자 지금 해보신 거 좀 이제 약간 이론적인 어떤 기반을 찾아오자 일 것 같아요 그니까 막 거창한게 아니라 그냥 설명해 주라 이 말이요 모델이나 라이브러리 설명 그렇게 알겠습니다 그게 정리의 시장 또 투도에 올려놓게요 또 할게 있나요 그래서 파이프라인은 다들 어떻게 생각하셨나요 먼저 스타트를 꺼줘 주시죠 아 근데 예시 좀 이렇게 들어주셨어요 전체적으로 이제 서비스에 대해서 생각을 해야 되어요 저희가 이제 기억서도 작성을 해야 되어요 어 이제 서비스 얘기하자 이 말이요 아 파이프라인이 나올라면 어 서비스가 나올라면 파이프라인이 먼저 나와야 되니까 어 좀 먼저 나는게 맞는 것 같은데 일단 음성 음성 데이터가 들어올 거 아니요 저는 제 생각에는 음성 데이터를 나누고 이제 와 다 나누고 거기에 대해서 들어온 음성 텍스트를 보고 어떤 이름이 있는지를 한번 식별하고 그 식별된 이름을 택이기 해주는 거죠 누가 이거고 누가 이거고 이 과정이 제일 어려울 것 같은데요 그걸 좀 수행을 하고 그 저 저하게 누가 언제 무슨 말을 했는지 나오잖아요 그거에 대해서 이제 좀 요약이나 레그나 자막이나  ⁇ 프도 있으니까 그렇게 세 가지를 일단 진행해 하는 게 이런 이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇   ⁇  이  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  실시간은 그 근키면서 나오잖아요 그 정도 생각 되어요 저는 그렇게 어떻게 해주실 미있는 너무 어려우지 않은 이  ⁇  일단 이  ⁇  이  ⁇  이 되어 이렇게 이  ⁇  이 되어 이렇게 이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇  이  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  그 가지 말장이 서비스적인 측면으로 간다면 그 회의록 요약과 잠깐이 있거든요 네 자막이 그냥 마치 우리 넷플릭스 볼 때 이 더 누가 정말 대사를 한지 알 수 있게요 그러면 뭐가 저희가 이 이름 태킹한 것도 살릴 수 있는 그런 식게 보여줄 수 있는데 그러니까 야간 셰도인 수요 좀 많지나요 그래서 그런 어떤 그 스크릭 예전에는 제가 잘 십년 전에 공부할 때는 뭐 다들 그렇게 되셨겠지만 네이버 카페가 엄청 큰게 있었어요 이만 명 회원짜리 그거는 영화 영화 영화 스크립트  ⁇ 페야 그래서 그냥 다운 받는 게 아니고 이 이 분이 게시면 오리기 고수님이요 아 고수님 저 뭐 열두 악만 프라다 입는다 공부해 보고 싶어서 신청합니다 이메일을 보내서 감사하는 보내주고 보내주거든요 그 스크립트를 그런데 이게 공식이 알다보니까 이 고수가 잘해 저 고수가 잘해를 찾아가서 같은 악만 프라다 입니다 스크립트를 여러게 받는 거요 그니까 그 어쨌든 그만큼 이게 굉장히 그 중요한 자료고 필요한데 어떻게 어려운 것 맞거든요 이것이 기술이 좋아졌지만 그런데 어떻게 내 네 네 네 네 치료를 해주시 좋아졌지만 그런데 어떻게 네 네 이거는 굉장히 자동화를 해주니까 지금 민선님을 말한 것 저 어떤 미디어 자료에 대한 어떤 노 뉴스도 될 수 있고 예능 같은 경우에는 자막이 안 달리니까 어 그 점각 장이 예능 처- 처- 처- 체육 재미있을 때 그래서 홍 경이 청약하시스템 그래서 홍 경이 청약하였을 때 그 무한도전 영상 어 좋다 요구 그거에 오디오를 받아가지고 저희가 자막 아 자막 파일을 만드는 거죠 그 다음에 그리고 이  ⁇  만약에 그 전화 만약에 그런 다 고객님 한 마디 그래서 만약 그런 두 가지 서비스를 정말 딱 보여주셨으면 좀 좋네요 좀 평가 있을 거 같아요 네 그런데 이제 제가 공분한 건 바라중에 이름이 나오지 않으면 이거 어떻게 되는지 그런데 어떻게 보면 어 홍기님 아이디오 굉장히 여- 뭐 있고 예뻐서 그렇지 이미 그 힌트가 없어도 잘 나니는 거죠 다들 그 저 나니는데 이제 이름을 택이 일 이 누구나 이 되게 이름이 이 누구나 이 되게 이 되게 이 되게 이 되게 이 되게 이 되게 이 되게 이 되게 이 되게 해야 하나요 그냥 저도 그래서 딱 생각한 게 진짜 처음에 양 에이비씨로 좀 나오다가 그 지 우리처럼 이제 막 상주님 홍기님 막 그렇게 언급됐을 때부터 그 다음에 택이 되는 거 아니면 그거 자체를 그리고 네이 ⁇ 센스를 네이 ⁇ 센스를 아 네이 ⁇  해주는 거 있는데 이 이런 식으로 알아서 네이 ⁇  해주는 거 아 그런 식으로 그러니까 그 한 개인 정보 이슈도 없고 그 한 거 그리고 그게 아니라 약간 뭐 어 뭐 여기서 제일 인기가 많아 보이는 킹카로 추천된 남성 약간 이런 식으로 그렇게 이름이 그때 이제 되는 거 아 그것도 되고 그냥 같이 예를 들면 이상 준 삼십일 세 뭐 약간 이런 식으로 약간 그게 좀 그게 다시 더 네 어려울 거 그게 좀 그게 좀 그게 한 해서 수 있잖아요 어 내용 교약이잖아 어떻게 요신 부르게 해서 그 어떻게 그냥 스피커 고민 해서 그냥  ⁇ 카가 가 되고 그걸로만 바꾸면 끝 아니요 마저요 그 그 그게 보면 생각포다 이름을 안 부르는 대화도 많다는 말이 그 지금 어 그리고 오히려 이걸 약간 좀 재미 호설 넣어야 되는 지금 찾아요 어 필링하잖아요 그 만약에 그 방법이 너무 좀 어려우면 어려우면 그냥 그리고 있다고 그 지 그 지 먼저 시작하기 전 입력을 한다던가 그리고 그게 좀 그게 좀 마음에 걸리는 게 그게 나도 비 ⁇ 지 사용자가 우아로 그러면 그 ⁇ 지 그 ⁇  그 ⁇  아 그러면 네가 알려줄게 이렇게 되는 이구나 아 아 아 그러면 아 아니면 다 식별이 되고 식별 안 된 거는 저희가 웹 유아이에서 아니면 아니면 지적 이렇게 택이 해 주실 수 있게 그거 그것도 좋은 것 같아요 그니까 약간 사랑을 짝 되기처럼 그런 거죠 그냥 쉽게 그런데 그걸 하나 웹을 또 구현을 좀 박색해야 하지 아니면 저는 이런 사용 이거 약간 또 갑자기 해주시는 거죠 그리고 스트림 리스로 안 되는 그러니까 지금 여기서 회세고 듣고 있나요 그리까 약간 그 지금 우리 미디어 자료 했잖아요 무한도 정도 멤버들이 못 입고 있게 다 하고 하냐하 그럼 그 한다음에 뭐 이름이 나오긴 하지 마 거기는 이름 마음 하하였게 나왔지만 어쨌든 그 그러니까 뭐 그 뭐지 그 착하지만 멍청한 친구 뭐 이런 식으로 네이 ⁇ 을 작아줘 되고 아까 제가 바른 것처럼 그게 하는 그냥 회세 곳 입고 있는 사람은 이런 말 했으면 이렇게 해서 해서 그니까 시브이로 트레킹이 하지지 그니까 시브이로 트레킹이 하지지 그 그게 이 정도는 너무 너무 거차한가 그게 아이 편하세요 저 아까 그 무한 주는 얘기다 어떻게 아니 그냥 뭐 예를 좀 안경 쓴 매 ⁇ 이 달 분 사람 아까 이런 식으로 되지 않은 그 주 유재석이랑 아내주도 되고 이런 생각하게 했는데 너무 거창해지니까 말 안 했는데 아까 그래도 이런 흐름이면 그런 식으로 살짝만 또 건지어 되지 않나 그게 사람의 생각으로 나이 시운데 아 아 구현하는 컴퓨터의 생각으로 하면은 아니면 이거도 괜찮으시 수 있어요 그냥 회세 곳 입은 사람이라 이름을 추출하는 게 또 인베딩의 인베딩이거거려고 근데 열 때면 그냥 잠깐 그 회세 곳 입은 사람의 실루해서 그냥 하나 대표적인 사람  ⁇ 처 그냥 거의 다 몰리고 이 모티콘청 몰린다면 이 사람이 주고 이런 얘기했어요 아까 이런 식을 되지 않나요 자막처럼 아니면은 진짜 방 만나지죠 르그 같은 걸로 너무나 그런데 따이트 저장이 느면은 출연진 데만 아 해도고 바로 번호 입니다 해당 하나요 네 그 쓰잖은 건 저희가 아 네 다른 게 없어요 아 다른 게 없어요 아 다른 게 없어요 아 다른 게 없어요 아 아까 무슨 말했는데요 되게 그 그 그 해서 하는 거죠 이름 질 일 에 아니 그런데 유저 입지 수 천천히 그 네가 그 집 이렇게 한 거 예가 알아서 엔딩 크레티 추출해 제가 아니까 그 집 이렇게 한 거 예가 알아서 엔딩 크레티 추출해 제가 아니까 그런데 그거는 자막 서비스일 경우에게 있는 경우지 그 나 되는 거는 이 서비스  ⁇ 스를 조표하게요 네플릭스 영상 유 ⁇  아이를 쓰는데 화자를 제가 다 태긴하고 그니까 음성을 넣는다는 게 아이가 맞나요라고 제가 그리고 그냥 그리고 그냥 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 아 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 그리고 해 이 요 어야기든 그냥 이렇게 해도 근본 남은 거잖아요 그렇죠 그냥 그 딱 그거야 근데 그 결론적으로 그거든요 그 어차피 우리는 대본 같은 텍스트를 받는데 이 아내 이름을 태기하는 게 목표잖아요 그 제로 없어요 이름이 등장하지 않아요 이런 경우가 있으면 그 그냥 누가 한편 백육번만 하고 오십 그리고 갑자기 오십번 칠십육번 삼십칠 만 지은님 따 그랬잖아 백육번만 하는지 누가 말 했지 알겠는데 따지 이렇잖아 에이다이한테 너 내 알지 제일 말 많은 사람이 예아 그러면 그러면 그러면 그러면 그러면 그러면 그게 네 그리고 내 네 맞은 이미 그거 예가 알고 있잖아 말이 제일 많은 사람이다 어려분 이미 아시는 그 있으면 그리고 하는 그러면 그런 거는 뭐 그 때 그러면 해주 해 곳이잖아 하지 않은 하지 않은 하지 않은 하지 않은 하지 말고 하지 말고 그렇게 거창하게 빅 스 ⁇ 하지 말고 입력이나 프로그램 하지 말고 이 안에서 뭔가 아 왼지 어 그치 아니면 상식적으로 예내도 누가 제일 말 아니 누가 엠 씨인지 누가 행동대장인지 아니면 누가 가만히 상식적 전 안 되다 그거는 안 돼 따 이런 갑자 예 드리 알았을 수 안 된데 이렇고 그러니까 리더인지 다 알고 아니요 그렇고 너무 프로팔링 거창하긴 한데 그 좀 더 가볍게 할 수 있는 거 있지 않을까 어 그것도 그거 대로 좀 문제에요 왜냐면은 그럼 대화의 모든 문맥을 이해하는 친구가 하나가 생겨야 됐다는 것 같아요 그 그 그 전도 라마로 안 될까요 그 뭐 파이는 투신 나면 될 것 같은데 그러면 그럼 이제 추론이 안 되고 하는데 그런데 차라리 오픈 소스보다는 에이피아이 쓰는 게 나을 수도 있기네요 그 때 전에 에이피아가 언급 될 수 밖에 없구나 네 나쁘지 않은 생각인 거 같고 이름이 만약에 없었을 때 사원이 저희는 그래서 물론 가지도 중요하지만 바라져라 어떻게 택이 그런 거예요 좀 많이 신경을 써야 되어 저는 이게 되 그래서 처음 시작할 때 만약에 이게 되면 진짜 박수를 받을 정도다 음 어 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우 우  그리고 뭐 이런 어떤 미세한 테스크에 있어서 약간의 어떤 키그를 어떻게 넣을지 아니면 어떻게 쉽게 조금  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇  거  ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   ⁇   그 다음 근본적으로 우리 바라 저택이 되하지 그 다음 하는 거예요 가지수는 더 생각하지 말고 하나는 일단 어 어 어 어 어 아무리 그거 많이 해나도 바라 저택이 안되면 맞아요 아무리 그거 많이 해나도 바라 저택이 안되면 맞아요 지금 지금한 거구요 그래서 저희의 파이프라인은 나왔고 지금 기술만 생각하시면 되면 어떻게 할 것이 그런데 그래서 저희의 파이프라인은 나왔고 지금 기술만 생각하시면 되면 어떻게 할 것이 그 그 그리고 아니다 강사님께 그러면 아 네 강사님 가기 전에 좀 정리해야 하지 말씀이 아 네 아까 이렇게 들어가는 지역 한 거예요 어 되어요 오늘은 네시 원이 어 다섯시 바 주문 갈 다 주실 거예요 어 어 다섯시 바 주문 갈 가요 되어요\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-juC6RgduMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}