# Mock ë°ì´í„°ë¥¼ ì‹¤ì œ AI ëª¨ë¸ë¡œ êµì²´í•˜ê¸°

## ê°œìš”

í˜„ì¬ í”„ë¡œì íŠ¸ëŠ” **Phase 1 (ì›¹ ì¸í”„ë¼ êµ¬ì¶•)** ë‹¨ê³„ë¡œ, ëª¨ë“  AI ì²˜ë¦¬ ë¡œì§ì´ **Mock ë°ì´í„°**ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ì´ ë¬¸ì„œëŠ” Phase 2ë¡œ ì§„ì…í•  ë•Œ Mock ë°ì´í„°ë¥¼ ì‹¤ì œ AI ëª¨ë¸ë¡œ êµì²´í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ìµœì‹  ì—…ë°ì´íŠ¸ (2025-11-22)

**ì‹¤ì œ êµ¬í˜„ ì™„ë£Œ:**
- âœ… STT (Whisper - Local/API ëª¨ë“œ)
- âœ… Diarization (Senko / NeMo)
- âœ… ì „ì²˜ë¦¬ (VAD, ë…¸ì´ì¦ˆ ì œê±°, ì²­í¬ ë¶„í• )
- âœ… NER (ì´ë¦„ ê°ì§€)
- âœ… í™”ì-ì´ë¦„ ë§¤ì¹­

**ì¶”ê°€ ì˜ˆì •:**
- ğŸ¯ **RAG ì‹œìŠ¤í…œ** (ì‹¤ì œ êµ¬í˜„)
- ğŸ“Š **ë”ë¯¸ UI ê¸°ëŠ¥ë“¤** (íšŒì˜ í…œí”Œë¦¿, ì •ëŸ‰ì  ì°¨íŠ¸, ë‚ ì§œ ì…ë ¥ ë“±)

---

## í˜„ì¬ Mock ë°ì´í„° ì‚¬ìš© ìœ„ì¹˜

### 1. íŒŒì¼ ì—…ë¡œë“œ (`backend/app/api/v1/upload.py`)
- **Mock:** íŒŒì¼ì„ `/app/uploads`ì— ì €ì¥ë§Œ í•˜ê³  ì‹¤ì œ ì²˜ë¦¬ X
- **ì‹¤ì œ êµ¬í˜„ í•„ìš”:**
  - ì˜¤ë””ì˜¤ íŒŒì¼ í¬ë§· ë³€í™˜ (â†’ WAV)
  - ì˜¤ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê¸¸ì´, ìƒ˜í”Œë ˆì´íŠ¸ ë“±)

### 2. ì „ì²˜ë¦¬ (í˜„ì¬ êµ¬í˜„ ì•ˆë¨)
- **Mock:** ì—†ìŒ (ë¡œë”© í™”ë©´ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ë§Œ)
- **ì‹¤ì œ êµ¬í˜„ í•„ìš”:**
  - VAD (Voice Activity Detection)
  - ë…¸ì´ì¦ˆ ì œê±°
  - ë‘ ê°œì˜ íŒŒì¼ ìƒì„± (STTìš©, Diarizationìš©)

### 3. STT (í˜„ì¬ êµ¬í˜„ ì•ˆë¨)
- **Mock:** `backend/app/api/v1/tagging.py`ì˜ í•˜ë“œì½”ë”©ëœ í…ìŠ¤íŠ¸
- **ì‹¤ì œ êµ¬í˜„ í•„ìš”:**
  - Whisper ëª¨ë¸ í†µí•©
  - ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ

### 4. í™”ì ë¶„ë¦¬ (í˜„ì¬ êµ¬í˜„ ì•ˆë¨)
- **Mock:** `backend/app/api/v1/tagging.py`ì˜ í•˜ë“œì½”ë”©ëœ SPEAKER_00, SPEAKER_01
- **ì‹¤ì œ êµ¬í˜„ í•„ìš”:**
  - Diarization ëª¨ë¸ í†µí•© (pyannote.audio, NeMo ë“±)
  - í™”ìë³„ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ

### 5. ì´ë¦„ ê°ì§€ (`backend/app/api/v1/tagging.py`)
- **Mock:** `["ë¯¼ì„œ", "ì¸ì„œ", "ê¹€íŒ€ì¥"]` í•˜ë“œì½”ë”©
- **ì‹¤ì œ êµ¬í˜„ í•„ìš”:**
  - NER (Named Entity Recognition)
  - í•œêµ­ì–´ ì´ë¦„ íŒ¨í„´ ê°ì§€

### 6. ìš”ì•½ (`frontend/src/pages/ResultPage.jsx`)
- **Mock:** í”„ë¡ íŠ¸ì—”ë“œì— í•˜ë“œì½”ë”©ëœ í…ìŠ¤íŠ¸
- **ì‹¤ì œ êµ¬í˜„ í•„ìš”:**
  - LLM API ì—°ë™ (GPT, Claude ë“±)
  - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

---

## êµì²´ ë¡œë“œë§µ

### Step 1: ì „ì²˜ë¦¬ ëª¨ë“ˆ êµ¬í˜„

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/preprocessing.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©
```python
# backend/app/services/preprocessing.py

import librosa
import noisereduce as nr
from pydub import AudioSegment
import os

class AudioPreprocessor:
    def __init__(self):
        self.sample_rate = 16000  # Whisper ê¶Œì¥ ìƒ˜í”Œë ˆì´íŠ¸

    def process(self, input_path: str, output_dir: str) -> dict:
        """
        ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
        Returns:
            {
                "stt_input_path": str,  # VADë§Œ ì ìš©
                "diar_input_path": str  # VAD + ë…¸ì´ì¦ˆ ì œê±°
            }
        """
        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
        audio, sr = librosa.load(input_path, sr=self.sample_rate)

        # 2. VAD ì ìš© (ë¬µìŒ ì œê±°)
        vad_audio = self._apply_vad(audio, sr)

        # 3. STTìš© íŒŒì¼ ì €ì¥ (VADë§Œ)
        stt_path = os.path.join(output_dir, "stt_input.wav")
        librosa.output.write_wav(stt_path, vad_audio, sr)

        # 4. ë…¸ì´ì¦ˆ ì œê±°
        clean_audio = nr.reduce_noise(y=vad_audio, sr=sr)

        # 5. Diarizationìš© íŒŒì¼ ì €ì¥ (VAD + ë…¸ì´ì¦ˆ ì œê±°)
        diar_path = os.path.join(output_dir, "diar_input.wav")
        librosa.output.write_wav(diar_path, clean_audio, sr)

        return {
            "stt_input_path": stt_path,
            "diar_input_path": diar_path
        }

    def _apply_vad(self, audio, sr):
        # TODO: VAD ë¡œì§ êµ¬í˜„
        # ì˜ˆ: webrtcvad, silero-vad ë“± ì‚¬ìš©
        return audio
```

#### API ìˆ˜ì •
```python
# backend/app/api/v1/upload.py

from app.services.preprocessing import AudioPreprocessor

@router.post("/upload", response_model=AudioFileUploadResponse)
async def upload_audio_file(file: UploadFile = File(...)):
    # ... íŒŒì¼ ì €ì¥ ë¡œì§ ...

    # [ì¶”ê°€] ì „ì²˜ë¦¬ ì‹¤í–‰
    preprocessor = AudioPreprocessor()
    result = preprocessor.process(file_path, upload_dir)

    # DBì— ì €ì¥ (í˜„ì¬ëŠ” ë©”ëª¨ë¦¬)
    UPLOADED_FILES[file_id]["preprocessing"] = result

    return AudioFileUploadResponse(...)
```

---

### Step 2: STT ëª¨ë“ˆ êµ¬í˜„ (Whisper)

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/stt_service.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©
```python
# backend/app/services/stt_service.py

import whisper

class STTService:
    def __init__(self, model_size: str = "medium"):
        """
        model_size: tiny, base, small, medium, large
        """
        self.model = whisper.load_model(model_size)

    def transcribe(self, audio_path: str) -> list:
        """
        STT ìˆ˜í–‰
        Returns: I,O.md Step 3 í˜•ì‹
            [
                {"text": "ë„¤", "start": 3.6, "end": 3.8},
                {"text": "ë¯¼ì„œì”¨", "start": 3.9, "end": 4.5},
                ...
            ]
        """
        result = self.model.transcribe(
            audio_path,
            language="ko",
            word_timestamps=True  # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
        )

        word_segments = []
        for segment in result["segments"]:
            for word in segment.get("words", []):
                word_segments.append({
                    "text": word["word"],
                    "start": word["start"],
                    "end": word["end"]
                })

        return word_segments
```

#### requirements.txt ì¶”ê°€
```txt
openai-whisper==20231117
```

#### API ìˆ˜ì •
```python
# backend/app/api/v1/tagging.py

from app.services.stt_service import STTService

stt_service = STTService(model_size="medium")

@router.get("/tagging/{file_id}")
async def get_tagging_suggestion(file_id: str):
    # [ìˆ˜ì •] Mock ëŒ€ì‹  ì‹¤ì œ STT ì‹¤í–‰
    file_info = UPLOADED_FILES[file_id]
    stt_input_path = file_info["preprocessing"]["stt_input_path"]

    stt_results = stt_service.transcribe(stt_input_path)

    # stt_resultsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¦„ ê°ì§€ ë¡œì§ ì‹¤í–‰
    # ...
```

---

### Step 3: í™”ì ë¶„ë¦¬ ëª¨ë“ˆ êµ¬í˜„

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/diarization_service.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©
```python
# backend/app/services/diarization_service.py

from pyannote.audio import Pipeline

class DiarizationService:
    def __init__(self):
        # Hugging Face í† í° í•„ìš”
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token="YOUR_HF_TOKEN"
        )

    def diarize(self, audio_path: str) -> dict:
        """
        í™”ì ë¶„ë¦¬
        Returns: I,O.md Step 4 í˜•ì‹
            {
                "turns": [
                    {"speaker_label": "SPEAKER_00", "start": 0.4, "end": 3.5},
                    {"speaker_label": "SPEAKER_01", "start": 3.6, "end": 5.8},
                    ...
                ],
                "embeddings": {
                    "SPEAKER_00": [0.12, -0.45, ...],
                    "SPEAKER_01": [-0.33, 0.76, ...]
                }
            }
        """
        diarization = self.pipeline(audio_path)

        turns = []
        embeddings = {}

        for turn, _, speaker in diarization.itertracks(yield_label=True):
            turns.append({
                "speaker_label": speaker,
                "start": turn.start,
                "end": turn.end
            })

            # ì„ë² ë”© ì¶”ì¶œ (pyannoteì˜ ê²½ìš°)
            if speaker not in embeddings:
                embeddings[speaker] = self._extract_embedding(audio_path, turn)

        return {
            "turns": turns,
            "embeddings": embeddings
        }

    def _extract_embedding(self, audio_path: str, turn):
        # TODO: í™”ìë³„ ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
        # ë™ì¼ì¸ íŒë³„ì— ì‚¬ìš©
        pass
```

#### requirements.txt ì¶”ê°€
```txt
pyannote.audio==3.1.1
torch>=2.0.0
```

---

### Step 4: ì´ë¦„ ê°ì§€ ë¡œì§ êµ¬í˜„

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/tagging_service.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©
```python
# backend/app/services/tagging_service.py

import re
from konlpy.tag import Okt

class NameDetectionService:
    def __init__(self):
        self.okt = Okt()
        # í•œêµ­ì–´ ì´ë¦„ íŒ¨í„´
        self.name_patterns = [
            r"([ê°€-í£]{2,3})(ì”¨|ë‹˜|ì„ ìƒë‹˜|êµìˆ˜ë‹˜|íŒ€ì¥|ë¶€ì¥|ê³¼ì¥)",
            r"([ê°€-í£]{2,3})\s*(ë¶„|ì´)",
        ]

    def detect_names(self, stt_results: list, diarization_results: dict) -> dict:
        """
        I,O.md Step 5a~5c êµ¬í˜„
        Returns:
            {
                "detected_names": ["ë¯¼ì„œ", "ì¸ì„œ", "ê¹€íŒ€ì¥"],
                "suggested_mappings": [
                    {
                        "speaker_label": "SPEAKER_00",
                        "suggested_name": "ë¯¼ì„œ"  # ì„ë² ë”© ìœ ì‚¬ë„ë¡œ íŒë‹¨
                    },
                    ...
                ]
            }
        """
        # 1. STT ê²°ê³¼ì—ì„œ ì´ë¦„ ì¶”ì¶œ
        detected_names = []
        name_occurrences = {}  # {ì´ë¦„: [(ì‹œê°„, SPEAKER_XX), ...]}

        for word_segment in stt_results:
            text = word_segment["text"]
            time = word_segment["start"]

            # íŒ¨í„´ ë§¤ì¹­
            for pattern in self.name_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    name = match[0] if isinstance(match, tuple) else match
                    detected_names.append(name)

                    # ì–´ëŠ í™”ìê°€ ë§í–ˆëŠ”ì§€ í™•ì¸
                    speaker = self._find_speaker_at_time(time, diarization_results)

                    if name not in name_occurrences:
                        name_occurrences[name] = []
                    name_occurrences[name].append((time, speaker))

        # 2. ì´ë¦„ê³¼ í™”ì ë§¤ì¹­ (I,O.md Step 5b~5c)
        suggested_mappings = self._match_names_to_speakers(
            name_occurrences,
            diarization_results
        )

        return {
            "detected_names": list(set(detected_names)),
            "suggested_mappings": suggested_mappings
        }

    def _find_speaker_at_time(self, time: float, diarization_results: dict) -> str:
        """í•´ë‹¹ ì‹œê°„ëŒ€ì— ë§í•˜ê³  ìˆëŠ” í™”ì ì°¾ê¸°"""
        for turn in diarization_results["turns"]:
            if turn["start"] <= time <= turn["end"]:
                return turn["speaker_label"]
        return None

    def _match_names_to_speakers(self, name_occurrences: dict, diarization_results: dict) -> list:
        """
        ì´ë¦„ê³¼ í™”ì ë§¤ì¹­
        - "ë¯¼ì„œì”¨"ë¼ê³  ë¶ˆë¦° ì‹œì ì˜ í™”ìê°€ ëˆ„êµ¬ì¸ì§€ í™•ì¸
        - ëŒ€í™” ë¬¸ë§¥ìƒ "ë¯¼ì„œ"ëŠ” ë‹¤ë¥¸ ì‚¬ëŒì´ ë¶€ë¥´ëŠ” ì´ë¦„
        - ì„ë² ë”© ìœ ì‚¬ë„ë¡œ "ë¯¼ì„œ"ì™€ "ì¸ì„œ"ê°€ ê°™ì€ ì‚¬ëŒì¸ì§€ íŒë‹¨
        """
        # TODO: ë³µì¡í•œ ë¡œì§ êµ¬í˜„
        # I,O.md Step 5c ì°¸ì¡°
        pass
```

#### requirements.txt ì¶”ê°€
```txt
konlpy==0.6.0
```

---

### Step 5: ë³‘í•© ë¡œì§ êµ¬í˜„

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/merge_service.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©
```python
# backend/app/services/merge_service.py

class MergeService:
    def merge(self, stt_results: list, diarization_results: dict, speaker_mappings: dict) -> list:
        """
        STT + Diarization ë³‘í•© (I,O.md Step 5f)

        Args:
            stt_results: STT ë‹¨ì–´ë³„ ê²°ê³¼
            diarization_results: í™”ì ë¶„ë¦¬ ê²°ê³¼
            speaker_mappings: {"SPEAKER_00": "ê¹€ë¯¼ì„œ", ...}

        Returns: ìµœì¢… ëŒ€ë³¸
            [
                {
                    "speaker_name": "ê¹€ë¯¼ì„œ",
                    "start_time": 0.4,
                    "end_time": 3.5,
                    "text": "ì˜¤ëŠ˜ íšŒì˜ ì•ˆê±´ì€..."
                },
                ...
            ]
        """
        # ìµœëŒ€ ê²¹ì¹¨ ì•Œê³ ë¦¬ì¦˜
        final_transcript = []

        # í™”ìë³„ ë°œí™” êµ¬ê°„ ê·¸ë£¹í™”
        speaker_segments = self._group_by_speaker(stt_results, diarization_results)

        for segment in speaker_segments:
            speaker_label = segment["speaker_label"]
            speaker_name = speaker_mappings.get(speaker_label, "Unknown")

            final_transcript.append({
                "speaker_name": speaker_name,
                "start_time": segment["start"],
                "end_time": segment["end"],
                "text": segment["text"]
            })

        return final_transcript

    def _group_by_speaker(self, stt_results: list, diarization_results: dict) -> list:
        """
        STT ë‹¨ì–´ë“¤ì„ í™”ìë³„ ë°œí™” êµ¬ê°„ìœ¼ë¡œ ê·¸ë£¹í™”
        ìµœëŒ€ ê²¹ì¹¨ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
        """
        segments = []
        current_speaker = None
        current_text = []
        current_start = None
        current_end = None

        for word_segment in stt_results:
            word_time = (word_segment["start"] + word_segment["end"]) / 2

            # ì´ ë‹¨ì–´ë¥¼ ë§í•œ í™”ì ì°¾ê¸° (ìµœëŒ€ ê²¹ì¹¨)
            speaker = self._find_speaker_max_overlap(
                word_segment["start"],
                word_segment["end"],
                diarization_results["turns"]
            )

            if speaker != current_speaker:
                # í™”ì ë³€ê²½ â†’ ì´ì „ ë°œí™” ì €ì¥
                if current_speaker is not None:
                    segments.append({
                        "speaker_label": current_speaker,
                        "start": current_start,
                        "end": current_end,
                        "text": " ".join(current_text)
                    })

                # ìƒˆ ë°œí™” ì‹œì‘
                current_speaker = speaker
                current_text = [word_segment["text"]]
                current_start = word_segment["start"]
                current_end = word_segment["end"]
            else:
                # ê°™ì€ í™”ì â†’ í…ìŠ¤íŠ¸ ì¶”ê°€
                current_text.append(word_segment["text"])
                current_end = word_segment["end"]

        # ë§ˆì§€ë§‰ ë°œí™” ì €ì¥
        if current_speaker is not None:
            segments.append({
                "speaker_label": current_speaker,
                "start": current_start,
                "end": current_end,
                "text": " ".join(current_text)
            })

        return segments

    def _find_speaker_max_overlap(self, word_start: float, word_end: float, turns: list) -> str:
        """
        ìµœëŒ€ ê²¹ì¹¨ ì•Œê³ ë¦¬ì¦˜
        ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í™”ì êµ¬ê°„ì˜ ê²¹ì¹¨ì„ ê³„ì‚°í•˜ì—¬
        ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” í™”ì ë°˜í™˜
        """
        max_overlap = 0
        best_speaker = None

        for turn in turns:
            overlap = self._calculate_overlap(
                word_start, word_end,
                turn["start"], turn["end"]
            )

            if overlap > max_overlap:
                max_overlap = overlap
                best_speaker = turn["speaker_label"]

        return best_speaker

    def _calculate_overlap(self, start1: float, end1: float, start2: float, end2: float) -> float:
        """ë‘ êµ¬ê°„ì˜ ê²¹ì¹¨ ì‹œê°„ ê³„ì‚°"""
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        return max(0, overlap_end - overlap_start)
```

---

### Step 6: RAG ì‹œìŠ¤í…œ êµ¬í˜„ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)

#### ê°œìš”
RAG (Retrieval-Augmented Generation)ëŠ” íšŒì˜ë¡ ì „ì²´ë¥¼ ë²¡í„° DBì— ì €ì¥í•˜ê³ , ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ëŒ€í™” êµ¬ê°„ì„ ê²€ìƒ‰í•˜ì—¬ LLMì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

#### ì•„í‚¤í…ì²˜
```
[íšŒì˜ë¡ ëŒ€ë³¸]
    â†“
[ì²­í¬ ë¶„í• ] (ë°œí™” ë‹¨ìœ„ ë˜ëŠ” ì‹œê°„ ë‹¨ìœ„)
    â†“
[ì„ë² ë”© ìƒì„±] (SentenceTransformer)
    â†“
[ë²¡í„° DB ì €ì¥] (ChromaDB / FAISS)
    â†“
[ì‚¬ìš©ì ì§ˆë¬¸] â†’ [ìœ ì‚¬ë„ ê²€ìƒ‰] â†’ [Top-K ë°œí™” ì¶”ì¶œ]
    â†“
[LLM ë‹µë³€ ìƒì„±] (ê²€ìƒ‰ëœ ë°œí™”ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ)
```

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/rag_service.py (ì‹ ê·œ ìƒì„±)
backend/app/services/vector_store.py (ì‹ ê·œ ìƒì„±)
backend/app/api/v1/rag.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©

##### 1. ë²¡í„° ìŠ¤í† ì–´ ì„œë¹„ìŠ¤
```python
# backend/app/services/vector_store.py

import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict

class VectorStoreService:
    def __init__(self):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path="/app/data/chromadb")

        # ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›)
        self.embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')

    def create_collection(self, file_id: str):
        """íŒŒì¼ë³„ ì»¬ë ‰ì…˜ ìƒì„±"""
        collection_name = f"meeting_{file_id}"
        return self.client.create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def add_transcripts(self, file_id: str, transcripts: List[Dict]):
        """
        ëŒ€ë³¸ì„ ë²¡í„° DBì— ì €ì¥

        Args:
            file_id: íŒŒì¼ ID
            transcripts: [
                {
                    "speaker_name": "ê¹€ë¯¼ì„œ",
                    "text": "ì˜¤ëŠ˜ íšŒì˜ ì•ˆê±´ì€...",
                    "start_time": 0.4,
                    "end_time": 3.5
                },
                ...
            ]
        """
        collection = self.client.get_collection(f"meeting_{file_id}")

        # ê° ë°œí™”ë¥¼ ë¬¸ì„œë¡œ ì €ì¥
        documents = []
        metadatas = []
        ids = []

        for i, segment in enumerate(transcripts):
            # í™”ì ì´ë¦„ + ë°œí™” ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ
            doc_text = f"{segment['speaker_name']}: {segment['text']}"
            documents.append(doc_text)

            metadatas.append({
                "speaker": segment['speaker_name'],
                "start_time": segment['start_time'],
                "end_time": segment['end_time'],
                "index": i
            })

            ids.append(f"{file_id}_{i}")

        # ì„ë² ë”© ìƒì„± ë° ì €ì¥
        embeddings = self.embedding_model.encode(documents).tolist()

        collection.add(
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )

    def search(self, file_id: str, query: str, top_k: int = 5) -> List[Dict]:
        """
        ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë°œí™” ê²€ìƒ‰

        Args:
            file_id: íŒŒì¼ ID
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ë°œí™” ê°œìˆ˜

        Returns:
            [
                {
                    "speaker": "ê¹€ë¯¼ì„œ",
                    "text": "ì˜¤ëŠ˜ íšŒì˜ ì•ˆê±´ì€...",
                    "start_time": 0.4,
                    "end_time": 3.5,
                    "score": 0.87
                },
                ...
            ]
        """
        collection = self.client.get_collection(f"meeting_{file_id}")

        # ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_model.encode([query]).tolist()

        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for i in range(len(results['ids'][0])):
            metadata = results['metadatas'][0][i]
            document = results['documents'][0][i]
            distance = results['distances'][0][i]

            # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (cosine distance -> similarity)
            similarity = 1 - distance

            formatted_results.append({
                "speaker": metadata['speaker'],
                "text": document.split(": ", 1)[1],  # "í™”ì: ë‚´ìš©"ì—ì„œ ë‚´ìš©ë§Œ ì¶”ì¶œ
                "start_time": metadata['start_time'],
                "end_time": metadata['end_time'],
                "score": round(similarity, 2)
            })

        return formatted_results
```

##### 2. RAG ì„œë¹„ìŠ¤
```python
# backend/app/services/rag_service.py

from openai import OpenAI
from typing import List, Dict
from .vector_store import VectorStoreService

class RAGService:
    def __init__(self):
        self.vector_store = VectorStoreService()
        self.llm_client = OpenAI(api_key="YOUR_API_KEY")

    def answer_question(self, file_id: str, question: str) -> Dict:
        """
        RAG ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€

        Args:
            file_id: íŒŒì¼ ID
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            {
                "answer": "LLMì´ ìƒì„±í•œ ë‹µë³€",
                "sources": [
                    {
                        "speaker": "ê¹€ë¯¼ì„œ",
                        "text": "ê´€ë ¨ ë°œí™” ë‚´ìš©",
                        "start_time": 0.4,
                        "end_time": 3.5,
                        "score": 0.87
                    },
                    ...
                ]
            }
        """
        # 1. ë²¡í„° DBì—ì„œ ê´€ë ¨ ë°œí™” ê²€ìƒ‰
        relevant_segments = self.vector_store.search(
            file_id=file_id,
            query=question,
            top_k=5
        )

        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[{seg['start_time']:.1f}s - {seg['end_time']:.1f}s] "
            f"{seg['speaker']}: {seg['text']}"
            for seg in relevant_segments
        ])

        # 3. LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
ë‹¤ìŒì€ íšŒì˜ë¡ì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ íšŒì˜ë¡ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

<íšŒì˜ë¡>
{context}
</íšŒì˜ë¡>

<ì§ˆë¬¸>
{question}
</ì§ˆë¬¸>

ë‹µë³€ ì‹œ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. íšŒì˜ë¡ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ì¶”ì¸¡ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
3. ë‹µë³€ ì‹œ "XXì´ˆ êµ¬ê°„ì—ì„œ OOë‹˜ì´ ì–¸ê¸‰í–ˆë“¯ì´..." í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ë°íˆì„¸ìš”.
4. íšŒì˜ë¡ì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ "íšŒì˜ë¡ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
"""

        # 4. LLM í˜¸ì¶œ
        response = self.llm_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒì˜ë¡ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # ë‚®ì€ ì˜¨ë„ë¡œ ì •í™•ì„± ë†’ì„
        )

        answer = response.choices[0].message.content

        return {
            "answer": answer,
            "sources": relevant_segments
        }
```

##### 3. API ì—”ë“œí¬ì¸íŠ¸
```python
# backend/app/api/v1/rag.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from app.services.rag_service import RAGService
from sqlalchemy.orm import Session
from app.api.deps import get_db
from fastapi import Depends

router = APIRouter()
rag_service = RAGService()

class QuestionRequest(BaseModel):
    question: str

class AnswerResponse(BaseModel):
    answer: str
    sources: list

@router.post("/rag/{file_id}/index")
async def index_transcript(file_id: str, db: Session = Depends(get_db)):
    """
    íšŒì˜ë¡ì„ ë²¡í„° DBì— ì¸ë±ì‹±
    (íƒœê¹… ì™„ë£Œ í›„ ìë™ í˜¸ì¶œ ë˜ëŠ” ìˆ˜ë™ íŠ¸ë¦¬ê±°)
    """
    # DBì—ì„œ ìµœì¢… ëŒ€ë³¸ ê°€ì ¸ì˜¤ê¸°
    from app.models.stt import STTResult

    stt_results = db.query(STTResult).filter(
        STTResult.audio_file_id == int(file_id)
    ).order_by(STTResult.start_time).all()

    if not stt_results:
        raise HTTPException(status_code=404, detail="ëŒ€ë³¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ëŒ€ë³¸ í¬ë§· ë³€í™˜
    transcripts = [
        {
            "speaker_name": result.speaker_name or "Unknown",
            "text": result.text,
            "start_time": result.start_time,
            "end_time": result.end_time
        }
        for result in stt_results
    ]

    # ë²¡í„° DBì— ì €ì¥
    try:
        rag_service.vector_store.create_collection(file_id)
        rag_service.vector_store.add_transcripts(file_id, transcripts)

        return {
            "message": "ì¸ë±ì‹± ì™„ë£Œ",
            "indexed_segments": len(transcripts)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")

@router.post("/rag/{file_id}/ask", response_model=AnswerResponse)
async def ask_question(file_id: str, request: QuestionRequest):
    """
    íšŒì˜ë¡ì— ëŒ€í•´ ì§ˆë¬¸í•˜ê¸°
    """
    try:
        result = rag_service.answer_question(file_id, request.question)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
```

#### requirements.txt ì¶”ê°€
```txt
sentence-transformers==2.2.2
chromadb==0.4.22
openai>=1.0.0
```

#### ì‚¬ìš© íë¦„
1. **ì¸ë±ì‹±**: íƒœê¹… ì™„ë£Œ í›„ `POST /api/v1/rag/{file_id}/index` í˜¸ì¶œ
2. **ì§ˆë¬¸**: ê²°ê³¼ í˜ì´ì§€ì—ì„œ `POST /api/v1/rag/{file_id}/ask` í˜¸ì¶œ

#### í”„ë¡ íŠ¸ì—”ë“œ í†µí•© ì˜ˆì‹œ
```jsx
// ResultPageNew.jsxì— RAG íƒ­ ì¶”ê°€

const [question, setQuestion] = useState('')
const [answer, setAnswer] = useState(null)
const [isLoading, setIsLoading] = useState(false)

const handleAskQuestion = async () => {
  setIsLoading(true)
  try {
    const response = await axios.post(
      `${API_BASE_URL}/api/v1/rag/${fileId}/ask`,
      { question }
    )
    setAnswer(response.data)
  } catch (error) {
    console.error('ì§ˆë¬¸ ì‹¤íŒ¨:', error)
  } finally {
    setIsLoading(false)
  }
}

// UI
<div>
  <input
    value={question}
    onChange={(e) => setQuestion(e.target.value)}
    placeholder="íšŒì˜ë¡ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."
  />
  <button onClick={handleAskQuestion}>ì§ˆë¬¸í•˜ê¸°</button>

  {answer && (
    <div>
      <h3>ë‹µë³€:</h3>
      <p>{answer.answer}</p>

      <h4>ì¶œì²˜:</h4>
      {answer.sources.map((src, i) => (
        <div key={i}>
          [{src.start_time}s] {src.speaker}: {src.text}
          (ìœ ì‚¬ë„: {src.score})
        </div>
      ))}
    </div>
  )}
</div>
```

---

### Step 7: ìš”ì•½ ê¸°ëŠ¥ êµ¬í˜„ (LLM) - ì„ íƒì 

#### íŒŒì¼ ìœ„ì¹˜
```
backend/app/services/llm_service.py (ì‹ ê·œ ìƒì„±)
backend/app/api/v1/summary.py (ì‹ ê·œ ìƒì„±)
```

#### êµ¬í˜„ ë‚´ìš©
```python
# backend/app/services/llm_service.py

from openai import OpenAI

class LLMService:
    def __init__(self):
        self.client = OpenAI(api_key="YOUR_API_KEY")

    def summarize(self, transcript: list) -> dict:
        """
        ìµœì¢… ëŒ€ë³¸ì„ ìš”ì•½
        Returns:
            {
                "summary": "í•µì‹¬ ë‚´ìš©...",
                "key_points": ["ê²°ì • ì‚¬í•­ 1", "ê²°ì • ì‚¬í•­ 2"],
                "action_items": ["ì•¡ì…˜ ì•„ì´í…œ 1", ...]
            }
        """
        # ëŒ€ë³¸ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text = "\n".join([
            f"{seg['speaker_name']}: {seg['text']}"
            for seg in transcript
        ])

        prompt = f"""
ë‹¤ìŒì€ íšŒì˜ ëŒ€ë³¸ì…ë‹ˆë‹¤. ìš”ì•½í•´ì£¼ì„¸ìš”.

{text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½:
1. í•µì‹¬ ë‚´ìš© (2-3ë¬¸ì¥)
2. ì£¼ìš” ê²°ì • ì‚¬í•­
3. ì•¡ì…˜ ì•„ì´í…œ
"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒì˜ë¡ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )

        # ì‘ë‹µ íŒŒì‹± ë° êµ¬ì¡°í™”
        # ...

        return {
            "summary": "...",
            "key_points": [...],
            "action_items": [...]
        }
```

#### API ìƒì„±
```python
# backend/app/api/v1/summary.py

from fastapi import APIRouter
from app.services.llm_service import LLMService

router = APIRouter()
llm_service = LLMService()

@router.get("/summary/{file_id}")
async def get_summary(file_id: str):
    # íƒœê¹… ê²°ê³¼ì—ì„œ ìµœì¢… ëŒ€ë³¸ ê°€ì ¸ì˜¤ê¸°
    result = TAGGING_RESULTS[file_id]
    final_transcript = result["final_transcript"]

    # LLMìœ¼ë¡œ ìš”ì•½
    summary = llm_service.summarize(final_transcript)

    return summary
```

#### requirements.txt ì¶”ê°€
```txt
openai>=1.0.0
```

---

## êµì²´ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 2 ì‹œì‘ ì „ í™•ì¸ì‚¬í•­

- [ ] ì „ì²˜ë¦¬ ë°©ì‹ í™•ì • (VAD, ë…¸ì´ì¦ˆ ì œê±° ê¸°ë²•)
- [ ] Diarization ëª¨ë¸ ì„ ì • ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶• (ë‹¤ì–‘í•œ ì†ŒìŒ í™˜ê²½)
- [ ] KPI ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥ ì—¬ë¶€ ê²€ì¦ (WER 90%, DER 90%)

### ëª¨ë“ˆë³„ êµì²´ ìˆœì„œ

1. **ì „ì²˜ë¦¬ ëª¨ë“ˆ** â†’ STTì™€ Diarizationì˜ ì…ë ¥ ìƒì„±
2. **STT ëª¨ë“ˆ** â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
3. **Diarization ëª¨ë“ˆ** â†’ í™”ì ë¶„ë¦¬
4. **ì´ë¦„ ê°ì§€ + ë³‘í•©** â†’ íƒœê¹… ì œì•ˆ
5. **LLM ìš”ì•½** â†’ ë¶€ê°€ ê¸°ëŠ¥

### Mock â†’ Real êµì²´ ì‹œ ì£¼ì˜ì‚¬í•­

1. **DB ì‚¬ìš© ì‹œì‘**
   - í˜„ì¬ ë©”ëª¨ë¦¬ ì €ì¥ â†’ DB ì €ì¥ìœ¼ë¡œ ë³€ê²½
   - `UPLOADED_FILES`, `TAGGING_RESULTS` ë”•ì…”ë„ˆë¦¬ ì œê±°
   - SQLAlchemy ëª¨ë¸ í™œìš©

2. **ë¹„ë™ê¸° ì²˜ë¦¬**
   - AI ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
   - Celery, RQ ë“± Task Queue ë„ì… ê²€í† 
   - WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì „ë‹¬

3. **ì—ëŸ¬ í•¸ë“¤ë§**
   - AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§
   - íƒ€ì„ì•„ì›ƒ ì„¤ì •
   - ì‚¬ìš©ì ì¹œí™”ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€

4. **ì„±ëŠ¥ ìµœì í™”**
   - GPU í™œìš© (Whisper, Diarization)
   - ëª¨ë¸ ìºì‹±
   - ë°°ì¹˜ ì²˜ë¦¬

---

## Frontend ìˆ˜ì • ì‚¬í•­

### 1. ë¡œë”© í™”ë©´ ì—…ë°ì´íŠ¸

```jsx
// frontend/src/pages/ProcessingPage.jsx

// [ìˆ˜ì •] Mock ì‹œë®¬ë ˆì´ì…˜ ëŒ€ì‹  ì‹¤ì œ API í´ë§
useEffect(() => {
  const pollStatus = setInterval(async () => {
    try {
      const response = await axios.get(`${API_BASE_URL}/api/v1/status/${fileId}`);

      if (response.data.status === 'completed') {
        clearInterval(pollStatus);
        navigate(`/tagging/${fileId}`);
      } else if (response.data.status === 'failed') {
        clearInterval(pollStatus);
        setError('ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
      } else {
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        setProgress(response.data.progress);
        setCurrentStep(response.data.current_step);
      }
    } catch (err) {
      console.error(err);
    }
  }, 2000); // 2ì´ˆë§ˆë‹¤ í´ë§

  return () => clearInterval(pollStatus);
}, [fileId]);
```

### 2. ìš”ì•½ íƒ­ ì—…ë°ì´íŠ¸

```jsx
// frontend/src/pages/ResultPage.jsx

// [ìˆ˜ì •] Mock í…ìŠ¤íŠ¸ ëŒ€ì‹  ì‹¤ì œ API í˜¸ì¶œ
useEffect(() => {
  if (activeTab === 'summary') {
    fetchSummary();
  }
}, [activeTab]);

const fetchSummary = async () => {
  try {
    const response = await axios.get(`${API_BASE_URL}/api/v1/summary/${fileId}`);
    setSummaryData(response.data);
  } catch (err) {
    console.error(err);
  }
};
```

---

## í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# backend/tests/test_stt_service.py

def test_stt_transcribe():
    service = STTService(model_size="tiny")  # í…ŒìŠ¤íŠ¸ëŠ” ì‘ì€ ëª¨ë¸
    result = service.transcribe("test_audio.wav")

    assert len(result) > 0
    assert "text" in result[0]
    assert "start" in result[0]
    assert "end" in result[0]
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸

```python
# backend/tests/test_full_pipeline.py

def test_full_pipeline():
    # 1. íŒŒì¼ ì—…ë¡œë“œ
    # 2. ì „ì²˜ë¦¬
    # 3. STT
    # 4. Diarization
    # 5. ë³‘í•©
    # 6. íƒœê¹…
    # 7. ìš”ì•½

    # ê° ë‹¨ê³„ì˜ ì¶œë ¥ì´ I,O.md í˜•ì‹ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦
```

### 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
import time

def test_performance():
    start = time.time()

    # 10ë¶„ì§œë¦¬ ì˜¤ë””ì˜¤ ì²˜ë¦¬
    result = process_audio("10min_audio.wav")

    elapsed = time.time() - start

    # ì²˜ë¦¬ ì‹œê°„ì´ í•©ë¦¬ì ì¸ì§€ í™•ì¸
    assert elapsed < 600  # 10ë¶„ ì´ë‚´
```

---

## ì°¸ê³  ë¬¸ì„œ

- **I,O.md** - ê° ë‹¨ê³„ë³„ Input/Output í˜•ì‹
- **pdr.md** - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„¤ê³„
- **database_schema.md** - DB ìŠ¤í‚¤ë§ˆ

---

## ë¬¸ì˜ì‚¬í•­

Mock â†’ Real êµì²´ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ pdr.mdì˜ "ë¯¸ê²°ì • ì‚¬í•­" ì„¹ì…˜ì„ ì°¸ê³ í•˜ê±°ë‚˜, íŒ€ íšŒì˜ë¥¼ í†µí•´ í•´ê²°í•˜ì„¸ìš”.
