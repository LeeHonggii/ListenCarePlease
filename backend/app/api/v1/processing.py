"""
ì˜¤ë””ì˜¤ ì²˜ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
- ì „ì²˜ë¦¬ (Step 2)
- STT (Step 3)
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from pathlib import Path
from typing import Dict
from app.services.preprocessing import preprocess_audio
from app.services.stt import run_stt_pipeline
from app.services.diarization import run_diarization, merge_stt_with_diarization
from app.services.ner_service import get_ner_service
from app.core.config import settings
from app.core.device import get_device
import json
from sqlalchemy.orm import Session
from app.api.deps import get_db
from fastapi import Depends
from app.models.audio_file import AudioFile
from app.models.preprocessing import PreprocessingResult
from app.models.stt import STTResult


router = APIRouter()

# ì²˜ë¦¬ ìƒíƒœ ì €ì¥ (ì‹¤ì œë¡œëŠ” DB ì‚¬ìš©)
PROCESSING_STATUS: Dict[str, dict] = {}


def process_audio_pipeline(
    file_id: str,
    whisper_mode: str = "local",
    diarization_mode: str = "senko"
):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        file_id: íŒŒì¼ ID
        whisper_mode: Whisper ëª¨ë“œ ("local" ë˜ëŠ” "api")
        diarization_mode: í™”ì ë¶„ë¦¬ ëª¨ë¸ ("senko" ë˜ëŠ” "nemo")
    """
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ìš© ìƒˆ DB ì„¸ì…˜ ìƒì„±
    from app.db.base import SessionLocal
    db = SessionLocal()

    try:
        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        device = get_device()

        # ëª¨ë¸ í¬ê¸° ê³ ì •
        model_size = "large-v3"

        # ìƒíƒœ ì—…ë°ì´íŠ¸: ì „ì²˜ë¦¬ ì‹œì‘
        PROCESSING_STATUS[file_id] = {
            "status": "preprocessing",
            "step": "ì „ì²˜ë¦¬ ì¤‘...",
            "progress": 10,
            "device": device,
            "model_size": model_size,
        }

        # 1) íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ì„ì‹œë¡œ í•˜ë“œì½”ë”©, ë‚˜ì¤‘ì— DBì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        upload_dir = Path("/app/uploads")
        input_files = list(upload_dir.glob(f"{file_id}.*"))
        if not input_files:
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
        input_path = input_files[0]

        # ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        work_dir = Path(f"/app/temp/{file_id}")
        work_dir.mkdir(parents=True, exist_ok=True)

        # 2) ì „ì²˜ë¦¬
        preprocessed_path = work_dir / "preprocessed.wav"
        _, original_dur, processed_dur = preprocess_audio(input_path, preprocessed_path)

        PROCESSING_STATUS[file_id] = {
            "status": "preprocessing",
            "step": "ì „ì²˜ë¦¬ ì™„ë£Œ",
            "progress": 30,
            "original_duration": original_dur,
            "processed_duration": processed_dur,
        }

        # 3) STT
        use_local = whisper_mode == "local"
        stt_method = f"{'ë¡œì»¬' if use_local else 'API'} Whisper ({model_size})"

        PROCESSING_STATUS[file_id] = {
            "status": "stt",
            "step": f"STT ì§„í–‰ ì¤‘... ({stt_method})",
            "progress": 40,
        }

        # Whisper ì „ì‚¬ (ë¡œì»¬ ë˜ëŠ” API)
        final_txt = run_stt_pipeline(
            preprocessed_path,
            work_dir,
            openai_api_key=settings.OPENAI_API_KEY if not use_local else None,
            use_local_whisper=use_local,
            model_size=model_size,
            device=device
        )

        # STT ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ (Diarization ì „ ë©”ëª¨ë¦¬ í™•ë³´)
        print("ğŸ§¹ STT ì™„ë£Œ, ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        import gc
        import torch
        gc.collect()  # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # CUDA ìºì‹œ ì •ë¦¬
        print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

        # 4) Diarization (í™”ì ë¶„ë¦¬)
        diarization_method = "Senko" if diarization_mode == "senko" else "NeMo"

        PROCESSING_STATUS[file_id] = {
            "status": "diarization",
            "step": f"í™”ì ë¶„ë¦¬ ì¤‘... ({diarization_method})",
            "progress": 70,
        }

        try:
            diarization_result = run_diarization(
                preprocessed_path,
                device=device,
                mode=diarization_mode
            )

            # Diarization ê²°ê³¼ ì €ì¥
            diarization_json = work_dir / "diarization_result.json"
            with open(diarization_json, 'w', encoding='utf-8') as f:
                json.dump(diarization_result, f, ensure_ascii=False, indent=2)

            # STT ê²°ê³¼ íŒŒì‹±
            stt_segments = []
            for line in final_txt.read_text(encoding='utf-8').splitlines():
                if line.strip():
                    # [00:00:00.000 - 00:00:02.800] í…ìŠ¤íŠ¸ í˜•ì‹ íŒŒì‹±
                    import re
                    match = re.match(r'\[(\d{2}:\d{2}:\d{2}\.\d{3}) - (\d{2}:\d{2}:\d{2}\.\d{3})\] (.+)', line)
                    if match:
                        start_str, end_str, text = match.groups()
                        # ì‹œê°„ì„ ì´ˆë¡œ ë³€í™˜
                        def time_to_seconds(t):
                            h, m, s = t.split(':')
                            return int(h) * 3600 + int(m) * 60 + float(s)

                        stt_segments.append({
                            "text": text,
                            "start": time_to_seconds(start_str),
                            "end": time_to_seconds(end_str)
                        })

            # STT + Diarization ë³‘í•©
            merged_result = merge_stt_with_diarization(stt_segments, diarization_result)

            # ë³‘í•© ê²°ê³¼ ì €ì¥
            merged_json = work_dir / "merged_result.json"
            with open(merged_json, 'w', encoding='utf-8') as f:
                json.dump(merged_result, f, ensure_ascii=False, indent=2)

        except Exception as diarization_error:
            print(f"âš ï¸ Diarization failed: {diarization_error}")
            # Diarization ì‹¤íŒ¨í•´ë„ STT ê²°ê³¼ëŠ” ìœ ì§€
            diarization_result = None
            merged_result = None

        # 5) NER (ì´ë¦„ ì¶”ì¶œ ë° êµ°ì§‘í™”)
        PROCESSING_STATUS[file_id] = {
            "status": "ner",
            "step": "ì´ë¦„ ì¶”ì¶œ ì¤‘...",
            "progress": 80,
        }

        ner_result = None
        try:
            if merged_result:
                # NER ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                ner_service = get_ner_service()

                # NER ì²˜ë¦¬
                ner_result = ner_service.process_segments(merged_result)

                # NER ê²°ê³¼ ì €ì¥
                ner_json = work_dir / "ner_result.json"
                with open(ner_json, 'w', encoding='utf-8') as f:
                    json.dump(ner_result, f, ensure_ascii=False, indent=2)

                print(f"âœ… NER ì™„ë£Œ: {len(ner_result['final_namelist'])}ê°œ ëŒ€í‘œëª… ì¶”ì¶œ")

        except Exception as ner_error:
            print(f"âš ï¸ NER failed: {ner_error}")
            # NER ì‹¤íŒ¨í•´ë„ ë³‘í•© ê²°ê³¼ëŠ” ìœ ì§€
            ner_result = None

        # 6) DB ì €ì¥
        PROCESSING_STATUS[file_id] = {
            "status": "saving",
            "step": "DB ì €ì¥ ì¤‘...",
            "progress": 90,
        }

        # DB ì €ì¥ ì‹œì‘
        if db:
            try:
                from app.models.diarization import DiarizationResult
                from app.models.tagging import SpeakerMapping
                from app.models.audio_file import AudioFile, FileStatus

                # 6-1) AudioFile ë ˆì½”ë“œ ìƒì„±/ì—…ë°ì´íŠ¸
                # file_idë¡œ íŒŒì¼ ê²€ìƒ‰ (original_filename ë˜ëŠ” file_pathì— í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
                audio_file = db.query(AudioFile).filter(
                    (AudioFile.file_path.like(f"%{file_id}%")) |
                    (AudioFile.original_filename.like(f"%{file_id}%"))
                ).first()

                # ìƒˆ íŒŒì¼ì´ë©´ ìƒì„± (ì„ì‹œ: user_id=1 í•˜ë“œì½”ë”©)
                if not audio_file:
                    audio_file = AudioFile(
                        user_id=1,  # TODO: ì‹¤ì œ user_idë¡œ êµì²´
                        original_filename=input_path.name,
                        file_path=str(input_path),
                        file_size=input_path.stat().st_size,
                        duration=original_dur,
                        mimetype="audio/wav",
                        status=FileStatus.PROCESSING
                    )
                    db.add(audio_file)
                    db.flush()  # ID ìƒì„±

                audio_file_id_db = audio_file.id

                # 6-2) STTResult ì €ì¥ (merged_resultì˜ ê° ì„¸ê·¸ë¨¼íŠ¸)
                if merged_result:
                    for idx, segment in enumerate(merged_result):
                        stt_record = STTResult(
                            audio_file_id=audio_file_id_db,
                            word_index=idx,
                            text=segment.get("text", ""),
                            start_time=segment.get("start", 0.0),
                            end_time=segment.get("end", 0.0),
                            confidence=None  # Whisper doesn't provide word-level confidence
                        )
                        db.add(stt_record)

                # 6-3) DiarizationResult ì €ì¥ (í™”ìë³„ ì„ë² ë”©)
                if diarization_result and 'segments' in diarization_result:
                    for segment in diarization_result['segments']:
                        speaker_label = segment.get('speaker', 'UNKNOWN')

                        # í•´ë‹¹ í™”ìì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
                        embeddings = diarization_result.get('embeddings', {})
                        embedding_vector = embeddings.get(speaker_label)

                        diar_record = DiarizationResult(
                            audio_file_id=audio_file_id_db,
                            speaker_label=speaker_label,
                            start_time=segment.get('start', 0.0),
                            end_time=segment.get('end', 0.0),
                            embedding=embedding_vector  # JSON í˜•íƒœë¡œ ì €ì¥
                        )
                        db.add(diar_record)

                # 6-4) DetectedName ì €ì¥ (NERë¡œ ê°ì§€ëœ ì´ë¦„ë“¤ - has_name: trueì¸ ì„¸ê·¸ë¨¼íŠ¸)
                if ner_result:
                    from app.models.tagging import DetectedName

                    segments_with_names = ner_result.get('segments_with_names', [])

                    # ì´ë¦„ì´ ê°ì§€ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ë§Œ í•„í„°ë§
                    for idx, segment in enumerate(segments_with_names):
                        if segment.get('has_name', False) and segment.get('name'):
                            # ì•ë’¤ 5ë¬¸ì¥ ë¬¸ë§¥ ì¶”ì¶œ (I,O.md 5a~5c)
                            context_before_idx = max(0, idx - 5)
                            context_after_idx = min(len(segments_with_names), idx + 6)

                            context_before = [
                                {
                                    "index": i - idx,
                                    "speaker": seg.get("speaker"),
                                    "text": seg.get("text"),
                                    "time": seg.get("start")
                                }
                                for i, seg in enumerate(segments_with_names[context_before_idx:idx], start=context_before_idx)
                            ]

                            context_after = [
                                {
                                    "index": i - idx,
                                    "speaker": seg.get("speaker"),
                                    "text": seg.get("text"),
                                    "time": seg.get("start")
                                }
                                for i, seg in enumerate(segments_with_names[idx+1:context_after_idx], start=idx+1)
                            ]

                            # ì´ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ê°ì§€ëœ ê° ì´ë¦„ì— ëŒ€í•´ ë ˆì½”ë“œ ìƒì„±
                            for detected_name in segment['name']:
                                name_record = DetectedName(
                                    audio_file_id=audio_file_id_db,
                                    detected_name=detected_name,
                                    speaker_label=segment.get('speaker', 'UNKNOWN'),
                                    time_detected=segment.get('start', 0.0),
                                    confidence=None,  # NER ì‹ ë¢°ë„ (í˜„ì¬ ë¯¸êµ¬í˜„)
                                    similarity_score=None,
                                    context_before=context_before,  # ì• 5ë¬¸ì¥ (I,O.md ì°¸ì¡°)
                                    context_after=context_after,   # ë’¤ 5ë¬¸ì¥ (I,O.md ì°¸ì¡°)
                                    llm_reasoning=None,  # ë©€í‹°í„´ LLM ì¶”ë¡  ê²°ê³¼ (í–¥í›„ êµ¬í˜„)
                                    is_consistent=None   # ì´ì „ ì¶”ë¡ ê³¼ ì¼ì¹˜ ì—¬ë¶€ (í–¥í›„ êµ¬í˜„)
                                )
                                db.add(name_record)

                # 6-5) SpeakerMapping ì €ì¥ (í™”ìë³„ ì´ˆê¸° ë ˆì½”ë“œë§Œ ìƒì„±, ë§¤í•‘ì€ ë‚˜ì¤‘ì—)
                if diarization_result:
                    # í™”ìë³„ ê³ ìœ  ë ˆì´ë¸” ì¶”ì¶œ
                    speaker_labels = list(diarization_result.get('embeddings', {}).keys())

                    # ê° í™”ìì— ëŒ€í•´ SpeakerMapping ìƒì„± (ì´ˆê¸° ì œì•ˆ ì—†ì´)
                    for speaker_label in speaker_labels:
                        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                        existing = db.query(SpeakerMapping).filter(
                            SpeakerMapping.audio_file_id == audio_file_id_db,
                            SpeakerMapping.speaker_label == speaker_label
                        ).first()

                        if not existing:
                            mapping = SpeakerMapping(
                                audio_file_id=audio_file_id_db,
                                speaker_label=speaker_label,
                                suggested_name=None,  # ì´ˆê¸° ì œì•ˆ ì—†ìŒ (í–¥í›„ LLMì´ ì¶”ë¡ )
                                name_confidence=None,
                                name_mentions=0,
                                suggested_role=None,
                                role_confidence=None,
                                conflict_detected=False,
                                needs_manual_review=True,  # ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ì í™•ì¸ í•„ìš”
                                final_name="",  # ì‚¬ìš©ìê°€ í™•ì • ì „ê¹Œì§€ ë¹ˆ ê°’
                                is_modified=False
                            )
                            db.add(mapping)

                # 6-6) AudioFile ìƒíƒœ ì—…ë°ì´íŠ¸
                audio_file.status = FileStatus.COMPLETED

                # ì»¤ë°‹
                db.commit()
                print(f"âœ… DB ì €ì¥ ì™„ë£Œ: audio_file_id={audio_file_id_db}")

                # DetectedName ê°œìˆ˜ í™•ì¸
                detected_name_count = db.query(DetectedName).filter(
                    DetectedName.audio_file_id == audio_file_id_db
                ).count()
                speaker_mapping_count = db.query(SpeakerMapping).filter(
                    SpeakerMapping.audio_file_id == audio_file_id_db
                ).count()
                print(f"  - DetectedName ë ˆì½”ë“œ: {detected_name_count}ê°œ")
                print(f"  - STTResult ë ˆì½”ë“œ: {len(merged_result) if merged_result else 0}ê°œ")
                print(f"  - DiarizationResult ë ˆì½”ë“œ: {len(diarization_result.get('segments', [])) if diarization_result else 0}ê°œ")
                print(f"  - SpeakerMapping ë ˆì½”ë“œ: {speaker_mapping_count}ê°œ")

            except Exception as db_error:
                print(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {db_error}")
                db.rollback()
                # DB ì €ì¥ ì‹¤íŒ¨í•´ë„ íŒŒì¼ ê²°ê³¼ëŠ” ìœ ì§€

        # ì™„ë£Œ
        PROCESSING_STATUS[file_id] = {
            "status": "completed",
            "step": "ì™„ë£Œ",
            "progress": 100,
            "transcript_path": str(final_txt),
            "diarization_path": str(work_dir / "diarization_result.json") if diarization_result else None,
            "merged_path": str(work_dir / "merged_result.json") if merged_result else None,
            "ner_path": str(work_dir / "ner_result.json") if ner_result else None,
            "detected_names": ner_result['final_namelist'] if ner_result else [],
            "speaker_count": len(diarization_result.get('embeddings', {})) if diarization_result else 0,
        }

    except Exception as e:
        PROCESSING_STATUS[file_id] = {
            "status": "failed",
            "step": "ì˜¤ë¥˜ ë°œìƒ",
            "progress": 0,
            "error": str(e),
        }
    finally:
        # DB ì„¸ì…˜ ì¢…ë£Œ
        db.close()


@router.post("/process/{file_id}")
async def start_processing(
    file_id: str,
    background_tasks: BackgroundTasks,
    whisper_mode: str = None,  # "local" or "api" (Noneì¼ ê²½ìš° ì„¤ì •ê°’ ì‚¬ìš©)
    diarization_mode: str = None  # "senko" or "nemo" (Noneì¼ ê²½ìš° ì„¤ì •ê°’ ì‚¬ìš©)
):
    """
    ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)

    Args:
        file_id: ì—…ë¡œë“œëœ íŒŒì¼ ID
        whisper_mode: Whisper ëª¨ë“œ ("local" ë˜ëŠ” "api", ê¸°ë³¸ê°’: ì„¤ì •ê°’)
        diarization_mode: í™”ì ë¶„ë¦¬ ëª¨ë¸ ("senko" ë˜ëŠ” "nemo", ê¸°ë³¸ê°’: ì„¤ì •ê°’)

    Returns:
        ì²˜ë¦¬ ì‹œì‘ í™•ì¸ ë©”ì‹œì§€

    Note:
        - model_size: large-v3 ê³ ì •
        - device: ìë™ ê°ì§€ (CUDA > MPS > CPU)
        - senko: ë¹ ë¦„, ê°„ë‹¨
        - nemo: ì •í™•, ì„¸ë°€í•œ ì„¤ì •
    """
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    upload_dir = Path("/app/uploads")
    input_files = list(upload_dir.glob(f"{file_id}.*"))
    if not input_files:
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì„¤ì •ê°’ ë˜ëŠ” íŒŒë¼ë¯¸í„° ì‚¬ìš©
    use_whisper_mode = whisper_mode if whisper_mode else settings.WHISPER_MODE
    use_diarization_mode = diarization_mode if diarization_mode else settings.DIARIZATION_MODE

    # Whisper ëª¨ë“œ ê²€ì¦
    if use_whisper_mode not in ["local", "api"]:
        raise HTTPException(status_code=400, detail="whisper_modeëŠ” 'local' ë˜ëŠ” 'api'ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # Diarization ëª¨ë“œ ê²€ì¦
    if use_diarization_mode not in ["senko", "nemo"]:
        raise HTTPException(status_code=400, detail="diarization_modeëŠ” 'senko' ë˜ëŠ” 'nemo'ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # API ëª¨ë“œì¼ ë•Œ API í‚¤ í™•ì¸
    if use_whisper_mode == "api" and not settings.OPENAI_API_KEY:
        raise HTTPException(status_code=400, detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
    detected_device = get_device()

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
    PROCESSING_STATUS[file_id] = {
        "status": "queued",
        "step": "ëŒ€ê¸° ì¤‘...",
        "progress": 0,
        "whisper_mode": use_whisper_mode,
        "diarization_mode": use_diarization_mode,
        "model_size": "large-v3",
        "device": detected_device,
    }

    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘ (ë‚´ë¶€ì—ì„œ DB ì„¸ì…˜ ìƒì„±)
    background_tasks.add_task(
        process_audio_pipeline,
        file_id,
        use_whisper_mode,
        use_diarization_mode
    )

    return {
        "file_id": file_id,
        "message": "ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "status": "queued",
        "whisper_mode": use_whisper_mode,
        "diarization_mode": use_diarization_mode,
        "model_size": "large-v3",
        "device": detected_device,
    }


@router.get("/status/{file_id}")
async def get_processing_status(file_id: str):
    """
    ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        í˜„ì¬ ì²˜ë¦¬ ìƒíƒœ
    """
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return PROCESSING_STATUS[file_id]


@router.get("/transcript/{file_id}")
async def get_transcript(file_id: str):
    """
    ì „ì‚¬ ê²°ê³¼ ì¡°íšŒ

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        ì „ì‚¬ í…ìŠ¤íŠ¸
    """
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status = PROCESSING_STATUS[file_id]
    if status["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    transcript_path = Path(status["transcript_path"])
    if not transcript_path.exists():
        raise HTTPException(status_code=404, detail="ì „ì‚¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    lines = []
    for line in transcript_path.read_text(encoding="utf-8").splitlines():
        if line.strip():
            lines.append(line)

    return {"file_id": file_id, "transcript": lines, "total_lines": len(lines)}


@router.get("/ner/{file_id}")
async def get_ner_result(file_id: str):
    """
    NER ê²°ê³¼ ì¡°íšŒ

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        NER ì²˜ë¦¬ ê²°ê³¼ (ì´ë¦„ ëª©ë¡, êµ°ì§‘í™” ì •ë³´, í†µê³„ ë“±)
    """
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status = PROCESSING_STATUS[file_id]
    if status["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    ner_path = status.get("ner_path")
    if not ner_path or not Path(ner_path).exists():
        raise HTTPException(status_code=404, detail="NER ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # NER ê²°ê³¼ ë¡œë“œ
    with open(ner_path, 'r', encoding='utf-8') as f:
        ner_result = json.load(f)

    return {
        "file_id": file_id,
        "detected_names": ner_result.get("final_namelist", []),
        "name_clusters": ner_result.get("name_clusters", {}),
        "unique_names": ner_result.get("unique_names", []),
        "stats": ner_result.get("stats", {}),
        "segments_with_names": ner_result.get("segments_with_names", []),
    }


@router.get("/merged/{file_id}")
async def get_merged_result(file_id: str):
    """
    ë³‘í•©ëœ ê²°ê³¼ ì¡°íšŒ (STT + Diarization + NER)

    Args:
        file_id: íŒŒì¼ ID

    Returns:
        í™”ì ì •ë³´ì™€ ì´ë¦„ì´ í¬í•¨ëœ ì „ì‚¬ ê²°ê³¼
    """
    if file_id not in PROCESSING_STATUS:
        raise HTTPException(status_code=404, detail="ì²˜ë¦¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    status = PROCESSING_STATUS[file_id]
    if status["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # NER ê²°ê³¼ ë¡œë“œ
    ner_path = status.get("ner_path")
    if ner_path and Path(ner_path).exists():
        with open(ner_path, 'r', encoding='utf-8') as f:
            ner_result = json.load(f)
        segments = ner_result.get("segments_with_names", [])
    else:
        # NER ì—†ìœ¼ë©´ ë³‘í•© ê²°ê³¼ë§Œ
        merged_path = status.get("merged_path")
        if not merged_path or not Path(merged_path).exists():
            raise HTTPException(status_code=404, detail="ë³‘í•© ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with open(merged_path, 'r', encoding='utf-8') as f:
            segments = json.load(f)

    return {
        "file_id": file_id,
        "segments": segments,
        "total_segments": len(segments),
        "detected_names": status.get("detected_names", []),
        "speaker_count": status.get("speaker_count", 0),
    }
