{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194Lm3JCyA62",
        "outputId": "42ee108f-7b51-4d5d-d6c1-58ce6d15493f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "huggingface = userdata.get('HF_API')\n",
        "login(huggingface)"
      ],
      "metadata": {
        "id": "FeX0NIzUduKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python.Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dthGzw4LRr",
        "outputId": "f6e6e3aa-474c-43e3-9059-e1ac60a3ea83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python.Levenshtein\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting Levenshtein==0.27.3 (from python.Levenshtein)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python.Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python.Levenshtein\n",
            "Successfully installed Levenshtein-0.27.3 python.Levenshtein-0.27.3 rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbdIhb-mPIwC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "import Levenshtein\n",
        "from transformers import pipeline\n",
        "\n",
        "# ========== 파일 경로 설정 ==========\n",
        "input_json_path = '/content/drive/MyDrive/1106_오후회의_whisper.json'\n",
        "\n",
        "output_json_path = '/content/drive/MyDrive/1106_오후회의_stt_segments_with_name_nf.json'\n",
        "cluster_json_path = '/content/drive/MyDrive/1106_오후회의_name_clusters_nf.json'\n",
        "final_namelist_path = '/content/drive/MyDrive/1106_오후회의_final_namelist_nf.txt'\n",
        "output_txt_path = '/content/drive/MyDrive/1106_오후회의_name_check_transcript_nf.txt'\n",
        "# ====================================\n",
        "\n",
        "# ========== 하이퍼파라미터 설정 ==========\n",
        "NER_THRESHOLD = 0.8  # NER 신뢰도 임계값 (0.0 ~ 1.0)\n",
        "CLUSTER_THRESHOLD = 1.5  # 군집화 거리 임계값\n",
        "# ========================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"이름 추출 및 군집화 파이프라인 시작\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========== 모델 로드 ==========\n",
        "print(\"\\n[1/5] NER 모델 로딩 중...\")\n",
        "ner = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=\"seungkukim/korean-pii-masking\",\n",
        "    aggregation_strategy=\"simple\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"✓ 모델 로드 완료\")\n",
        "\n",
        "# ========== 함수 정의 ==========\n",
        "def extract_person_names(ner_results, threshold=0.85):\n",
        "    \"\"\"NER 결과에서 PERSON 엔티티 추출\"\"\"\n",
        "    if not ner_results:\n",
        "        return []\n",
        "\n",
        "    persons = []\n",
        "\n",
        "    for entity in ner_results:\n",
        "        if entity['score'] >= threshold and entity['entity_group'] == 'PS_NAME':\n",
        "            persons.append({\n",
        "                'name': entity['word'],\n",
        "                'score': entity['score']\n",
        "            })\n",
        "\n",
        "    return persons\n",
        "\n",
        "def cluster_names(name_score_dict, threshold=1.5):\n",
        "    \"\"\"레벤슈타인 거리 기반 이름 군집화 (score 기반 대표명 선정)\"\"\"\n",
        "    if len(name_score_dict) == 0:\n",
        "        return {}\n",
        "\n",
        "    names = list(name_score_dict.keys())\n",
        "\n",
        "    if len(names) == 1:\n",
        "        return {names[0]: names[0]}\n",
        "\n",
        "    n = len(names)\n",
        "    distance_matrix = np.zeros((n, n))\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            dist = Levenshtein.distance(names[i], names[j])\n",
        "            distance_matrix[i][j] = dist\n",
        "            distance_matrix[j][i] = dist\n",
        "\n",
        "    condensed_dist = squareform(distance_matrix)\n",
        "    linkage_matrix = linkage(condensed_dist, method='average')\n",
        "    clusters = fcluster(linkage_matrix, threshold, criterion='distance')\n",
        "\n",
        "    cluster_dict = {}\n",
        "    for name, cluster_id in zip(names, clusters):\n",
        "        if cluster_id not in cluster_dict:\n",
        "            cluster_dict[cluster_id] = []\n",
        "        cluster_dict[cluster_id].append(name)\n",
        "\n",
        "    name_clusters = {}\n",
        "    for cluster_id, cluster_names in cluster_dict.items():\n",
        "        # score 기준으로 정렬하여 가장 높은 score를 가진 이름을 대표명으로 선정\n",
        "        cluster_names_sorted = sorted(cluster_names,\n",
        "                                     key=lambda x: name_score_dict[x],\n",
        "                                     reverse=True)\n",
        "        representative = cluster_names_sorted[0]\n",
        "\n",
        "        if len(cluster_names) == 1:\n",
        "            name_clusters[representative] = cluster_names[0]\n",
        "        else:\n",
        "            # 대표명을 제외한 나머지 이름들도 score 순으로 정렬\n",
        "            name_clusters[representative] = cluster_names_sorted\n",
        "\n",
        "    return name_clusters\n",
        "\n",
        "# ========== STEP 1: NER 수행 ==========\n",
        "print(\"\\n[2/5] NER 수행 중...\")\n",
        "with open(input_json_path, 'r', encoding='utf-8') as f:\n",
        "    segments = json.load(f)\n",
        "\n",
        "print(f\"  - 총 {len(segments)}개 세그먼트 처리\")\n",
        "print(f\"  - NER 임계값: {NER_THRESHOLD}\")\n",
        "\n",
        "segments_with_names = []\n",
        "all_names = []\n",
        "name_scores = {}  # 이름별 최대 score 저장\n",
        "\n",
        "for idx, segment in enumerate(segments):\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        print(f\"  - 진행: {idx + 1}/{len(segments)}\")\n",
        "\n",
        "    text = segment['text']\n",
        "    start_time = segment['start']\n",
        "    end_time = segment['end']\n",
        "\n",
        "    ner_results = ner(text)\n",
        "    person_names_with_scores = extract_person_names(ner_results, threshold=NER_THRESHOLD)\n",
        "\n",
        "    # 이름만 추출\n",
        "    person_names = [item['name'] for item in person_names_with_scores]\n",
        "\n",
        "    # 각 이름의 최대 score 업데이트\n",
        "    for item in person_names_with_scores:\n",
        "        name = item['name']\n",
        "        score = item['score']\n",
        "        if name not in name_scores or score > name_scores[name]:\n",
        "            name_scores[name] = score\n",
        "\n",
        "    segment_with_name = {\n",
        "        'text': text,\n",
        "        'start': start_time,\n",
        "        'end': end_time,\n",
        "        'name': person_names if person_names else None\n",
        "    }\n",
        "\n",
        "    segments_with_names.append(segment_with_name)\n",
        "\n",
        "    if person_names:\n",
        "        all_names.extend(person_names)\n",
        "\n",
        "unique_names = sorted(set(all_names))\n",
        "print(f\"✓ NER 완료: {len(unique_names)}개 고유 이름 추출\")\n",
        "\n",
        "# ========== STEP 2: 이름 군집화 ==========\n",
        "print(\"\\n[3/5] 이름 군집화 중...\")\n",
        "print(f\"  - 군집화 임계값: {CLUSTER_THRESHOLD}\")\n",
        "\n",
        "if len(unique_names) > 0:\n",
        "    name_clusters = cluster_names(name_scores, threshold=CLUSTER_THRESHOLD)\n",
        "    final_namelist = sorted(name_clusters.keys())\n",
        "\n",
        "    multi_clusters = sum(1 for v in name_clusters.values() if isinstance(v, list) and len(v) > 1)\n",
        "    print(f\"✓ 군집화 완료: {len(unique_names)} → {len(final_namelist)}개 대표명\")\n",
        "    print(f\"  - 유사 이름 군집: {multi_clusters}개\")\n",
        "else:\n",
        "    name_clusters = {}\n",
        "    final_namelist = []\n",
        "    print(\"  - 추출된 이름 없음\")\n",
        "\n",
        "# ========== STEP 3: 이름 체크 트랜스크립트 생성 ==========\n",
        "print(\"\\n[4/5] 이름 체크 트랜스크립트 생성 중...\")\n",
        "\n",
        "# unique_names를 set으로 변환하여 빠른 검색\n",
        "unique_names_set = set(unique_names)\n",
        "\n",
        "output_lines = []\n",
        "name_found_count = 0\n",
        "\n",
        "for segment in segments_with_names:\n",
        "    text = segment['text']\n",
        "    names = segment.get('name')\n",
        "\n",
        "    # unique_names에 있는 이름이 하나라도 포함되어 있는지 확인\n",
        "    has_valid_name = False\n",
        "    if names is not None and names != [] and names != '':\n",
        "        # names가 리스트인 경우\n",
        "        if isinstance(names, list):\n",
        "            for name in names:\n",
        "                if name in unique_names_set:\n",
        "                    has_valid_name = True\n",
        "                    break\n",
        "        # names가 문자열인 경우\n",
        "        elif isinstance(names, str):\n",
        "            if names in unique_names_set:\n",
        "                has_valid_name = True\n",
        "\n",
        "    if has_valid_name:\n",
        "        check_mark = 'v'\n",
        "        name_found_count += 1\n",
        "    else:\n",
        "        check_mark = ' '\n",
        "\n",
        "    line = f\"[{check_mark}] '{text}'\"\n",
        "    output_lines.append(line)\n",
        "\n",
        "print(f\"✓ 트랜스크립트 생성 완료: {name_found_count}/{len(segments_with_names)}개 세그먼트에서 이름 발견\")\n",
        "\n",
        "# ========== STEP 4: 파일 저장 ==========\n",
        "print(\"\\n[5/5] 결과 파일 저장 중...\")\n",
        "\n",
        "# 1. stt_segments_with_name.json\n",
        "with open(output_json_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(segments_with_names, f, ensure_ascii=False, indent=2)\n",
        "print(f\"  ✓ {output_json_path}\")\n",
        "\n",
        "# 2. name_clusters.json\n",
        "if len(name_clusters) > 0:\n",
        "    with open(cluster_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(name_clusters, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"  ✓ {cluster_json_path}\")\n",
        "\n",
        "# 3. final_namelist.txt\n",
        "if len(final_namelist) > 0:\n",
        "    with open(final_namelist_path, 'w', encoding='utf-8') as f:\n",
        "        for name in final_namelist:\n",
        "            f.write(name + '\\n')\n",
        "    print(f\"  ✓ {final_namelist_path}\")\n",
        "\n",
        "# 4. name_check_transcript.txt\n",
        "with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
        "    for line in output_lines:\n",
        "        f.write(line + '\\n')\n",
        "print(f\"  ✓ {output_txt_path}\")\n",
        "\n",
        "# ========== 최종 통계 ==========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"처리 완료!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"NER 추출 이름: {len(unique_names)}개\")\n",
        "print(f\"최종 대표명: {len(final_namelist)}개\")\n",
        "print(f\"이름 발견 세그먼트: {name_found_count}/{len(segments_with_names)}개 ({name_found_count/len(segments_with_names)*100:.1f}%)\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}