# TO DO

- [x]  senko I/O 확인
    - [x]  [모듈 B] Speaker Diarization (화자 분리)
    - [x]  [모듈 A] STT (Whisper)
- [x]  태깅 알고리즘 및 아이디어.

# 결과

# senko I/O

1.[모듈 B] Speaker Diarization (화자 분리)

```markdown
{
	'turns': [
		{'speaker_label': 'SPEAKER_04', 'start': 4.57, 'end': 8.92},
	  {'speaker_label': 'SPEAKER_02', 'start': 8.92, 'end': 10.37},
	  {'speaker_label': 'SPEAKER_04', 'start': 10.37, 'end': 16.33},
	  {'speaker_label': 'SPEAKER_02', 'start': 17.38, 'end': 19.16},
	  ...
	  ],
	'embeddings': {
		'SPEAKER_01': [0.34316712617874146, 0.013158809393644333, 0.4214532971382141,0.001457923324778676,...],
	  'SPEAKER_02': [0.4227710962295532, 0.032404541969299316, 0.5192215442657471,...],
	   ...
	   }
}
```

[diarization_result.json](attachment:db905e34-eaea-45d6-b23c-de2455ed2e4c:diarization_result.json)

2.[모듈 A] STT (Whisper)

```markdown
[
  {
    "text": "그러면 너무 회의만 하지 말고",
    "start": 0.0,
    "end": 6.7
  },
  {
    "text": "좀 토킹 어바웃을 해볼까요?",
    "start": 6.7,
    "end": 9.3
  },
  {
    "text": "여유롭게",
    "start": 9.4,
    "end": 10.2
  },
  ...
]
```

[stt_segments.json](attachment:59b85f72-5a4f-41d3-bcab-480db30b052c:stt_segments.json)

1. 병합

```markdown
{
	"turns": [
    {
      "speaker_label": "SPEAKER_01",
      "start": 4.57,
      "end": 16.3,
      "text": "좀 토킹 어바웃을 해볼까요? 여유롭게 처음에 아이트 브레이킹? 지금 누는 거예요? 네 저 재형님 말하세요 방금 강재림 20분 설치가 들어왔습니다"
    },
    {
      "speaker_label": "SPEAKER_04",
      "start": 17.4,
      "end": 19.19,
      "text": ""
    },
    {
      "speaker_label": "SPEAKER_01",
      "start": 19.19,
      "end": 20.5,
      "text": "강재로요?"
    }, ...
  ],
	"embeddings": {
    "SPEAKER_01": [
      0.2775922417640686,
      0.023142734542489052,
      0.6203261017799377,
      0.0014643314061686397, ...],
    ...
  }
}
    
```

[diarization_with_stt_result.json](attachment:3cc21027-3d6e-4f57-a181-9fcc9b793ac7:diarization_with_stt_result.json)

[whisper+senko_결과포맷팅.ipynb](attachment:9e6da9ca-b1e6-4fc2-8f95-95f69ff6a695:whispersenko_결과포맷팅.ipynb)

---

# 태깅 알고리즘 및 아이디어

이름 사전 만든다고 하자 ← NER

‘민서’와 ‘인서’ , ‘재영’와 ‘재형’, ‘상준’와 ‘상주’를 비슷한 인물이라 인식할 수 있을까?

레벤슈타인 거리(Levenshtein Distance)를 이용하자.

- 코드
    
    ```markdown
    # =========================================
    # Levenshtein 기반 이름 유사도 매칭/클러스터링 (Colab OK)
    # 외부 라이브러리 불필요
    # =========================================
    
    from typing import List, Dict, Tuple
    
    # 1) 예시 이름 리스트
    names = ['고수', '기호', '김준형', '니모', '도랑', '동당', '리모', '민서', '상주', '상준', '재영', 
             '성현', '소람이', '송경', '윤희', '이상준', '자 재훈', '재형', '지윤이', '지효', '인서', 
             '지훈', '차론', '차론이', '최진', '치즈', '태깅의', '태깅이', '홍기']
    
    # 2) 간단 정규화(표기 변이 완화)
    # - 공백 제거
    # - 흔한 호칭/어미 제거: '님', '씨', (어말) '이'
    # ※ 과도한 정규화는 오탐 가능 → 필요한 항목만 사용
    def normalize_name(s: str) -> str:
        t = s.strip()
        t = t.replace(" ", "")
        # 접미 호칭 제거(끝 부분에만 적용)
        for suf in ("님", "씨"):
            if t.endswith(suf) and len(t) > 2:
                t = t[: -len(suf)]
        # 구어체 어미 '이' 제거(짧은 이름 보호)
        if t.endswith("이") and len(t) > 2:
            t = t[:-1]
        return t
    
    # 3) 레벤슈타인 거리 (O(nm) DP)
    def levenshtein(a: str, b: str) -> int:
        la, lb = len(a), len(b)
        if la == 0: return lb
        if lb == 0: return la
        # 1차원 DP
        prev = list(range(lb + 1))
        for i in range(1, la + 1):
            cur = [i] + [0] * lb
            ca = a[i - 1]
            for j in range(1, lb + 1):
                cb = b[j - 1]
                cost = 0 if ca == cb else 1
                cur[j] = min(
                    prev[j] + 1,       # deletion
                    cur[j - 1] + 1,    # insertion
                    prev[j - 1] + cost # substitution
                )
            prev = cur
        return prev[-1]
    
    # 4) 유사도 = 1 - dist / max_len
    def name_similarity(a: str, b: str) -> float:
        if not a and not b: 
            return 1.0
        if not a or not b:
            return 0.0
        dist = levenshtein(a, b)
        return 1.0 - dist / max(len(a), len(b))
    
    # 5) 유니온파인드로 클러스터링
    class UnionFind:
        def __init__(self, n: int):
            self.p = list(range(n))
            self.r = [0]*n
        def find(self, x: int) -> int:
            if self.p[x] != x:
                self.p[x] = self.find(self.p[x])
            return self.p[x]
        def union(self, a: int, b: int):
            pa, pb = self.find(a), self.find(b)
            if pa == pb: return
            if self.r[pa] < self.r[pb]:
                self.p[pa] = pb
            elif self.r[pa] > self.r[pb]:
                self.p[pb] = pa
            else:
                self.p[pb] = pa
                self.r[pa] += 1
    
    # 6) 매칭/클러스터링 파라미터
    SIM_THRESHOLD = 0.4   # 유사도 기준(0~1). 높일수록 보수적
    MAX_LEN_GAP   = 2     # 길이 차이 허용(과도한 길이 차이는 매칭 배제)
    
    # 7) 정규화 사전 준비
    norm_map = {name: normalize_name(name) for name in names}
    norm_list = [norm_map[n] for n in names]
    
    # 8) 페어 매칭 & 클러스터링
    n = len(names)
    uf = UnionFind(n)
    pairs: List[Tuple[str, str, float]] = []
    
    for i in range(n):
        for j in range(i+1, n):
            a_raw, b_raw = names[i], names[j]
            a, b = norm_list[i], norm_list[j]
            # 빠른 가지치기: 길이 차이
            if abs(len(a) - len(b)) > MAX_LEN_GAP:
                continue
            sim = name_similarity(a, b)
            if sim >= SIM_THRESHOLD:
                uf.union(i, j)
                pairs.append((a_raw, b_raw, sim))
    
    # 9) 클러스터별로 묶기(원본 표기 유지)
    clusters: Dict[int, List[str]] = {}
    for idx in range(n):
        root = uf.find(idx)
        clusters.setdefault(root, []).append(names[idx])
    
    # 10) 대표명(캐노니컬) 선택 규칙:
    # - 정규화된 형태가 가장 짧은 것 우선
    # - 동률이면 사전순
    def pick_canonical(group: List[str]) -> str:
        group_sorted = sorted(group, key=lambda x: (len(norm_map[x]), norm_map[x]))
        return group_sorted[0]
    
    # 11) 결과 출력
    print("=== 유사 페어 후보(임곗값 충족) ===")
    for a, b, s in sorted(pairs, key=lambda x: -x[2]):
        print(f"{a}  ~  {b}   (유사도: {s:.2f})")
    
    print("\n=== 클러스터 결과 ===")
    for root, group in clusters.items():
        if len(group) == 1:
            continue  # 단독이면 제외하려면 유지/제외 선택 가능
        canon = pick_canonical(group)
        print(f"- 대표명: {canon}  ←  {group}")
    
    # 단독까지 포함해 전체 매핑 보고 싶으면 아래 주석 해제
    # print("\n=== 전체 이름 → 대표명 매핑 ===")
    # for root, group in clusters.items():
    #     canon = pick_canonical(group)
    #     for g in group:
    #         print(f"{g} -> {canon}")
    
    ```
    

gliner로 아래와 같은 이름 후보 사전을 만들었다고 하자.

```markdown
names = ['고수', '기호', '김준형', '니모', '도랑', '동당', '리모', '민서', '상주', '상준', '재영', 
         '성현', '소람이', '송경', '윤희', '이상준', '자 재훈', '재형', '지윤이', '지효', '인서', 
         '지훈', '차론', '차론이', '최진', '치즈', '태깅의', '태깅이', '홍기']
```

결과

```markdown
{0: ['고수'],
 1: ['기호'],
 2: ['김준형'],
 3: ['니모', '리모'],
 4: ['도랑'],
 5: ['동당'],
 7: ['민서', '인서'],
 8: ['상주', '상준', '이상준'],
 10: ['재영', '재형'],
 11: ['성현'],
 12: ['소람이'],
 13: ['송경'],
 14: ['윤희'],
 16: ['자 재훈'],
 18: ['지윤이', '지효', '지훈'],
 22: ['차론', '차론이'],
 24: ['최진'],
 25: ['치즈'],
 26: ['태깅의', '태깅이'],
 28: ['홍기']}
```

# 진행 중 어려운 점 (장애물이나 막힌곳 설명)

1. toruchaudio와 senko 버전 충돌
    1. 패키지 다운그레이드 없이 faster-whisper 만 install 하고 하니 diarization은 동작하나 whisper 모듈이 튕김

# 해결방안

1. vad가 가능한 faster-whisper 대신 wishper 사용함 
2. 세그먼트는 senko diarization 결과를 가지고 진행
3. I/O는 맞추었습니다.

오디오 세그먼트 (오디오 물림)

NER 퀄리티

1. bert 계열 classification
2. 문장 유형 판정 후 다음 speaker 태깅