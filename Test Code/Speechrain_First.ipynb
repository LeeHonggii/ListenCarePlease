{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f399947",
   "metadata": {},
   "source": [
    "#Speech Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) 환경 준비: 버전 충돌 최소화를 위한 가벼운 설치\n",
    "#  - torch/torchaudio는 Colab 기본 버전 사용 (CUDA에 맞춤)\n",
    "#  - speechbrain만 설치, 나머지는 범위 고정\n",
    "# ============================================================\n",
    "!pip -q install \"speechbrain>=0.5.16,<0.6.0\" \\\n",
    "                \"librosa>=0.10,<0.11\" \\\n",
    "                \"soundfile>=0.12,<0.14\" \\\n",
    "                \"webrtcvad>=2.0.10,<3.0\" \\\n",
    "                \"scikit-learn>=1.3,<1.6\" \\\n",
    "                \"numpy>=1.23,<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) 의존성 임포트\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import webrtcvad\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.cluster import SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Google Drive 마운트 & 입출력 경로\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "INPUT_WAV = '/content/drive/MyDrive/AI_NLP_FINAL/1105_오전회의.wav'\n",
    "WORKDIR   = '/content'\n",
    "BASENAME  = 'result'\n",
    "\n",
    "OUT_RTTM  = os.path.join(WORKDIR, f'{BASENAME}.rttm')\n",
    "OUT_CSV   = os.path.join(WORKDIR, f'{BASENAME}.csv')\n",
    "OUT_VTT   = os.path.join(WORKDIR, f'{BASENAME}.vtt')\n",
    "OUT_JSON  = os.path.join(WORKDIR, f'{BASENAME}.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) 디바이스 설정\n",
    "# ============================================================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) 오디오 로드 + 리샘플(16 kHz, mono)\n",
    "# ============================================================\n",
    "target_sr = 16000\n",
    "wav, sr = librosa.load(INPUT_WAV, sr=target_sr, mono=True)\n",
    "wav = np.ascontiguousarray(wav, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) VAD(webrtcvad)로 음성 구간 검출\n",
    "#    - 30ms 프레임, aggressiveness=2 (중간)\n",
    "#    - 최소 구간 0.3s, 인접 병합 허용 0.15s\n",
    "# ============================================================\n",
    "vad = webrtcvad.Vad(2)\n",
    "frame_dur_ms = 30\n",
    "frame_len = int(target_sr * frame_dur_ms / 1000)\n",
    "\n",
    "def float_to_pcm16(x):\n",
    "    x = np.clip(x, -1.0, 1.0)\n",
    "    return (x * 32767.0).astype(np.int16)\n",
    "\n",
    "pcm16 = float_to_pcm16(wav).tobytes()\n",
    "\n",
    "frames = []\n",
    "for i in range(0, len(wav) - frame_len + 1, frame_len):\n",
    "    start = i\n",
    "    end   = i + frame_len\n",
    "    chunk = pcm16[start*2:end*2]  # int16 => 2 bytes/sample\n",
    "    is_speech = vad.is_speech(chunk, sample_rate=target_sr)\n",
    "    frames.append((start, end, is_speech))\n",
    "\n",
    "speech_regions = []\n",
    "min_region_ms       = 300\n",
    "merge_tolerance_ms  = 150\n",
    "min_region = int(target_sr * (min_region_ms / 1000))\n",
    "merge_tol  = int(target_sr * (merge_tolerance_ms / 1000))\n",
    "\n",
    "active = None\n",
    "for (start, end, is_speech) in frames:\n",
    "    if is_speech and active is None:\n",
    "        active = [start, end]\n",
    "    elif is_speech and active is not None:\n",
    "        active[1] = end\n",
    "    elif (not is_speech) and active is not None:\n",
    "        if active[1] - active[0] >= min_region:\n",
    "            speech_regions.append(tuple(active))\n",
    "        active = None\n",
    "if active is not None and active[1] - active[0] >= min_region:\n",
    "    speech_regions.append(tuple(active))\n",
    "\n",
    "# 인접 병합\n",
    "merged = []\n",
    "for seg in speech_regions:\n",
    "    if not merged:\n",
    "        merged.append(list(seg))\n",
    "    else:\n",
    "        if seg[0] - merged[-1][1] <= merge_tol:\n",
    "            merged[-1][1] = seg[1]\n",
    "        else:\n",
    "            merged.append(list(seg))\n",
    "speech_regions = [tuple(x) for x in merged]\n",
    "\n",
    "print(f'VAD speech regions: {len(speech_regions)} segments')\n",
    "\n",
    "# ============================================================\n",
    "# 6) 슬라이딩 윈도우 구성 후 임베딩 추출 (SpeechBrain ECAPA)\n",
    "#    - 윈도우 1.5s, 홉 0.50s (세분화)\n",
    "# ============================================================\n",
    "win_sec = 1.5\n",
    "hop_sec = 0.50   # 0.75 -> 0.50 로 조정\n",
    "win = int(target_sr * win_sec)\n",
    "hop = int(target_sr * hop_sec)\n",
    "\n",
    "windows = []\n",
    "for (s, e) in speech_regions:\n",
    "    cur = s\n",
    "    while cur + win <= e:\n",
    "        windows.append((cur, cur + win))\n",
    "        cur += hop\n",
    "    # 꼬리 구간도 일정 길이 이상이면 추가 (>= 0.9s)\n",
    "    if e - cur > win * 0.5:\n",
    "        end = min(e, cur + win)\n",
    "        if end - cur >= int(0.6 * win):\n",
    "            windows.append((cur, end))\n",
    "\n",
    "print(f'Windows to embed: {len(windows)}')\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    run_opts={\"device\": device}\n",
    ")\n",
    "\n",
    "emb_list = []\n",
    "if len(windows) > 0:\n",
    "    for (s, e) in windows:\n",
    "        seg = torch.tensor(wav[s:e]).unsqueeze(0)  # [1, time]\n",
    "        seg = seg.to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = classifier.encode_batch(seg)     # [1, 1, 192] 또는 [1, 192]\n",
    "        emb = emb.squeeze().cpu().numpy()          # -> (192,)\n",
    "        emb_list.append(emb)\n",
    "\n",
    "embeddings = np.stack(emb_list, axis=0) if len(emb_list) > 0 else np.empty((0, 192))\n",
    "print('Embeddings shape:', embeddings.shape)\n",
    "\n",
    "# ============================================================\n",
    "# (신규) 임베딩 전처리: 표준화 + PCA(whiten) + L2\n",
    "# ============================================================\n",
    "def preprocess_embeddings(E, pca_dim=64):\n",
    "    if E.shape[0] == 0:\n",
    "        return E, None\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "        ('pca', PCA(n_components=min(pca_dim, E.shape[1]), whiten=True))\n",
    "    ])\n",
    "    Z = pipe.fit_transform(E)\n",
    "    Z = Z / (np.linalg.norm(Z, axis=1, keepdims=True) + 1e-8)\n",
    "    return Z, pipe\n",
    "\n",
    "# ============================================================\n",
    "# (신규) eigengap 기반 K 추정 (스펙트럴용)\n",
    "# ============================================================\n",
    "def estimate_k_eigengap(Z, max_k=12):\n",
    "    if len(Z) < 2:\n",
    "        return 1\n",
    "    Z = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    A = cosine_similarity(Z)\n",
    "    A = np.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.fill_diagonal(A, 1.0)\n",
    "    D = np.diag(A.sum(axis=1))\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        D_inv_sqrt = np.diag(1.0 / (np.sqrt(np.diag(D)) + 1e-8))\n",
    "    L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    L = np.nan_to_num(L, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    try:\n",
    "        evals = np.sort(np.real(np.linalg.eigvals(L)))\n",
    "    except np.linalg.LinAlgError:\n",
    "        return 1\n",
    "    ks = range(2, min(max_k, len(evals)-1) + 1)\n",
    "    if not ks:\n",
    "        return 1\n",
    "    gaps = [(k, float(evals[k] - evals[k-1])) for k in ks]\n",
    "    k_star = max(gaps, key=lambda x: x[1])[0]\n",
    "    return int(k_star)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) 화자 수 추정(K) + 클러스터링 (강화)\n",
    "#    옵션:\n",
    "#      - USE_SPECTRAL: 스펙트럴 클러스터링 사용\n",
    "#      - USE_DP_GMM : Dirichlet Process GMM으로 보조 K 추정\n",
    "#      - FIXED_K    : K를 고정(정수)하면 그 값으로 바로 군집\n",
    "# ============================================================\n",
    "MAX_K        = 12\n",
    "USE_SPECTRAL = True\n",
    "USE_DP_GMM   = True\n",
    "FIXED_K      = None   # 예) 4로 고정하려면 4\n",
    "\n",
    "if len(embeddings) > 0:\n",
    "    Z, preproc_pipe = preprocess_embeddings(embeddings, pca_dim=64)\n",
    "\n",
    "    # --- 7-1) K 결정 ---\n",
    "    if FIXED_K is not None:\n",
    "        best_k = int(FIXED_K)\n",
    "    else:\n",
    "        k_eigen = estimate_k_eigengap(Z, max_k=MAX_K) if len(Z) >= 6 else 1\n",
    "        best_k = max(1, k_eigen)\n",
    "\n",
    "        if USE_DP_GMM and best_k < MAX_K and len(Z) >= 4:\n",
    "            dpgmm = BayesianGaussianMixture(\n",
    "                n_components=min(MAX_K, max(3, best_k + 2)),\n",
    "                covariance_type='full',\n",
    "                weight_concentration_prior_type='dirichlet_process',\n",
    "                max_iter=1000, random_state=0\n",
    "            ).fit(Z)\n",
    "            weights = dpgmm.weights_\n",
    "            active = (weights > (1.0 / (5 * len(weights))))  # 느슨한 임계\n",
    "            k_dp = int(max(1, active.sum()))\n",
    "            best_k = int(np.clip(round(0.5 * (best_k + k_dp)), 1, MAX_K))\n",
    "\n",
    "    # --- 7-2) 클러스터링 ---\n",
    "    if best_k == 1 or len(Z) < 2:\n",
    "        labels = np.zeros(len(Z), dtype=int)\n",
    "    else:\n",
    "        if USE_SPECTRAL:\n",
    "            A = cosine_similarity(Z)\n",
    "            A = (A - A.min()) / (A.max() - A.min() + 1e-8)\n",
    "            np.fill_diagonal(A, 1.0)\n",
    "            spec = SpectralClustering(\n",
    "                n_clusters=best_k,\n",
    "                affinity='precomputed',\n",
    "                assign_labels='kmeans',\n",
    "                random_state=0\n",
    "            )\n",
    "            labels = spec.fit_predict(A)\n",
    "        else:\n",
    "            # 대안: 유클리드 공간 AHC (코사인 기반은 scikit 설정 제약 있음)\n",
    "            ahc = AgglomerativeClustering(\n",
    "                n_clusters=best_k, linkage='average'\n",
    "            )\n",
    "            labels = ahc.fit_predict(Z)\n",
    "\n",
    "    # --- 7-3) 극소 군집 정리 (윈도우 수 1% 미만 또는 2개 미만) ---\n",
    "    if len(Z) > 0:\n",
    "        uniq, counts = np.unique(labels, return_counts=True)\n",
    "        tiny = set(uniq[counts < max(2, int(0.01 * len(labels)))])\n",
    "        if tiny:\n",
    "            centroids = {c: Z[labels == c].mean(axis=0) for c in uniq}\n",
    "            keep = [c for c in uniq if c not in tiny]\n",
    "            for c in tiny:\n",
    "                if not keep:\n",
    "                    continue\n",
    "                # 코사인 유사도 최대 centroid에 흡수\n",
    "                target = max(\n",
    "                    keep,\n",
    "                    key=lambda kk: np.dot(centroids[c], centroids[kk]) /\n",
    "                                   (np.linalg.norm(centroids[c]) * np.linalg.norm(centroids[kk]) + 1e-8)\n",
    "                )\n",
    "                labels[labels == c] = target\n",
    "\n",
    "        # 라벨 재인덱스(0..K-1)\n",
    "        _, inv = np.unique(labels, return_inverse=True)\n",
    "        labels = inv\n",
    "        best_k = len(np.unique(labels))\n",
    "else:\n",
    "    labels = np.array([])\n",
    "    best_k = 0\n",
    "\n",
    "print(f'Estimated #speakers (K): {best_k}')\n",
    "\n",
    "# ============================================================\n",
    "# 8) 윈도우 라벨을 타임라인으로 투영 + 후처리 병합\n",
    "#    - gap 병합 임계 0.15s (더 세분)\n",
    "#    - 최소 세그 길이 0.25s (짧은 턴 보존)\n",
    "# ============================================================\n",
    "segments = []\n",
    "for (idx, (s, e)) in enumerate(windows):\n",
    "    spk = f'spk{labels[idx]}' if len(labels) > 0 else 'spk0'\n",
    "    segments.append([s, e, spk])\n",
    "\n",
    "def merge_labeled_segments(segs, gap_samples=int(0.15 * target_sr)):\n",
    "    if not segs:\n",
    "        return []\n",
    "    merged = [segs[0][:]]\n",
    "    for s, e, spk in segs[1:]:\n",
    "        ps, pe, pspk = merged[-1]\n",
    "        if spk == pspk and s - pe <= gap_samples:\n",
    "            merged[-1][1] = max(pe, e)\n",
    "        else:\n",
    "            merged.append([s, e, spk])\n",
    "    return merged\n",
    "\n",
    "segments = merge_labeled_segments(segments)\n",
    "\n",
    "def clamp_to_regions(segs, regions):\n",
    "    if not segs:\n",
    "        return []\n",
    "    out = []\n",
    "    r_idx = 0\n",
    "    for s, e, spk in segs:\n",
    "        while r_idx < len(regions) and regions[r_idx][1] <= s:\n",
    "            r_idx += 1\n",
    "        rj = r_idx\n",
    "        while rj < len(regions) and regions[rj][0] < e:\n",
    "            cs = max(s, regions[rj][0])\n",
    "            ce = min(e, regions[rj][1])\n",
    "            if ce > cs:\n",
    "                out.append([cs, ce, spk])\n",
    "            rj += 1\n",
    "    return merge_labeled_segments(sorted(out, key=lambda x: x[0]))\n",
    "\n",
    "segments = clamp_to_regions(segments, speech_regions)\n",
    "\n",
    "min_keep = int(0.25 * target_sr)   # 0.35 -> 0.25 로 완화\n",
    "segments = [seg for seg in segments if seg[1] - seg[0] >= min_keep]\n",
    "\n",
    "print(f'Final diarized segments: {len(segments)}')\n",
    "\n",
    "# ============================================================\n",
    "# 9) 결과 파일 저장 (RTTM / CSV / VTT / JSONL)\n",
    "# ============================================================\n",
    "def samples_to_time(s):\n",
    "    return s / target_sr\n",
    "\n",
    "file_id = os.path.splitext(os.path.basename(INPUT_WAV))[0]\n",
    "\n",
    "# RTTM\n",
    "with open(OUT_RTTM, 'w', encoding='utf-8') as f:\n",
    "    for s, e, spk in segments:\n",
    "        start = samples_to_time(s)\n",
    "        dur   = samples_to_time(e - s)\n",
    "        line = f\"SPEAKER {file_id} 1 {start:.3f} {dur:.3f} <NA> <NA> {spk} <NA> <NA>\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "# CSV\n",
    "with open(OUT_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['start', 'end', 'duration', 'speaker'])\n",
    "    for s, e, spk in segments:\n",
    "        start = samples_to_time(s)\n",
    "        end   = samples_to_time(e)\n",
    "        writer.writerow([f'{start:.3f}', f'{end:.3f}', f'{end - start:.3f}', spk])\n",
    "\n",
    "# VTT\n",
    "def to_vtt_timestamp(t):\n",
    "    h = int(t // 3600); t -= h*3600\n",
    "    m = int(t // 60);   t -= m*60\n",
    "    s = int(t);         ms = int(round((t - s) * 1000))\n",
    "    return f\"{h:02}:{m:02}:{s:02}.{ms:03}\"\n",
    "\n",
    "with open(OUT_VTT, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"WEBVTT\\n\\n\")\n",
    "    for idx, (s, e, spk) in enumerate(segments, 1):\n",
    "        f.write(f\"{idx}\\n\")\n",
    "        f.write(f\"{to_vtt_timestamp(samples_to_time(s))} --> {to_vtt_timestamp(samples_to_time(e))}\\n\")\n",
    "        f.write(f\"{spk}\\n\\n\")\n",
    "\n",
    "# JSONL\n",
    "with open(OUT_JSON, 'w', encoding='utf-8') as f:\n",
    "    for s, e, spk in segments:\n",
    "        rec = {\n",
    "            'start': round(samples_to_time(s), 3),\n",
    "            'end'  : round(samples_to_time(e), 3),\n",
    "            'speaker': spk\n",
    "        }\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 요약 출력\n",
    "dur_total = len(wav) / target_sr\n",
    "speakers = sorted({spk for _,_,spk in segments})\n",
    "print(\"\\n================ SUMMARY ================\")\n",
    "print(f\"Processed file : {INPUT_WAV}\")\n",
    "print(f\"Audio duration : {dur_total:.1f} sec\")\n",
    "print(f\"Speakers (est) : {len(speakers)} -> {speakers}\")\n",
    "print(\"Saved files:\")\n",
    "print(\" -\", OUT_RTTM)\n",
    "print(\" -\", OUT_CSV)\n",
    "print(\" -\", OUT_VTT)\n",
    "print(\" -\", OUT_JSON)\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b227ef",
   "metadata": {},
   "source": [
    "#Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 설정\n",
    "# ============================================================\n",
    "DIAR_JSONL = \"/content/drive/MyDrive/AI_NLP_FINAL/SpeechBrain_result2.jsonl\"  # ← SpeechBrain 결과(JSONL, 초단위)\n",
    "AUDIO_PATH = \"/content/drive/MyDrive/AI_NLP_FINAL/1105_오전회의.wav\"\n",
    "WORKDIR    = \"/content\"  # 출력 파일 저장 폴더\n",
    "\n",
    "# 출력 파일 (CSV / JSONL만)\n",
    "import os\n",
    "BASENAME = \"whisper_speaker\"\n",
    "OUT_CSV   = os.path.join(WORKDIR, f\"{BASENAME}.csv\")\n",
    "OUT_JSONL = os.path.join(WORKDIR, f\"{BASENAME}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea19f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) 다이어리제이션 JSONL 로드 (초 단위)\n",
    "# ============================================================\n",
    "import json\n",
    "\n",
    "speaker_regions = []  # [(start_sec, end_sec, speaker)]\n",
    "with open(DIAR_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        s = float(rec[\"start\"])\n",
    "        e = float(rec[\"end\"])\n",
    "        spk = str(rec[\"speaker\"])\n",
    "        if e > s:\n",
    "            speaker_regions.append((s, e, spk))\n",
    "\n",
    "speaker_regions.sort(key=lambda x: x[0])\n",
    "print(f\"[INFO] Loaded diarization regions: {len(speaker_regions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Whisper 로드 + 타임스탬프 포함 ASR\n",
    "# ============================================================\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "print(f\"[INFO] Loading Whisper: {model_id} on {device} ({torch_dtype})\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Transcribing '{AUDIO_PATH}' ...\")\n",
    "whisper_out = asr(\n",
    "    AUDIO_PATH,\n",
    "    chunk_length_s=30,\n",
    "    stride_length_s=(4, 2),\n",
    "    generate_kwargs={\"language\": \"ko\", \"task\": \"transcribe\"},\n",
    ")\n",
    "\n",
    "chunks = whisper_out.get(\"chunks\", []) or whisper_out.get(\"segments\", [])\n",
    "print(f\"[INFO] Whisper chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5130b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3) Whisper 세그먼트를 diarization 구간에 매핑\n",
    "# ============================================================\n",
    "def overlap_len(a0, a1, b0, b1):\n",
    "    return max(0.0, min(a1, b1) - max(a0, b0))\n",
    "\n",
    "assigned = []  # [(start, end, speaker, text)]\n",
    "for ch in chunks:\n",
    "    ts = ch.get(\"timestamp\", None) or ch.get(\"timestamps\", None)\n",
    "    if isinstance(ts, dict):\n",
    "        st, ed = ts.get(\"start\"), ts.get(\"end\")\n",
    "    elif isinstance(ts, (list, tuple)) and len(ts) == 2:\n",
    "        st, ed = ts\n",
    "    else:\n",
    "        st = ed = None\n",
    "\n",
    "    if st is None or ed is None:\n",
    "        continue\n",
    "    s_sec, e_sec = float(st), float(ed)\n",
    "    text = (ch.get(\"text\") or \"\").strip()\n",
    "    if not text or e_sec <= s_sec:\n",
    "        continue\n",
    "\n",
    "    best_spk, best_ov = None, 0.0\n",
    "    for (ds, de, spk) in speaker_regions:\n",
    "        if de < s_sec:\n",
    "            continue\n",
    "        if ds > e_sec:\n",
    "            break\n",
    "        ov = overlap_len(s_sec, e_sec, ds, de)\n",
    "        if ov > best_ov:\n",
    "            best_ov = ov\n",
    "            best_spk = spk\n",
    "\n",
    "    if best_spk is None or best_ov < 0.10:\n",
    "        continue\n",
    "    assigned.append([s_sec, e_sec, best_spk, text])\n",
    "\n",
    "print(f\"[INFO] Speaker-assigned chunks: {len(assigned)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) 같은 화자이면서 인접(≤0.5s) 세그먼트는 병합\n",
    "# ============================================================\n",
    "def merge_speaker_chunks(rows, gap_thresh=0.5):\n",
    "    if not rows:\n",
    "        return []\n",
    "    rows = sorted(rows, key=lambda x: x[0])\n",
    "    merged = [rows[0][:]]\n",
    "    for s, e, spk, txt in rows[1:]:\n",
    "        ps, pe, pspk, ptxt = merged[-1]\n",
    "        if spk == pspk and (s - pe) <= gap_thresh:\n",
    "            merged[-1][1] = max(pe, e)\n",
    "            if ptxt and not ptxt.endswith(('。','.', '!', '?', '…')):\n",
    "                ptxt += ' '\n",
    "            merged[-1][3] = (ptxt + txt).strip()\n",
    "        else:\n",
    "            merged.append([s, e, spk, txt])\n",
    "    return merged\n",
    "\n",
    "turns = merge_speaker_chunks(assigned, gap_thresh=0.5)\n",
    "print(f\"[INFO] Merged speaker turns: {len(turns)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) 저장: CSV / JSONL\n",
    "# ============================================================\n",
    "import csv\n",
    "\n",
    "# CSV\n",
    "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"start\", \"end\", \"duration\", \"speaker\", \"text\"])\n",
    "    for s, e, spk, txt in turns:\n",
    "        w.writerow([f\"{s:.3f}\", f\"{e:.3f}\", f\"{(e-s):.3f}\", spk, txt])\n",
    "\n",
    "# JSONL\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s, e, spk, txt in turns:\n",
    "        rec = {\"start\": round(s, 3), \"end\": round(e, 3), \"speaker\": spk, \"text\": txt}\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"\\n================ SUMMARY ================\")\n",
    "print(f\"Input audio          : {AUDIO_PATH}\")\n",
    "print(f\"Diarization jsonl    : {DIAR_JSONL}\")\n",
    "print(f\"Assigned speaker turns: {len(turns)}\")\n",
    "print(\"Saved files:\")\n",
    "print(\" -\", OUT_CSV)\n",
    "print(\" -\", OUT_JSONL)\n",
    "print(\"=========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
